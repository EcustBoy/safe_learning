{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for a Cart-Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.linalg import block_diag\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "import safe_learning\n",
    "import plotting\n",
    "from utilities import constrained_batch_sampler, CartPole, compute_closedloop_response, get_parameter_change\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x\n",
    "    \n",
    "np_dtype = safe_learning.config.np_dtype\n",
    "tf_dtype = safe_learning.config.dtype\n",
    "\n",
    "# TODO testing ****************************************#\n",
    "\n",
    "class Options(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Options, self).__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "OPTIONS = Options(np_dtype              = safe_learning.config.np_dtype,\n",
    "                  tf_dtype              = safe_learning.config.dtype,\n",
    "                  train_policy          = True,\n",
    "                  train_value_function  = True,\n",
    "                  use_linear_dynamics   = False,\n",
    "                  saturate              = True,\n",
    "                  eps                   = 1e-8,\n",
    "                  dpi                   = 100,\n",
    "                  fontproperties        = FontProperties(size=10),\n",
    "                  save_figs             = True,\n",
    "                  fig_path              = 'figures/cartpole_rl/')\n",
    "\n",
    "HEAT_MAP = plt.get_cmap('inferno', lut=None)\n",
    "HEAT_MAP.set_over('white')\n",
    "HEAT_MAP.set_under('black')\n",
    "\n",
    "LEVEL_MAP = plt.get_cmap('viridis', lut=21)\n",
    "LEVEL_MAP.set_over('gold')\n",
    "LEVEL_MAP.set_under('white')\n",
    "\n",
    "BINARY_MAP = ListedColormap([(1., 1., 1., 0.), (0., 1., 0., 0.65)])\n",
    "\n",
    "def binary_cmap(color='red', alpha=1.):\n",
    "    if color=='red':\n",
    "        color_code = (1., 0., 0., alpha)\n",
    "    elif color=='green':\n",
    "        color_code = (0., 1., 0., alpha)\n",
    "    elif color=='blue':\n",
    "        color_code = (0., 0., 1., alpha)\n",
    "    else:\n",
    "        color_code = (0., 0., 0., alpha)\n",
    "    transparent_code = (1., 1., 1., 0.)\n",
    "    return ListedColormap([transparent_code, color_code])\n",
    "\n",
    "#******************************************************#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CPU_COUNT = os.cpu_count()\n",
    "NUM_CORES = 8\n",
    "NUM_SOCKETS = 2\n",
    "\n",
    "os.environ[\"KMP_BLOCKTIME\"]    = str(0)\n",
    "os.environ[\"KMP_SETTINGS\"]     = str(1)\n",
    "os.environ[\"KMP_AFFINITY\"]     = 'granularity=fine,noverbose,compact,1,0'\n",
    "os.environ[\"OMP_NUM_THREADS\"]  = str(NUM_CORES)\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads  = NUM_CORES,\n",
    "                        inter_op_parallelism_threads  = NUM_SOCKETS,\n",
    "                        allow_soft_placement          = False,\n",
    "#                         log_device_placement          = True,\n",
    "                        device_count                  = {'CPU': MAX_CPU_COUNT})\n",
    "\n",
    "# TODO manually for CPU-only?\n",
    "config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n",
    "\n",
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "session = tf.InteractiveSession(config=config)\n",
    "\n",
    "# print('Found MAX_CPU_COUNT =', MAX_CPU_COUNT)\n",
    "# for dev in session.list_devices():\n",
    "#     print(dev)\n",
    "\n",
    "initialized = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy(tf_actions, tf_true_actions, n_points, fixed_state, colors=['r','b'], show=True):\n",
    "    fig = plt.figure(figsize=(12, 5), dpi=OPTIONS.dpi)\n",
    "    fig.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "    xx, yy = np.mgrid[-1:1:np.complex(0, n_points[0]), -1:1:np.complex(0, n_points[1])]\n",
    "    \n",
    "    x_fix, theta_fix, x_dot_fix, theta_dot_fix = fixed_state\n",
    "\n",
    "    # Fix x_dot and theta_dot, plot value function over x and theta\n",
    "    grid = np.column_stack((xx.ravel(), yy.ravel(), \n",
    "                            x_dot_fix*np.ones_like(xx.ravel()), theta_dot_fix*np.ones_like(yy.ravel())))\n",
    "    learned_control = session.run(tf_actions, feed_dict={states: grid}).reshape(n_points)\n",
    "    true_control = session.run(tf_true_actions, feed_dict={states: grid}).reshape(n_points)\n",
    "    ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    ax.plot_surface(xx, yy, learned_control, color=colors[0], alpha=0.75)\n",
    "    ax.plot_surface(xx, yy, true_control, color=colors[1], alpha=0.5)\n",
    "    ax.set_title(r'Control, $\\dot{x} = %.3g,\\ \\dot{\\theta} = %.3g$' % (x_dot_fix, theta_dot_fix), fontsize=16)\n",
    "    ax.set_xlabel(r'$x$', fontsize=14)\n",
    "    ax.set_ylabel(r'$\\theta$', fontsize=14)\n",
    "    ax.set_zlabel(r'$u$', fontsize=14)\n",
    "    ax.view_init(elev=20., azim=15.)\n",
    "\n",
    "    # Fix x and theta, plot value function over x_dot and theta_dot\n",
    "    grid = np.column_stack((x_fix*np.ones_like(xx.ravel()), theta_fix*np.ones_like(yy.ravel()), \n",
    "                            xx.ravel(), yy.ravel()))\n",
    "    learned_control = session.run(tf_actions, feed_dict={states: grid}).reshape(n_points)\n",
    "    true_control = session.run(tf_true_actions, feed_dict={states: grid}).reshape(n_points)\n",
    "    ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    ax.plot_surface(xx, yy, learned_control, color=colors[0], alpha=0.75)\n",
    "    ax.plot_surface(xx, yy, true_control, color=colors[1], alpha=0.5)\n",
    "    ax.set_title(r'Control, $x = %.3g,\\ \\theta = %.3g$' % (x_fix, theta_fix), fontsize=16)\n",
    "    ax.set_xlabel(r'$\\dot{x}$', fontsize=14)\n",
    "    ax.set_ylabel(r'$\\dot{\\theta}$', fontsize=14)\n",
    "    ax.set_zlabel(r'$u$', fontsize=14)\n",
    "    ax.view_init(elev=20., azim=100.)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_closedloop_response(dynamics, policy1, policy2, steps, dt, reference='zero', const=1.0, ic=None,\n",
    "                             labels=['Policy 1','Policy 2'], colors=['r','b'], denormalize=False, show=True):\n",
    "    \n",
    "    state_dim = 4\n",
    "    state_traj1, action_traj1, t, _ = compute_closedloop_response(dynamics, policy1, state_dim,\n",
    "                                                                  steps, dt, reference, const, ic)\n",
    "    state_traj2, action_traj2, _, _ = compute_closedloop_response(dynamics, policy2, state_dim,\n",
    "                                                                  steps, dt, reference, const, ic)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 5), dpi=OPTIONS.dpi)\n",
    "    fig.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "    \n",
    "    if reference=='zero':\n",
    "        title_string = r'Zero-Input Response'\n",
    "    elif reference=='impulse':\n",
    "        title_string = r'Impulse Response'\n",
    "    elif reference=='step':\n",
    "        title_string = r'Step Response, $r = %.1gu_{max} = %.1g$ N' % (const, const*u_max)\n",
    "    \n",
    "    if ic is not None:\n",
    "        ic_tuple = (ic[0], ic[1], ic[2], ic[3])\n",
    "    else:\n",
    "        ic_tuple = (0, 0, 0, 0)\n",
    "    title_string = title_string + r', $s_0 = (%.1g, %.1g, %.1g, %.1g)$' % ic_tuple\n",
    "    fig.suptitle(title_string, fontsize=18)\n",
    "    \n",
    "    if denormalize:\n",
    "        state_names = [r'$x$ [m]', r'$\\theta$ [deg]', r'$\\dot{x}$ [m/s]', r'$\\dot{\\theta}$ [deg/s]']\n",
    "        state_traj1, action_traj1 = session.run(cart_pole.denormalize(state_traj1, action_traj1))\n",
    "        state_traj2, action_traj2 = session.run(cart_pole.denormalize(state_traj2, action_traj2))\n",
    "        for col in [1, 3]:\n",
    "            state_traj1[:, col] = np.rad2deg(state_traj1[:, col])\n",
    "            state_traj2[:, col] = np.rad2deg(state_traj2[:, col])\n",
    "    else:\n",
    "        state_names = [r'$x$', r'$\\theta$', r'$\\dot{x}$', r'$\\dot{\\theta}$']\n",
    "    \n",
    "    plot_idx = (1, 2, 4, 5)               \n",
    "    for i in range(state_dim):\n",
    "        ax = fig.add_subplot(2, 3, plot_idx[i])\n",
    "        ax.plot(t, state_traj1[:, i], colors[0])\n",
    "        ax.plot(t, state_traj2[:, i], colors[1])\n",
    "        ax.set_xlabel(r'$t$ [s]', fontsize=14)\n",
    "        ax.set_ylabel(state_names[i], fontsize=14)\n",
    "    ax = fig.add_subplot(2, 3, 3)\n",
    "    plot1 = ax.plot(t, action_traj1, color=colors[0])\n",
    "    plot2 = ax.plot(t, action_traj2, color=colors[1])\n",
    "    ax.set_xlabel(r'$t$ [s]', fontsize=14)\n",
    "    if denormalize:\n",
    "        ax.set_ylabel(r'$u$ [N]', fontsize=14)\n",
    "    else:\n",
    "        ax.set_ylabel(r'$u$', fontsize=14)\n",
    "    fig.legend((plot1[0], plot2[0]), (labels[0], labels[1]), loc=(0.75, 0.2), fontsize=14)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "def gridify(norms, maxes=None, num_points=25):    \n",
    "    norms = np.asarray(norms).ravel()\n",
    "    if maxes is None:\n",
    "        maxes = norms\n",
    "    else:\n",
    "        maxes = np.asarray(maxes).ravel()\n",
    "    limits = np.column_stack((- maxes / norms, maxes / norms))\n",
    "    \n",
    "    if isinstance(num_points, int):\n",
    "        num_points = [num_points, ] * len(norms)\n",
    "    grid = safe_learning.GridWorld(limits, num_points)\n",
    "    return grid\n",
    "\n",
    "\n",
    "def compute_roa(grid, closed_loop_dynamics, horizon=250, tol=1e-3, equilibrium=None, no_traj=True):\n",
    "    if isinstance(grid, np.ndarray):\n",
    "        all_points = grid\n",
    "        nindex = grid.shape[0]\n",
    "        ndim = grid.shape[1]\n",
    "    else:\n",
    "        all_points = grid.all_points\n",
    "        nindex = grid.nindex\n",
    "        ndim = grid.ndim\n",
    "    \n",
    "    # Forward-simulate all trajectories from initial points in the discretization\n",
    "    if no_traj:\n",
    "        end_states = all_points\n",
    "        for t in range(1, horizon):\n",
    "            end_states = closed_loop_dynamics(end_states)\n",
    "    else:\n",
    "        trajectories = np.empty((nindex, ndim, horizon))\n",
    "        trajectories[:, :, 0] = all_points\n",
    "        for t in range(1, horizon):\n",
    "            trajectories[:, :, t] = closed_loop_dynamics(trajectories[:, :, t - 1])\n",
    "        end_states = trajectories[:, :, -1]\n",
    "            \n",
    "    if equilibrium is None:\n",
    "        equilibrium = np.zeros((1, ndim))\n",
    "    \n",
    "    # Compute an approximate ROA as all states that end up \"close\" to 0\n",
    "    dists = np.linalg.norm(end_states - equilibrium, ord=2, axis=1, keepdims=True).ravel()\n",
    "    roa = (dists <= tol)\n",
    "    if no_traj:\n",
    "        return roa, dists\n",
    "    else:\n",
    "        return roa, dists, trajectories\n",
    "\n",
    "\n",
    "def estimate_cost(grid, closed_loop_dynamics, cost_function, discount, horizon=250, tol=1e-3):\n",
    "    # Estimate true cost function using a finite-horizon rollout\n",
    "    converged = False\n",
    "    if isinstance(grid, safe_learning.GridWorld):\n",
    "        rollout = np.zeros(grid.nindex)\n",
    "        current_states = grid.all_points\n",
    "    else:\n",
    "        rollout = np.zeros(grid.shape[0])\n",
    "        current_states = grid\n",
    "        \n",
    "    for t in range(horizon):\n",
    "        temp = (discount ** t) * cost_function(current_states).ravel()\n",
    "        rollout += temp\n",
    "        if np.max(np.abs(temp)) < tol:\n",
    "            converged = True\n",
    "            break\n",
    "        current_states = closed_loop_dynamics(current_states)\n",
    "    if converged:\n",
    "        print('Cost converged after {} steps!'.format(t + 1))\n",
    "    else:\n",
    "        print('Cost did not converge!')\n",
    "            \n",
    "    return rollout\n",
    "\n",
    "\n",
    "def find_nearest(array, value, sorted_1d=True):\n",
    "    if not sorted_1d:\n",
    "        array = np.sort(array)\n",
    "    idx = np.searchsorted(array, value, side='left')\n",
    "    if idx > 0 and (idx == len(array) or np.abs(value - array[idx - 1]) < np.abs(value - array[idx])):\n",
    "        idx -= 1\n",
    "    return idx, array[idx]\n",
    "\n",
    "\n",
    "def plot_cost(cost, discretization, norms, masks, roa=None):\n",
    "    planes = [[0, 2], [1, 3]]\n",
    "    limits = np.asarray(norms).reshape((-1, 1)) * discretization.limits\n",
    "    \n",
    "    plt.rc('font', size=10)\n",
    "    fig = plt.figure(figsize=(6, 12), dpi=OPTIONS.dpi)\n",
    "#     fig.subplots_adjust(wspace=0.4, hspace=0.2)\n",
    "    \n",
    "    for i, p in enumerate(planes):\n",
    "        if isinstance(cost, list):\n",
    "            z = cost[i].ravel()\n",
    "        else:\n",
    "            z = cost(discretization.all_points[masks[i]]).eval()\n",
    "        z = z.reshape(discretization.num_points[p])\n",
    "        \n",
    "        scaled_discrete_points = [norm * points for norm, points in zip(norms, grid.discrete_points)]\n",
    "        xx, yy = np.meshgrid(*[scaled_discrete_points[p[0]], scaled_discrete_points[p[1]]])\n",
    "        \n",
    "        ax = fig.add_subplot(211 + i, projection='3d')        \n",
    "        surf = ax.plot_surface(xx, yy, z, color='blue', alpha=0.65) #, label=r'$J_{\\bf \\theta}({\\bf x})$')\n",
    "        surf._facecolors2d = surf._facecolors3d\n",
    "        surf._edgecolors2d = surf._edgecolors3d\n",
    "        \n",
    "        if roa is not None:\n",
    "            z = roa[i].reshape(discretization.num_points[p])\n",
    "            ax.contourf(xx, yy, z, cmap=BINARY_MAP, zdir='z', offset=0)\n",
    "    \n",
    "        if i == 0:\n",
    "            ax.set_xlabel(r'$x$ [m]')\n",
    "            ax.set_ylabel(r'$\\dot{x}$ [m/s]')  \n",
    "        else:\n",
    "            ax.set_xlabel(r'$\\phi$ [deg]')\n",
    "            ax.set_ylabel(r'$\\dot{\\phi}$ [deg/s]')\n",
    "        ax.view_init(None, -45)\n",
    " \n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System parameters\n",
    "# m = 0.1     # pendulum mass\n",
    "# M = 1.5     # cart mass\n",
    "# L = 0.2     # pole length\n",
    "# b = 0.0     # rotational friction\n",
    "m = 0.175    # pendulum mass\n",
    "M = 1.732    # cart mass\n",
    "L = 0.28     # pole length\n",
    "b = 0.01      # rotational friction\n",
    "\n",
    "# Constants\n",
    "dt = 0.01   # sampling time\n",
    "g = 9.81    # gravity\n",
    "\n",
    "# State and action normalizers\n",
    "x_max         = 0.5\n",
    "theta_max     = np.deg2rad(30)\n",
    "x_dot_max     = 2\n",
    "theta_dot_max = np.deg2rad(30)\n",
    "# theta_dot_max = np.deg2rad(30)\n",
    "# u_max         = (m + M) * x_dot_max / (5 * dt)\n",
    "# u_max         = (M + m) * g * np.tan(theta_max)\n",
    "u_max         = (m + M) * (x_dot_max ** 2) / x_max\n",
    "\n",
    "state_norm = (x_max, theta_max, x_dot_max, theta_dot_max)\n",
    "action_norm = (u_max,)\n",
    "\n",
    "# Define system and dynamics\n",
    "cart_pole = CartPole(m, M, L, b, dt, [state_norm, action_norm])\n",
    "state_dim = 4\n",
    "action_dim = 1\n",
    "\n",
    "state_limits = np.array([[-1., 1.]]*cart_pole.state_dim)\n",
    "action_limits = np.array([[-1., 1.]]*cart_pole.action_dim)\n",
    "\n",
    "A, B = cart_pole.linearize()   \n",
    "dynamics = safe_learning.functions.LinearSystem((A, B), name='dynamics')\n",
    "\n",
    "print(state_norm)\n",
    "print(action_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State cost matrix\n",
    "Q = np.diag([0.1, 0.1, 0.1, 0.1])\n",
    "\n",
    "# Action cost matrix\n",
    "R = 0.1 * np.identity(action_dim)\n",
    "\n",
    "# Quadratic reward (-cost) function\n",
    "reward_function = safe_learning.QuadraticFunction(block_diag(-Q, -R), name='reward_function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric Policy and Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy\n",
    "layer_dims = [64, 64, action_dim]\n",
    "activations = [tf.nn.relu, tf.nn.relu, None]\n",
    "if OPTIONS.saturate:\n",
    "    activations[-1] = tf.nn.tanh\n",
    "policy = safe_learning.functions.NeuralNetwork(layer_dims, activations, name='policy', use_bias=False)\n",
    "\n",
    "# Value function\n",
    "layer_dims = [64, 64, 1]\n",
    "activations = [tf.nn.relu, tf.nn.relu, None]\n",
    "value_function = safe_learning.functions.NeuralNetwork(layer_dims, activations, name='value_function', use_bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LQR Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K, P = safe_learning.utilities.dlqr(A, B, Q, R)\n",
    "policy_lqr = safe_learning.functions.LinearSystem((-K, ), name='policy_lqr')\n",
    "if OPTIONS.saturate:\n",
    "    policy_lqr = safe_learning.Saturation(policy_lqr, -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Graph and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use parametric policy and value function\n",
    "states = tf.placeholder(OPTIONS.tf_dtype, shape=[None, state_dim], name='states')\n",
    "actions = policy(states)\n",
    "rewards = reward_function(states, actions)\n",
    "values = value_function(states)\n",
    "future_states = dynamics(states, actions)\n",
    "future_values = value_function(future_states)\n",
    "\n",
    "# Compare with LQR solution, possibly with saturation constraints\n",
    "actions_lqr = policy_lqr(states)\n",
    "rewards_lqr = reward_function(states, actions_lqr)\n",
    "future_states_lqr = dynamics(states, actions_lqr)\n",
    "\n",
    "# Discount factor and scaling\n",
    "max_state = np.ones((1, state_dim))\n",
    "max_action = np.ones((1, action_dim))\n",
    "r_max = np.linalg.multi_dot((max_state, Q, max_state.T)) + np.linalg.multi_dot((max_action, R, max_action.T))\n",
    "gamma = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='discount_factor')\n",
    "\n",
    "val_scaling = 1 / r_max.ravel()\n",
    "pol_scaling = (1 - gamma) / r_max.ravel()\n",
    "\n",
    "# Policy evaluation\n",
    "with tf.name_scope('value_optimization'):\n",
    "    value_learning_rate = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='learning_rate')\n",
    "    target = tf.stop_gradient(rewards + gamma * future_values, name='target')\n",
    "    value_objective = pol_scaling * tf.reduce_mean(tf.abs(values - target), name='objective')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(value_learning_rate)\n",
    "    value_update = optimizer.minimize(value_objective, var_list=value_function.parameters)\n",
    "\n",
    "# Policy improvement\n",
    "with tf.name_scope('policy_optimization'):\n",
    "    lagrange_multiplier = tf.placeholder(tf_dtype, shape=[], name='lagrange_multiplier')\n",
    "    policy_learning_rate = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='learning_rate')\n",
    "    policy_objective = - pol_scaling * tf.reduce_mean(rewards + gamma * future_values, name='objective') # + lagrange_multiplier * policy.lipschitz()\n",
    "    optimizer = tf.train.GradientDescentOptimizer(policy_learning_rate)\n",
    "    policy_update = optimizer.minimize(policy_objective, var_list=policy.parameters)\n",
    "    \n",
    "# Sampling    \n",
    "with tf.name_scope('state_sampler'):\n",
    "    batch_size = tf.placeholder(tf.int32, shape=[], name='batch_size')\n",
    "    batch = tf.random_uniform([batch_size, state_dim], -1, 1, dtype=OPTIONS.tf_dtype, name='batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "value_obj_history = np.zeros(0)\n",
    "value_param_history = np.zeros(0)\n",
    "policy_obj_history = np.zeros(0)\n",
    "policy_param_history = np.zeros(0)\n",
    "\n",
    "# Uniformly distributed test set\n",
    "test_size = 1e3\n",
    "grid_length = np.power(test_size, 1 / state_dim)\n",
    "grid_length = int(2*np.floor(grid_length / 2) + 1)\n",
    "test_set = safe_learning.GridWorld(state_limits, [grid_length,]*state_dim).all_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "max_iters    = 200\n",
    "value_iters  = 100\n",
    "policy_iters = 10\n",
    "\n",
    "feed_dict = {\n",
    "    states:                test_set,\n",
    "    gamma:                 0.99,\n",
    "    value_learning_rate:   0.2,\n",
    "    policy_learning_rate:  0.5,\n",
    "    batch_size:            1e2,\n",
    "    lagrange_multiplier:   0.\n",
    "}\n",
    "# batch = constrained_batch_sampler(dynamics, policy, state_dim, feed_dict[batch_size], action_limit=0.999, zero_pad=0)\n",
    "\n",
    "# Record objective values over time\n",
    "value_obj_eval = np.zeros(max_iters + 1)\n",
    "policy_obj_eval = np.zeros(max_iters + 1)\n",
    "value_obj_eval[0] = value_objective.eval(feed_dict)\n",
    "policy_obj_eval[0] = policy_objective.eval(feed_dict)\n",
    "\n",
    "# For convergence, check the parameter values\n",
    "converged = False\n",
    "value_param_changes = np.zeros(max_iters)\n",
    "policy_param_changes = np.zeros(max_iters)\n",
    "old_value_params = session.run(value_function.parameters)\n",
    "old_policy_params = session.run(policy.parameters)\n",
    "\n",
    "\n",
    "for i in tqdm(range(max_iters)):\n",
    "    # Policy evaluation (value update)\n",
    "    for _ in range(value_iters):\n",
    "        feed_dict[states] = batch.eval(feed_dict)\n",
    "        session.run(value_update, feed_dict)\n",
    "    new_value_params = session.run(value_function.parameters)\n",
    "    value_param_changes[i] = get_parameter_change(old_value_params, new_value_params)\n",
    "    old_value_params = new_value_params\n",
    "\n",
    "    # Policy improvement (policy update)\n",
    "    for _ in range(policy_iters):\n",
    "        feed_dict[states] = batch.eval(feed_dict)\n",
    "        session.run(policy_update, feed_dict)\n",
    "    new_policy_params = session.run(policy.parameters)\n",
    "    policy_param_changes[i] = get_parameter_change(old_policy_params, new_policy_params)\n",
    "    old_policy_params = new_policy_params\n",
    "    \n",
    "    # Record objectives\n",
    "    feed_dict[states] = test_set\n",
    "    value_obj_eval[i+1] = value_objective.eval(feed_dict)\n",
    "    policy_obj_eval[i+1] = policy_objective.eval(feed_dict)\n",
    "    \n",
    "    # TODO debugging    \n",
    "    if np.isnan(value_obj_eval[i+1]) or np.isnan(policy_obj_eval[i+1]):\n",
    "        raise ValueError('Encountered NAN value after {} iterations!'.format(i+1))\n",
    "\n",
    "final_iter           = i + 1\n",
    "value_obj_history    = np.concatenate((value_obj_history, value_obj_eval[:final_iter+1]))\n",
    "value_param_history  = np.concatenate((value_param_history, value_param_changes[:final_iter+1]))\n",
    "policy_obj_history   = np.concatenate((policy_obj_history, policy_obj_eval[:final_iter+1]))\n",
    "policy_param_history = np.concatenate((policy_param_history, policy_param_changes[:final_iter+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=12)\n",
    "fig = plt.figure(figsize=(6, 5), dpi=200)\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "ax = fig.add_subplot(211)\n",
    "ax.plot(value_obj_history, '.-r')\n",
    "ax.set_xlabel(r'Policy iteration $k$')\n",
    "ax.set_ylabel(r'$G(\\mathcal{X}_v, {\\bf \\theta}_k)$')\n",
    "ax.set_ylim([0, 0.06])\n",
    "\n",
    "ax = fig.add_subplot(212)\n",
    "ax.plot(value_param_history, '.-r')\n",
    "ax.set_xlabel(r'Policy iteration $k$')\n",
    "ax.set_ylabel(r'$||{\\bf \\theta}_k - {\\bf \\theta}_{k-1}||_\\infty$')\n",
    "ax.set_ylim([0, 0.8])\n",
    "\n",
    "if OPTIONS.save_figs:\n",
    "    gamma_string = str(feed_dict[gamma])[2:]\n",
    "    fig.savefig(OPTIONS.fig_path + 'cartpole_policyiter_costfunc_training_gamma' + gamma_string + '.pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "#\n",
    "fig = plt.figure(figsize=(6, 5), dpi=200)\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "ax = fig.add_subplot(211)\n",
    "ax.plot(policy_obj_history, '.-r')\n",
    "ax.set_xlabel(r'Policy iteration $k$')\n",
    "ax.set_ylabel(r'$H(\\mathcal{X}_v, {\\bf \\delta}_k)$')\n",
    "ax.set_ylim([0, 3])\n",
    "\n",
    "ax = fig.add_subplot(212)\n",
    "ax.plot(policy_param_history, '.-r')\n",
    "ax.set_xlabel(r'Policy iteration $k$')\n",
    "ax.set_ylabel(r'$||{\\bf \\delta}_k - {\\bf \\delta}_{k-1}||_\\infty$')\n",
    "ax.set_ylim([0, 0.1])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if OPTIONS.save_figs:\n",
    "    gamma_string = str(feed_dict[gamma])[2:]\n",
    "    fig.savefig(OPTIONS.fig_path + 'cartpole_policyiter_policy_training_gamma' + gamma_string + '.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimated Value Functions and ROAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid for plotting\n",
    "N = 51\n",
    "norms = np.asarray([x_max, np.rad2deg(theta_max), x_dot_max, np.rad2deg(theta_dot_max)])\n",
    "maxes = np.copy(norms)\n",
    "grid = gridify(norms, maxes, N)\n",
    "\n",
    "# Estimate value functions and ROAs with rollout\n",
    "roa_horizon  = 5000\n",
    "cost_horizon = 500\n",
    "roa_tol      = 0.1\n",
    "cost_tol     = 0.01\n",
    "discount     = feed_dict[gamma]\n",
    "fixed_state  = [0., 0., 0., 0.]\n",
    "\n",
    "# Snap fixed_state to the closest grid point\n",
    "fixed_state = np.asarray(fixed_state, dtype=OPTIONS.np_dtype)\n",
    "fixed_index = np.zeros_like(fixed_state, dtype=int)\n",
    "for d in range(grid.ndim):\n",
    "    fixed_index[d], fixed_state[d] = find_nearest(grid.discrete_points[d], fixed_state[d])\n",
    "\n",
    "# Get 2d-planes of the discretization (x vs. v, theta vs. omega) according to fixed_state\n",
    "planes = [[1, 3], [0, 2]]\n",
    "grid_slices = []\n",
    "for p in planes:\n",
    "    grid_slices.append(np.logical_and(grid.all_points[:, p[0]] == fixed_state[p[0]], \n",
    "                                      grid.all_points[:, p[1]] == fixed_state[p[1]]).ravel())\n",
    "\n",
    "# LQR solution\n",
    "closed_loop_dynamics = lambda x: future_states_lqr.eval({states: x})\n",
    "cost_function        = lambda x: - rewards_lqr.eval({states: x})\n",
    "true_costs           = [estimate_cost(grid.all_points[mask], closed_loop_dynamics, cost_function, discount, cost_horizon, cost_tol) \n",
    "                        for mask in grid_slices]\n",
    "# true_costs           = [c / c.max() for c in true_costs]\n",
    "true_roas            = [compute_roa(grid.all_points[mask], closed_loop_dynamics, roa_horizon, roa_tol)[0]\n",
    "                        for mask in grid_slices]\n",
    "\n",
    "# Parametric policy's value function\n",
    "closed_loop_dynamics = lambda x: future_states.eval({states: x})\n",
    "cost_function        = lambda x: - rewards.eval({states: x})\n",
    "est_costs            = [estimate_cost(grid.all_points[mask], closed_loop_dynamics, cost_function, discount, cost_horizon, cost_tol) \n",
    "                        for mask in grid_slices]\n",
    "# est_costs            = [c / c.max() for c in est_costs]\n",
    "est_roas             = [compute_roa(grid.all_points[mask], closed_loop_dynamics, roa_horizon, roa_tol)[0]\n",
    "                        for mask in grid_slices]\n",
    "\n",
    "# Parametric value function\n",
    "par_costs = [- values.eval({states: grid.all_points[mask]}) for mask in grid_slices]\n",
    "# par_costs = [c / c.max() for c in par_costs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "# plot_cost(true_costs, grid, norms, grid_slices, true_roas)\n",
    "# plot_cost(est_costs, grid, norms, grid_slices, est_roas)\n",
    "\n",
    "planes = [[0, 2], [1, 3]]\n",
    "limits = np.asarray(norms).reshape((-1, 1)) * grid.limits\n",
    "scaled_discrete_points = [norm * points for norm, points in zip(norms, grid.discrete_points)]\n",
    "\n",
    "plt.rc('font', size=20)\n",
    "pad = 20\n",
    "fig = plt.figure(figsize=(10, 16), dpi=OPTIONS.dpi)\n",
    "# fig.subplots_adjust(wspace=0.4, hspace=0.2)\n",
    "\n",
    "for i, p in enumerate(planes):\n",
    "    ax = fig.add_subplot(211 + i, projection='3d')\n",
    "    if i == 0:\n",
    "        ax.set_title(r'$\\phi = \\dot{\\phi} = 0$' + '\\n')\n",
    "        ax.set_xlabel(r'$x$ [m]', labelpad=pad)\n",
    "        ax.set_ylabel(r'$\\dot{x}$ [m/s]', labelpad=pad)\n",
    "        ax.xaxis.set_ticks(np.arange(-x_max, 1.01 * x_max, 0.25))\n",
    "        ax.yaxis.set_ticks(np.arange(-x_dot_max, 1.01 * x_dot_max, 1))\n",
    "    else:\n",
    "        ax.set_title(r'$x = \\dot{x} = 0$' + '\\n')\n",
    "        ax.set_xlabel(r'$\\phi$ [deg]', labelpad=pad)\n",
    "        ax.set_ylabel(r'$\\dot{\\phi}$ [deg/s]', labelpad=pad)\n",
    "    ax.view_init(None, -45)\n",
    "\n",
    "    xx, yy = np.meshgrid(*[scaled_discrete_points[p[0]], scaled_discrete_points[p[1]]])\n",
    "\n",
    "    for j, (costs, roas, color) in enumerate(zip([true_costs, est_costs, par_costs], \n",
    "                                                 [true_roas, est_roas, None],\n",
    "                                                 [(0, 0, 1, 0.6), (0, 1, 0, 0.8), (1, 0, 0, 0.65)])):\n",
    "        z = costs[i].reshape(grid.num_points[p])\n",
    "#         z /= z.max()\n",
    "        surf = ax.plot_surface(xx, yy, z, color=color) #, label=r'$J_{\\bf \\theta}({\\bf x})$')\n",
    "        surf._facecolors2d = surf._facecolors3d\n",
    "        surf._edgecolors2d = surf._edgecolors3d\n",
    "#         if roas is not None:\n",
    "#             z = roas[i].reshape(grid.num_points[p])\n",
    "#             ax.contourf(xx, yy, z, cmap=binary_cmap(color, 0.5), zdir='z', offset=0)\n",
    "    proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in [(0, 0, 1, 0.6), (0, 1, 0, 0.8), (1, 0, 0, 0.65)]]    \n",
    "    ax.legend(proxy, [r'$J_{\\pi}({\\bf x})$', r'$J_{\\pi_{\\bf \\delta}}({\\bf x})$', r'$J_{\\bf \\theta}({\\bf x})$'], \n",
    "              loc=(0.85, 0.85))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if OPTIONS.save_figs:\n",
    "    gamma_string = str(feed_dict[gamma])[2:]\n",
    "    fig.savefig(OPTIONS.fig_path + 'cartpole_policyiter_costfunc_gamma' + gamma_string + '.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=20)\n",
    "pad = 20\n",
    "fig = plt.figure(figsize=(10, 16), dpi=OPTIONS.dpi)\n",
    "\n",
    "for i, (p, mask) in enumerate(zip(planes, grid_slices)):\n",
    "    ax = fig.add_subplot(211 + i, projection='3d')\n",
    "    if i == 0:\n",
    "        ax.set_title(r'$\\phi = \\dot{\\phi} = 0$' + '\\n')\n",
    "        ax.set_xlabel(r'$x$ [m]', labelpad=pad)\n",
    "        ax.set_ylabel(r'$\\dot{x}$ [m/s]', labelpad=pad) \n",
    "        ax.xaxis.set_ticks(np.arange(-x_max, 1.01 * x_max, 0.25))\n",
    "        ax.yaxis.set_ticks(np.arange(-x_dot_max, 1.01 * x_dot_max, 1))\n",
    "    else:\n",
    "        ax.set_title(r'$x = \\dot{x} = 0$' + '\\n')\n",
    "        ax.set_xlabel(r'$\\phi$ [deg]', labelpad=pad)\n",
    "        ax.set_ylabel(r'$\\dot{\\phi}$ [deg/s]', labelpad=pad)\n",
    "    ax.view_init(None, -45)\n",
    "    \n",
    "    xx, yy = np.meshgrid(*[scaled_discrete_points[p[0]], scaled_discrete_points[p[1]]])\n",
    "    acts = u_max * actions.eval({states: grid.all_points[mask]})\n",
    "    true_acts = u_max * actions_lqr.eval({states: grid.all_points[mask]})\n",
    "\n",
    "    ax.plot_surface(xx, yy, true_acts.reshape(grid.num_points[p]), color='blue', alpha=0.55)\n",
    "    ax.plot_surface(xx, yy, acts.reshape(grid.num_points[p]), color='red', alpha=0.75)\n",
    "\n",
    "    z = est_roas[i].reshape(grid.num_points[p])\n",
    "    ax.contourf(xx, yy, z, cmap=binary_cmap('green', 0.65), zdir='z', offset=-u_max)\n",
    "    \n",
    "    ax.tick_params(axis='z', which='major', pad=10)\n",
    "    proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in [(0, 0, 1, 0.6), (1, 0, 0, 0.65), (0., 1., 0., 0.65)]]\n",
    "    ax.legend(proxy, [r'$\\pi({\\bf x})$ [N]', r'$\\pi_{\\bf \\delta}({\\bf x})$ [N]', r'ROA for $\\pi_{\\bf \\delta}$'], \n",
    "              loc=(0.85, 0.85))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if OPTIONS.save_figs:\n",
    "    gamma_string = str(feed_dict[gamma])[2:]\n",
    "    fig.savefig(OPTIONS.fig_path + 'cartpole_policyiter_policy_gamma' + gamma_string + '.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full ROA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid for ROA computation\n",
    "# N = 11\n",
    "# norms = np.asarray([x_max, np.rad2deg(theta_max), x_dot_max, np.rad2deg(theta_dot_max)])\n",
    "# maxes = np.copy(norms)\n",
    "# disc = gridify(norms, maxes, N)\n",
    "\n",
    "# #\n",
    "# closed_loop_dynamics = lambda x: future_states.eval({states: x})\n",
    "# roa_horizon          = 3000\n",
    "# roa_tol              = 0.1\n",
    "\n",
    "# #\n",
    "# roa, _ = compute_roa(disc.all_points, closed_loop_dynamics, roa_horizon, roa_tol)\n",
    "# roa_size = roa.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roa_size = roa.sum()\n",
    "\n",
    "# print(disc.nindex)\n",
    "# print(roa_size)\n",
    "# print(roa_size / disc.nindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Input and Step Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic = np.array([0.1, 0.2, 0.1, 0.1])\n",
    "const = 1 / u_max\n",
    "steps = 1000\n",
    "\n",
    "print(session.run(actions, {states: np.zeros([1, state_dim])}))\n",
    "print(session.run(values, {states: np.zeros([1, state_dim])}))\n",
    "\n",
    "# Zero-input response, zero initial condition\n",
    "plot_closedloop_response(dynamics, policy, policy_lqr, steps, dt, 'zero', ic=None, labels=['Learned','True'], denormalize=True)\n",
    "\n",
    "# Zero-input response, non-zero initial condition\n",
    "plot_closedloop_response(dynamics, policy, policy_lqr, steps, dt, 'zero', ic=ic, labels=['Learned','True'], denormalize=False)\n",
    "\n",
    "# Step response\n",
    "# plot_closedloop_response(dynamics, policy, policy_lqr, steps, dt, 'step', const=const, labels=['Learned','True'], denormalize=False)\n",
    "\n",
    "# Impulse response\n",
    "# plot_closedloop_response(dynamics, policy, lqr_policy, steps, dt, 'impulse', labels=['Learned','True'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting.show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
