{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for a Cart-Pole System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.linalg import block_diag\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "import safe_learning\n",
    "import plotting\n",
    "from utilities import (sample_box, sample_box_boundary, sample_ellipsoid, constrained_batch_sampler,\n",
    "                       CartPole, compute_closedloop_response, get_max_parameter_change)\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x\n",
    "    \n",
    "np_dtype = safe_learning.config.np_dtype\n",
    "tf_dtype = safe_learning.config.dtype\n",
    "\n",
    "tf.reset_default_graph()\n",
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "initialized = False\n",
    "\n",
    "\n",
    "# TODO testing ****************************************#\n",
    "\n",
    "train_policy = True\n",
    "\n",
    "train_value_function = True\n",
    "\n",
    "clip_states = False\n",
    "\n",
    "saturate = True\n",
    "\n",
    "#******************************************************#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_function(tf_learned_values, tf_true_values, n_points, fixed_state, colors=['r','b'], show=True):\n",
    "    fig = plt.figure(figsize=(12, 5), dpi=200)\n",
    "    fig.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "    xx, yy = np.mgrid[-1:1:np.complex(0, n_points[0]), -1:1:np.complex(0, n_points[1])]\n",
    "    \n",
    "    x_fix, theta_fix, x_dot_fix, theta_dot_fix = fixed_state\n",
    "\n",
    "    # Fix x_dot and theta_dot, plot value function over x and theta\n",
    "    grid = np.column_stack((xx.ravel(), yy.ravel(), \n",
    "                            x_dot_fix*np.ones_like(xx.ravel()), theta_dot_fix*np.ones_like(yy.ravel())))\n",
    "    learned_values = session.run(tf_learned_values, feed_dict={states: grid}).reshape(n_points)\n",
    "    true_values = session.run(tf_true_values, feed_dict={states: grid}).reshape(n_points)\n",
    "    ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    ax.plot_surface(xx, yy, learned_values, color=colors[0], alpha=0.75)\n",
    "    ax.plot_surface(xx, yy, true_values, color=colors[1], alpha=0.5)\n",
    "    ax.set_title(r'Value function, $\\dot{x} = %.3g,\\ \\dot{\\theta} = %.3g$' % (x_dot_fix, theta_dot_fix), fontsize=16)\n",
    "    ax.set_xlabel(r'$x$', fontsize=14)\n",
    "    ax.set_ylabel(r'$\\theta$', fontsize=14)\n",
    "    ax.set_zlabel(r'$V(s)$', fontsize=14)\n",
    "\n",
    "    # Fix x and theta, plot value function over x_dot and theta_dot\n",
    "    grid = np.column_stack((x_fix*np.ones_like(xx.ravel()), theta_fix*np.ones_like(yy.ravel()), \n",
    "                            xx.ravel(), yy.ravel()))\n",
    "    learned_values = session.run(tf_learned_values, feed_dict={states: grid}).reshape(n_points)\n",
    "    true_values = session.run(tf_true_values, feed_dict={states: grid}).reshape(n_points)\n",
    "    ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    ax.plot_surface(xx, yy, learned_values, color=colors[0], alpha=0.75)\n",
    "    ax.plot_surface(xx, yy, true_values, color=colors[1], alpha=0.5)\n",
    "    ax.set_title(r'Value function, $x = %.3g,\\ \\theta = %.3g$' % (x_fix, theta_fix), fontsize=16)\n",
    "    ax.set_xlabel(r'$\\dot{x}$', fontsize=14)\n",
    "    ax.set_ylabel(r'$\\dot{\\theta}$', fontsize=14)\n",
    "    ax.set_zlabel(r'$V(s)$', fontsize=14)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "def plot_policy(tf_actions, tf_true_actions, n_points, fixed_state, colors=['r','b'], show=True):\n",
    "    fig = plt.figure(figsize=(12, 5), dpi=200)\n",
    "    fig.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "    xx, yy = np.mgrid[-1:1:np.complex(0, n_points[0]), -1:1:np.complex(0, n_points[1])]\n",
    "    \n",
    "    x_fix, theta_fix, x_dot_fix, theta_dot_fix = fixed_state\n",
    "\n",
    "    # Fix x_dot and theta_dot, plot value function over x and theta\n",
    "    grid = np.column_stack((xx.ravel(), yy.ravel(), \n",
    "                            x_dot_fix*np.ones_like(xx.ravel()), theta_dot_fix*np.ones_like(yy.ravel())))\n",
    "    learned_control = session.run(tf_actions, feed_dict={states: grid}).reshape(n_points)\n",
    "    true_control = session.run(tf_true_actions, feed_dict={states: grid}).reshape(n_points)\n",
    "    ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    ax.plot_surface(xx, yy, learned_control, color=colors[0], alpha=0.75)\n",
    "    ax.plot_surface(xx, yy, true_control, color=colors[1], alpha=0.5)\n",
    "    ax.set_title(r'Control, $\\dot{x} = %.3g,\\ \\dot{\\theta} = %.3g$' % (x_dot_fix, theta_dot_fix), fontsize=16)\n",
    "    ax.set_xlabel(r'$x$', fontsize=14)\n",
    "    ax.set_ylabel(r'$\\theta$', fontsize=14)\n",
    "    ax.set_zlabel(r'$u$', fontsize=14)\n",
    "    ax.view_init(elev=20., azim=15.)\n",
    "\n",
    "    # Fix x and theta, plot value function over x_dot and theta_dot\n",
    "    grid = np.column_stack((x_fix*np.ones_like(xx.ravel()), theta_fix*np.ones_like(yy.ravel()), \n",
    "                            xx.ravel(), yy.ravel()))\n",
    "    learned_control = session.run(tf_actions, feed_dict={states: grid}).reshape(n_points)\n",
    "    true_control = session.run(tf_true_actions, feed_dict={states: grid}).reshape(n_points)\n",
    "    ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    ax.plot_surface(xx, yy, learned_control, color=colors[0], alpha=0.75)\n",
    "    ax.plot_surface(xx, yy, true_control, color=colors[1], alpha=0.5)\n",
    "    ax.set_title(r'Control, $x = %.3g,\\ \\theta = %.3g$' % (x_fix, theta_fix), fontsize=16)\n",
    "    ax.set_xlabel(r'$\\dot{x}$', fontsize=14)\n",
    "    ax.set_ylabel(r'$\\dot{\\theta}$', fontsize=14)\n",
    "    ax.set_zlabel(r'$u$', fontsize=14)\n",
    "    ax.view_init(elev=20., azim=100.)\n",
    "\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System parameters\n",
    "m = 0.1     # pendulum mass\n",
    "M = 1.5     # cart mass\n",
    "L = 0.2     # pole length\n",
    "b = 0.0     # rotational friction\n",
    "\n",
    "# Constants\n",
    "dt = 0.01   # sampling time\n",
    "g = 9.81    # gravity\n",
    "\n",
    "# State and action normalizers\n",
    "x_max = 0.25\n",
    "theta_max = np.deg2rad(15)\n",
    "x_dot_max = 0.5\n",
    "theta_dot_max = np.deg2rad(15)\n",
    "u_max = (m + M)*x_dot_max / (5*dt)\n",
    "\n",
    "state_norm = (x_max, theta_max, x_dot_max, theta_dot_max)\n",
    "action_norm = (u_max,)\n",
    "\n",
    "# Define system and dynamics\n",
    "cart_pole = CartPole(m, M, L, b, dt, [state_norm, action_norm])\n",
    "state_dim = 4\n",
    "action_dim = 1\n",
    "\n",
    "state_limits = np.array([[-1., 1.]]*cart_pole.state_dim)\n",
    "action_limits = np.array([[-1., 1.]]*cart_pole.action_dim)\n",
    "\n",
    "A, B = cart_pole.linearize()   \n",
    "dynamics = safe_learning.functions.LinearSystem((A, B), name='dynamics')\n",
    "\n",
    "\n",
    "def plot_closedloop_response(dynamics, policy1, policy2, steps, dt, reference='zero', const=1.0, ic=None,\n",
    "                             labels=['Policy 1','Policy 2'], colors=['r','b'], denormalize=False, show=True):\n",
    "    \n",
    "    state_dim = 4\n",
    "    state_traj1, action_traj1, t, _ = compute_closedloop_response(dynamics, policy1, state_dim,\n",
    "                                                                  steps, dt, reference, const, ic)\n",
    "    state_traj2, action_traj2, _, _ = compute_closedloop_response(dynamics, policy2, state_dim,\n",
    "                                                                  steps, dt, reference, const, ic)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 5), dpi=200)\n",
    "    fig.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "    \n",
    "    if reference=='zero':\n",
    "        title_string = r'Zero-Input Response'\n",
    "    elif reference=='impulse':\n",
    "        title_string = r'Impulse Response'\n",
    "    elif reference=='step':\n",
    "        title_string = r'Step Response, $r = %.1gu_{max} = %.1g$ N' % (const, const*u_max)\n",
    "    \n",
    "    if ic is not None:\n",
    "        ic_tuple = (ic[0], ic[1], ic[2], ic[3])\n",
    "    else:\n",
    "        ic_tuple = (0, 0, 0, 0)\n",
    "    title_string = title_string + r', $s_0 = (%.1g, %.1g, %.1g, %.1g)$' % ic_tuple\n",
    "    fig.suptitle(title_string, fontsize=18)\n",
    "    \n",
    "    if denormalize:\n",
    "        state_names = [r'$x$ [m]', r'$\\theta$ [deg]', r'$\\dot{x}$ [m/s]', r'$\\dot{\\theta}$ [deg/s]']\n",
    "        state_traj1, action_traj1 = session.run(cart_pole.denormalize(state_traj1, action_traj1))\n",
    "        state_traj2, action_traj2 = session.run(cart_pole.denormalize(state_traj2, action_traj2))\n",
    "        for col in [1, 3]:\n",
    "            state_traj1[:, col] = np.rad2deg(state_traj1[:, col])\n",
    "            state_traj2[:, col] = np.rad2deg(state_traj2[:, col])\n",
    "    else:\n",
    "        state_names = [r'$x$', r'$\\theta$', r'$\\dot{x}$', r'$\\dot{\\theta}$']\n",
    "    \n",
    "    plot_idx = (1, 2, 4, 5)               \n",
    "    for i in range(cart_pole.state_dim):\n",
    "        ax = fig.add_subplot(2, 3, plot_idx[i])\n",
    "        ax.plot(t, state_traj1[:, i], colors[0])\n",
    "        ax.plot(t, state_traj2[:, i], colors[1])\n",
    "        ax.set_xlabel(r'$t$ [s]', fontsize=14)\n",
    "        ax.set_ylabel(state_names[i], fontsize=14)\n",
    "    ax = fig.add_subplot(2, 3, 3)\n",
    "    plot1 = ax.plot(t, action_traj1, color=colors[0])\n",
    "    plot2 = ax.plot(t, action_traj2, color=colors[1])\n",
    "    ax.set_xlabel(r'$t$ [s]', fontsize=14)\n",
    "    if denormalize:\n",
    "        ax.set_ylabel(r'$u$ [N]', fontsize=14)\n",
    "    else:\n",
    "        ax.set_ylabel(r'$u$', fontsize=14)\n",
    "    fig.legend((plot1[0], plot2[0]), (labels[0], labels[1]), loc=(0.75, 0.2), fontsize=14)\n",
    "\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State cost matrix\n",
    "Q = np.diag([0.1, 0.1, 0.1, 0.1])\n",
    "\n",
    "# Action cost matrix\n",
    "R = 0.1*np.identity(action_dim)\n",
    "\n",
    "# Quadratic reward (-cost) function\n",
    "reward_function = safe_learning.QuadraticFunction(block_diag(-Q, -R), name='reward_function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact Optimal Policy and Value Function for the True Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve Lyapunov equation for the exact value function matrix P and optimal feedback law u = -K.dot(s)\n",
    "K, P = safe_learning.utilities.dlqr(A, B, Q, R)\n",
    "print('LQR gain:\\n{}\\n'.format(-K))\n",
    "print('Induced 2-norm of LQR gain:\\n{}\\n'.format(np.linalg.norm(K, 2)))\n",
    "print('LQR cost matrix:\\n{}\\n'.format(P))\n",
    "\n",
    "# LQR policy\n",
    "lqr_policy = safe_learning.functions.LinearSystem((-K,), name='LQR_policy')\n",
    "if saturate:\n",
    "    lqr_policy = safe_learning.Saturation(lqr_policy, -1, 1)\n",
    "\n",
    "# Optimal value function\n",
    "lqr_value_function = safe_learning.functions.QuadraticFunction(-P, name='LQR_value_function')\n",
    "\n",
    "# Approximate maximum ellipsoidal level set contained inside [-1, 1]**d via sampling\n",
    "samples = sample_box_boundary(state_limits, 1e6)\n",
    "test_values = lqr_value_function(tf.constant(samples, tf_dtype)).eval()\n",
    "c_min = np.amin(np.abs(test_values))\n",
    "c_max = np.amax(np.abs(test_values))\n",
    "print('Minimum LQR cost (approx.):\\n{}\\n'.format(c_min))\n",
    "print('Maximum LQR cost (approx.):\\n{}\\n'.format(c_max))\n",
    "\n",
    "# LQR response\n",
    "T = 1000\n",
    "ic = np.array([1., 1., 0., 0.]).reshape(1, -1)\n",
    "x, u, t, r = compute_closedloop_response(dynamics, lqr_policy, state_dim, T, dt, 'zero', ic=ic)\n",
    "\n",
    "names = [r'$x$', r'$\\theta$', r'$\\dot{x}$', r'$\\dot{\\theta}$', r'$u$']\n",
    "plt.plot(t, np.concatenate((x, u), axis=1))\n",
    "plt.legend(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Approximators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "max_state = np.ones(state_dim).reshape((1, -1))\n",
    "max_action = np.ones(action_dim).reshape((1, -1))\n",
    "r_max = np.linalg.multi_dot((max_state, Q, max_state.T)) + np.linalg.multi_dot((max_action, R, max_action.T))\n",
    "gamma = tf.placeholder(tf_dtype, shape=[], name='discount_factor')\n",
    "scaling = (1 - gamma) / r_max\n",
    "\n",
    "# Value function\n",
    "if train_value_function:\n",
    "    layer_dims = [64, 64, 1]\n",
    "    activations = [tf.nn.relu, tf.nn.relu, None]\n",
    "    value_function = safe_learning.functions.NeuralNetwork(layer_dims, activations, name='value_function')\n",
    "else:\n",
    "    value_function = lqr_value_function\n",
    "    \n",
    "# Policy\n",
    "if train_policy:\n",
    "    layer_dims = [64, 64, action_dim]\n",
    "    activations = [tf.nn.relu, tf.nn.relu, None]\n",
    "    if saturate:\n",
    "        activations[-1] = tf.nn.tanh\n",
    "    policy = safe_learning.functions.NeuralNetwork(layer_dims, activations, scaling=1., name='policy')\n",
    "else:\n",
    "    policy = lqr_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = tf.placeholder(tf_dtype, shape=[None, state_dim], name='states')\n",
    "actions = policy(states)\n",
    "rewards = reward_function(states, actions)\n",
    "future_states = dynamics(states, actions)\n",
    "\n",
    "values = value_function(states)\n",
    "if clip_states:\n",
    "    future_values = value_function(tf.clip_by_value(future_states, -1, 1))\n",
    "else:\n",
    "    future_values = value_function(future_states)\n",
    "\n",
    "true_values = lqr_value_function(states)\n",
    "true_actions = lqr_policy(states)\n",
    "true_rewards = reward_function(states, true_actions)\n",
    "true_future_states = dynamics(states, true_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bellman error objective for value update\n",
    "with tf.name_scope('value_optimization'):\n",
    "    value_learning_rate = tf.placeholder(tf_dtype, shape=[], name='learning_rate')\n",
    "    target = tf.stop_gradient(rewards + gamma*future_values, name='target')\n",
    "    value_obj = scaling*tf.reduce_mean(tf.abs(values - target), name='objective')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(value_learning_rate)\n",
    "    if train_value_function:\n",
    "        value_update = optimizer.minimize(value_obj, var_list=value_function.parameters)\n",
    "\n",
    "# Pseudo-integration objective for policy update\n",
    "with tf.name_scope('policy_optimization'):\n",
    "    policy_learning_rate = tf.placeholder(tf_dtype, shape=[], name='learning_rate')\n",
    "    lagrange_multiplier = tf.placeholder(tf_dtype, shape=[], name='lagrange_multiplier')\n",
    "    regularizer = -policy.lipschitz()\n",
    "    policy_obj = -scaling*(tf.reduce_mean(rewards + gamma*future_values, name='objective') \n",
    "                           + lagrange_multiplier*regularizer)    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(policy_learning_rate)\n",
    "    if train_policy:\n",
    "        policy_update = optimizer.minimize(policy_obj, var_list=policy.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow this cell to be run repeatedly to continue training if desired\n",
    "if not initialized:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    initialized = True\n",
    "if 'value_obj_history' not in locals():\n",
    "    value_obj_history = np.zeros(0)\n",
    "    value_param_history = np.zeros(0)\n",
    "    policy_obj_history = np.zeros(0)\n",
    "    policy_param_history = np.zeros(0)\n",
    "    \n",
    "# Uniformly distributed test set\n",
    "test_size = 1e3\n",
    "grid_length = np.power(test_size, 1 / state_dim)\n",
    "grid_length = int(2*np.floor(grid_length / 2) + 1)\n",
    "test_set = safe_learning.GridWorld(state_limits, [grid_length,]*state_dim).all_points\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 100\n",
    "min_iters = 100\n",
    "batch_size = 1e3\n",
    "batch = constrained_batch_sampler(dynamics, policy, state_dim, batch_size, action_limit=None, zero_pad=0)\n",
    "\n",
    "value_iters = 50\n",
    "policy_iters = 10\n",
    "\n",
    "value_tol = 1e-1\n",
    "policy_tol = 1e-3\n",
    "\n",
    "feed_dict = {\n",
    "    states:               test_set,\n",
    "    gamma:                0.99,\n",
    "    value_learning_rate:  0.2,\n",
    "    policy_learning_rate: 0.7,\n",
    "    lagrange_multiplier:  0.0,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Record objective values over time\n",
    "value_obj_eval = np.zeros(max_iters + 1)\n",
    "policy_obj_eval = np.zeros(max_iters + 1)\n",
    "value_obj_eval[0] = value_obj.eval(feed_dict)\n",
    "policy_obj_eval[0] = policy_obj.eval(feed_dict)\n",
    "\n",
    "# For convergence, check the parameter values\n",
    "converged = False\n",
    "iter_memory = 5\n",
    "value_param_changes = np.zeros(max_iters)\n",
    "policy_param_changes = np.zeros(max_iters)\n",
    "old_value_params = session.run(value_function.parameters)\n",
    "old_policy_params = session.run(policy.parameters)\n",
    "\n",
    "\n",
    "for i in tqdm(range(max_iters)):\n",
    "    \n",
    "    # Policy evaluation (value update)\n",
    "    if train_value_function:\n",
    "        for _ in range(value_iters):\n",
    "            feed_dict[states] = session.run(batch)\n",
    "            session.run(value_update, feed_dict)\n",
    "        new_value_params = session.run(value_function.parameters)\n",
    "        value_param_changes[i] = get_max_parameter_change(old_value_params, new_value_params)\n",
    "        old_value_params = new_value_params\n",
    "\n",
    "    # Policy improvement (policy update)\n",
    "    if train_policy:\n",
    "        for _ in range(policy_iters):\n",
    "            feed_dict[states] = session.run(batch)\n",
    "            session.run(policy_update, feed_dict)\n",
    "        new_policy_params = session.run(policy.parameters)\n",
    "        policy_param_changes[i] = get_max_parameter_change(old_policy_params, new_policy_params)\n",
    "        old_policy_params = new_policy_params\n",
    "    \n",
    "    feed_dict[states] = test_set\n",
    "    value_obj_eval[i+1] = value_obj.eval(feed_dict)\n",
    "    policy_obj_eval[i+1] = policy_obj.eval(feed_dict)\n",
    "    \n",
    "    # TODO debugging    \n",
    "    if np.isnan(value_obj_eval[i+1]) or np.isnan(policy_obj_eval[i+1]):\n",
    "        raise ValueError('Encountered NAN value after {} iterations!'.format(i+1))\n",
    "    \n",
    "    # TODO Break if converged\n",
    "#     if i >= iter_memory and i >= min_iters:\n",
    "#         value_params_converged = np.all(value_param_changes[i-iter_memory+1:i+1] <= value_tol)\n",
    "#         policy_params_converged = np.all(policy_param_changes[i-iter_memory+1:i+1] <= policy_tol)\n",
    "#         if value_params_converged and policy_params_converged:\n",
    "#             converged = True\n",
    "#             break\n",
    "\n",
    "final_iter = i+1\n",
    "if converged:\n",
    "    print('Converged after {} iterations.'.format(final_iter))\n",
    "else:\n",
    "    print('Did not converge!')\n",
    "\n",
    "value_obj_history = np.concatenate((value_obj_history, value_obj_eval[:final_iter+1]))\n",
    "value_param_history = np.concatenate((value_param_history, value_param_changes[:final_iter+1]))\n",
    "policy_obj_history = np.concatenate((policy_obj_history, policy_obj_eval[:final_iter+1]))\n",
    "policy_param_history = np.concatenate((policy_param_history, policy_param_changes[:final_iter+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5), dpi=200)\n",
    "fig.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "cap = 50\n",
    "start = 0\n",
    "end = -1\n",
    "\n",
    "ax = fig.add_subplot(221)\n",
    "ax.plot(np.clip(value_obj_history[start:end], None, cap), '.-r')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Value Function Objective')\n",
    "\n",
    "ax = fig.add_subplot(223)\n",
    "ax.plot(np.clip(value_param_history[start:end], None, cap), '.-r')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Max. Value Param. Change')\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(222)\n",
    "ax.plot(np.clip(policy_obj_history[start:end], None, cap), '.-r')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Policy Objective')\n",
    "\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(np.clip(policy_param_history[start:end], None, cap), '.-r')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Max. Policy Param. Change')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Input and Step Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic = np.array([0.1, 0.2, 0.1, 0.1])\n",
    "const = 1 / u_max\n",
    "steps = 1000\n",
    "\n",
    "print(session.run(actions, {states: np.zeros([1, state_dim])}))\n",
    "print(session.run(values, {states: np.zeros([1, state_dim])}))\n",
    "\n",
    "# Zero-input response, zero initial condition\n",
    "plot_closedloop_response(dynamics, policy, lqr_policy, steps, dt, 'zero', ic=None, \n",
    "                         labels=['Learned','True'], denormalize=True)\n",
    "\n",
    "# Zero-input response, non-zero initial condition\n",
    "plot_closedloop_response(dynamics, policy, lqr_policy, steps, dt, 'zero', ic=ic, \n",
    "                         labels=['Learned','True'], denormalize=False)\n",
    "\n",
    "# Step response\n",
    "plot_closedloop_response(dynamics, policy, lqr_policy, steps, dt, 'step', const=const, \n",
    "                         labels=['Learned','True'], denormalize=False)\n",
    "\n",
    "# Impulse response\n",
    "# plot_closedloop_response(dynamics, policy, lqr_policy, steps, dt, 'impulse', labels=['Learned','True'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function and Policy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not initialized:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    initialized = True\n",
    "    time.sleep(1.5)\n",
    "    print('Initialized!')\n",
    "\n",
    "fixed_state = [0.0, 0.0, 0., 0.]\n",
    "n_points = [25, 25]\n",
    "\n",
    "plot_value_function(values, true_values, n_points, fixed_state)\n",
    "plot_policy(actions, true_actions, n_points, fixed_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting.show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
