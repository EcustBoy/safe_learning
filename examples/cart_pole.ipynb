{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for a Cart-Pole System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.linalg import block_diag\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "import safe_learning\n",
    "import plotting\n",
    "from utilities import sample_box, sample_ellipsoid, compute_closedloop_response, CartPole, get_max_parameter_change\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x\n",
    "    \n",
    "np_dtype = safe_learning.config.np_dtype\n",
    "tf_dtype = safe_learning.config.dtype\n",
    "\n",
    "tf.reset_default_graph()\n",
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "initialized = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_function(tf_learned_values, tf_true_values, n_points, fixed_state, colors=['r','b'], show=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(12, 5), dpi=100)\n",
    "    fig.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "    xx, yy = np.mgrid[-1:1:np.complex(0, n_points[0]), -1:1:np.complex(0, n_points[1])]\n",
    "    \n",
    "    x_fix, theta_fix, x_dot_fix, theta_dot_fix = fixed_state\n",
    "\n",
    "    # Fix x_dot and theta_dot, plot value function over x and theta\n",
    "    grid = np.column_stack((xx.ravel(), yy.ravel(), \n",
    "                            x_dot_fix*np.ones_like(xx.ravel()), theta_dot_fix*np.ones_like(yy.ravel())))\n",
    "    learned_values = session.run(tf_learned_values, feed_dict={states: grid}).reshape(n_points)\n",
    "    true_values = session.run(tf_true_values, feed_dict={states: grid}).reshape(n_points)\n",
    "    ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    ax.plot_surface(xx, yy, learned_values, color=colors[0], alpha=0.75)\n",
    "    ax.plot_surface(xx, yy, true_values, color=colors[1], alpha=0.5)\n",
    "    ax.set_title(r'Value function, $\\dot{x} = %.3g,\\ \\dot{\\theta} = %.3g$' % (x_dot_fix, theta_dot_fix), fontsize=16)\n",
    "    ax.set_xlabel(r'$x$', fontsize=14)\n",
    "    ax.set_ylabel(r'$\\theta$', fontsize=14)\n",
    "    ax.set_zlabel(r'$V(s)$', fontsize=14)\n",
    "\n",
    "    # Fix x and theta, plot value function over x_dot and theta_dot\n",
    "    grid = np.column_stack((x_fix*np.ones_like(xx.ravel()), theta_fix*np.ones_like(yy.ravel()), \n",
    "                            xx.ravel(), yy.ravel()))\n",
    "    learned_values = session.run(tf_learned_values, feed_dict={states: grid}).reshape(n_points)\n",
    "    true_values = session.run(tf_true_values, feed_dict={states: grid}).reshape(n_points)\n",
    "    ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    ax.plot_surface(xx, yy, learned_values, color=colors[0], alpha=0.75)\n",
    "    ax.plot_surface(xx, yy, true_values, color=colors[1], alpha=0.5)\n",
    "    ax.set_title(r'Value function, $x = %.3g,\\ \\theta = %.3g$' % (x_fix, theta_fix), fontsize=16)\n",
    "    ax.set_xlabel(r'$\\dot{x}$', fontsize=14)\n",
    "    ax.set_ylabel(r'$\\dot{\\theta}$', fontsize=14)\n",
    "    ax.set_zlabel(r'$V(s)$', fontsize=14)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "def plot_policy(tf_actions, tf_true_actions, n_points, fixed_state, colors=['r','b'], show=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(12, 5), dpi=100)\n",
    "    fig.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "    xx, yy = np.mgrid[-1:1:np.complex(0, n_points[0]), -1:1:np.complex(0, n_points[1])]\n",
    "    \n",
    "    x_fix, theta_fix, x_dot_fix, theta_dot_fix = fixed_state\n",
    "\n",
    "    # Fix x_dot and theta_dot, plot value function over x and theta\n",
    "    grid = np.column_stack((xx.ravel(), yy.ravel(), \n",
    "                            x_dot_fix*np.ones_like(xx.ravel()), theta_dot_fix*np.ones_like(yy.ravel())))\n",
    "    learned_control = session.run(tf_actions, feed_dict={states: grid}).reshape(n_points)\n",
    "    true_control = session.run(tf_true_actions, feed_dict={states: grid}).reshape(n_points)\n",
    "    ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    ax.plot_surface(xx, yy, learned_control, color=colors[0], alpha=0.75)\n",
    "    ax.plot_surface(xx, yy, true_control, color=colors[1], alpha=0.5)\n",
    "    ax.set_title(r'Control, $\\dot{x} = %.3g,\\ \\dot{\\theta} = %.3g$' % (x_dot_fix, theta_dot_fix), fontsize=16)\n",
    "    ax.set_xlabel(r'$x$', fontsize=14)\n",
    "    ax.set_ylabel(r'$\\theta$', fontsize=14)\n",
    "    ax.set_zlabel(r'$u$', fontsize=14)\n",
    "    ax.view_init(elev=20., azim=15.)\n",
    "\n",
    "    # Fix x and theta, plot value function over x_dot and theta_dot\n",
    "    grid = np.column_stack((x_fix*np.ones_like(xx.ravel()), theta_fix*np.ones_like(yy.ravel()), \n",
    "                            xx.ravel(), yy.ravel()))\n",
    "    learned_control = session.run(tf_actions, feed_dict={states: grid}).reshape(n_points)\n",
    "    true_control = session.run(tf_true_actions, feed_dict={states: grid}).reshape(n_points)\n",
    "    ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    ax.plot_surface(xx, yy, learned_control, color=colors[0], alpha=0.75)\n",
    "    ax.plot_surface(xx, yy, true_control, color=colors[1], alpha=0.5)\n",
    "    ax.set_title(r'Control, $x = %.3g,\\ \\theta = %.3g$' % (x_fix, theta_fix), fontsize=16)\n",
    "    ax.set_xlabel(r'$\\dot{x}$', fontsize=14)\n",
    "    ax.set_ylabel(r'$\\dot{\\theta}$', fontsize=14)\n",
    "    ax.set_zlabel(r'$u$', fontsize=14)\n",
    "    ax.view_init(elev=20., azim=100.)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_closedloop_response(dynamics, policy1, policy2, steps, dt, reference='zero', const=1.0, ic=None,\n",
    "                             labels=['Policy 1','Policy 2'], colors=['r','b'], show=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    state_names = [r'$x$', r'$\\theta$', r'$\\dot{x}$', r'$\\dot{\\theta}$']\n",
    "    plot_idx = (1, 2, 4, 5)\n",
    "    \n",
    "    state_traj1, action_traj1, t, _ = compute_closedloop_response(dynamics, policy1, steps, dt, reference, const, ic)\n",
    "    state_traj2, action_traj2, _, _ = compute_closedloop_response(dynamics, policy2, steps, dt, reference, const, ic)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 5), dpi=100)\n",
    "    fig.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "    \n",
    "    if reference=='zero':\n",
    "        title_string = r'Zero-Input Response'\n",
    "    elif reference=='impulse':\n",
    "        title_string = r'Impulse Response'\n",
    "    elif reference=='step':\n",
    "        title_string = r'Step Response, $r = %.1g$' % const\n",
    "    \n",
    "    if ic is not None:\n",
    "        ic_tuple = (ic[0], ic[1], ic[2], ic[3])\n",
    "        title_string = title_string + r', $s_0 = (%.1g, %.1g, %.1g, %.1g)$' % ic_tuple\n",
    "    fig.suptitle(title_string, fontsize=18)         \n",
    "                   \n",
    "    for i in range(cart_pole.state_dim):\n",
    "        ax = fig.add_subplot(2, 3, plot_idx[i])\n",
    "        ax.plot(t, state_traj1[:, i], colors[0])\n",
    "        ax.plot(t, state_traj2[:, i], colors[1])\n",
    "        ax.set_xlabel(r'$t$', fontsize=14)\n",
    "        ax.set_ylabel(state_names[i], fontsize=14)\n",
    "    ax = fig.add_subplot(2, 3, 3)\n",
    "    plot1 = ax.plot(t, action_traj1, color=colors[0])\n",
    "    plot2 = ax.plot(t, action_traj2, color=colors[1])\n",
    "    ax.set_xlabel(r'$t$', fontsize=14)\n",
    "    ax.set_ylabel(r'$u$', fontsize=14)\n",
    "    fig.legend((plot1[0], plot2[0]), (labels[0], labels[1]), loc=(0.75, 0.2), fontsize=14)\n",
    "\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System parameters\n",
    "m = 0.1     # pendulum mass\n",
    "M = 1.     # cart mass\n",
    "L = 0.3     # pole length\n",
    "\n",
    "# Constants\n",
    "dt = 0.01   # sampling time\n",
    "g = 9.81    # gravity\n",
    "\n",
    "# State and action normalizers\n",
    "x_max = 5.\n",
    "theta_max = np.deg2rad(15)\n",
    "x_dot_max = 2.\n",
    "theta_dot_max = np.deg2rad(15)\n",
    "u_max = (m + M)*x_dot_max / (10*dt)\n",
    "\n",
    "state_norm = (x_max, theta_max, x_dot_max, theta_dot_max)\n",
    "action_norm = (u_max,)\n",
    "\n",
    "# Define system and dynamics\n",
    "cart_pole = CartPole(m, M, L, dt, [state_norm, action_norm])\n",
    "\n",
    "state_limits = np.array([[-1., 1.]]*cart_pole.state_dim)\n",
    "action_limits = np.array([[-1., 1.]]*cart_pole.action_dim)\n",
    "\n",
    "A, B = cart_pole.linearize()   \n",
    "dynamics = safe_learning.functions.LinearSystem((A, B), name='dynamics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State cost matrix\n",
    "Q = np.diag([0.1, 0.1, 0.5, 0.5])\n",
    "\n",
    "# Action cost matrix\n",
    "R = 0.1*np.identity(cart_pole.action_dim)\n",
    "\n",
    "# Quadratic reward (-cost) function\n",
    "reward_function = safe_learning.QuadraticFunction(block_diag(-Q, -R), name='reward_function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact Optimal Policy and Value Function for the True Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve Lyapunov equation for the exact value function matrix P and optimal feedback law u = -K.dot(s)\n",
    "K, P = safe_learning.utilities.dlqr(A, B, Q, R)\n",
    "print('Optimal linear gain:\\n{}\\n'.format(-K))\n",
    "print('Quadratic value function:\\n{}'.format(P))\n",
    "\n",
    "print(np.linalg.norm(K, 2))\n",
    "\n",
    "# LQR policy\n",
    "lqr_policy = safe_learning.functions.LinearSystem((-K,), name='LQR_policy')\n",
    "lqr_policy = safe_learning.Saturation(lqr_policy, -1, 1)\n",
    "\n",
    "# Optimal value function\n",
    "lqr_value_function = safe_learning.functions.QuadraticFunction(-P, name='LQR_value_function')\n",
    "\n",
    "# T = 2000\n",
    "# x = np.zeros((T, 4))\n",
    "# u = np.zeros((T, 1))\n",
    "# x[0, :] = np.array([1., 1., 0., 0.]).reshape(1, -1)\n",
    "# u[0, :] = x[0, :].dot(-K.T)\n",
    "# for t in range(T-1):\n",
    "#     x[t+1, :] = x[t, :].dot((A - B.dot(K)).T)\n",
    "#     u[t+1, :] = x[t+1, :].dot(-K.T)\n",
    "\n",
    "# state_names = [r'$x$', r'$\\theta$', r'$\\dot{x}$', r'$\\dot{\\theta}$', r'$u$']\n",
    "# plt.plot(np.concatenate((x, u), axis=1))\n",
    "# plt.legend(state_names)\n",
    "# plt.show()\n",
    "\n",
    "T = 1000\n",
    "ic = np.array([1., 1., 0., 0.]).reshape(1, -1)\n",
    "x, u, t, r = compute_closedloop_response(dynamics, lqr_policy, T, dt, 'zero', ic=ic)\n",
    "\n",
    "names = [r'$x$', r'$\\theta$', r'$\\dot{x}$', r'$\\dot{\\theta}$', r'$u$']\n",
    "plt.plot(t, np.concatenate((x, u), axis=1))\n",
    "plt.legend(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Approximators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "max_state = np.ones(cart_pole.state_dim).reshape((1, -1))\n",
    "max_action = np.ones(cart_pole.action_dim).reshape((1, -1))\n",
    "r_max = np.linalg.multi_dot((max_state, Q, max_state.T)) + np.linalg.multi_dot((max_action, R, max_action.T))\n",
    "gamma = 0.98\n",
    "scaling = (1 - gamma) / r_max\n",
    "\n",
    "print('Maximum reward (abs): {}'.format(r_max))\n",
    "print('Scaling: {}'.format(scaling))\n",
    "\n",
    "\n",
    "# TODO testing ****************************************#\n",
    "train_policy = True\n",
    "\n",
    "train_value_function = True\n",
    "\n",
    "supervise = False\n",
    "#******************************************************#\n",
    "\n",
    "\n",
    "# Value function\n",
    "if train_value_function:\n",
    "    layer_dims = [64, 64, 1]\n",
    "    activations = [tf.nn.relu, tf.nn.relu, None]\n",
    "    value_function = safe_learning.functions.NeuralNetwork(layer_dims, activations, name='value_function')\n",
    "else:\n",
    "    value_function = safe_learning.functions.QuadraticFunction(-scaling*P)\n",
    "    \n",
    "# Policy\n",
    "if train_policy:\n",
    "    layer_dims = [64, 64, cart_pole.action_dim]\n",
    "    activations = [tf.nn.relu, tf.nn.relu, tf.nn.tanh]\n",
    "    policy = safe_learning.functions.NeuralNetwork(layer_dims, activations, scaling=1., name='policy')\n",
    "else:\n",
    "    policy = lqr_policy\n",
    "\n",
    "# TensorFlow graph\n",
    "states = tf.placeholder(tf_dtype, shape=[None, cart_pole.state_dim], name='states')\n",
    "actions = policy(states)\n",
    "rewards = reward_function(states, actions)\n",
    "future_states = dynamics(states, actions)\n",
    "\n",
    "values = value_function(states)\n",
    "future_values = value_function(tf.clip_by_value(future_states, -1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning on a \"Wrong\" Model\n",
    "\n",
    "Create a cart-pole system with \"wrong\" parameters. Fit our parametric value function to the corresponding LQR solution in a supervised fashion as a starting point before tackling policy iteration for the real system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Wrong\" system\n",
    "m = 0.7     # pendulum mass\n",
    "M = 1.      # cart mass\n",
    "L = 0.6     # pole length\n",
    "\n",
    "wrong_cart_pole = CartPole(m, M, L, dt, [state_norm, action_norm])\n",
    "A_wrong, B_wrong = wrong_cart_pole.linearize()\n",
    "wrong_dynamics = safe_learning.LinearSystem((A_wrong, B_wrong), name='wrong_dynamics')\n",
    "K_wrong, P_wrong = safe_learning.utilities.dlqr(A_wrong, B_wrong, Q, R)\n",
    "\n",
    "wrong_lqr_policy = safe_learning.functions.LinearSystem(-K_wrong, name='wrong_LQR_policy')\n",
    "wrong_lqr_policy = safe_learning.Saturation(wrong_lqr_policy, -1, 1)\n",
    "wrong_lqr_value_function = safe_learning.functions.QuadraticFunction(-P_wrong, name='wrong_LQR_value_function')\n",
    "\n",
    "# TODO testing\n",
    "# wrong_lqr_policy = lqr_policy\n",
    "# wrong_lqr_value_function = lqr_value_function\n",
    "\n",
    "# Visualize closed-loop responses\n",
    "ic = np.array([0.5, 0.2, -0.1, 0.1])\n",
    "const = 0.7\n",
    "steps = 2000\n",
    "plot_closedloop_response(dynamics, wrong_lqr_policy, lqr_policy, steps, dt, \n",
    "                         'step', const, ic, ['Wrong','True'], ['g','b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_values = wrong_lqr_value_function(states)\n",
    "wrong_actions = wrong_lqr_policy(states)\n",
    "supervised_learning_rate = tf.placeholder(tf_dtype, shape=[], name='learning_rate')\n",
    "optimizer = tf.train.GradientDescentOptimizer(supervised_learning_rate)\n",
    "\n",
    "if supervise:\n",
    "    if train_value_function:\n",
    "        with tf.name_scope('supervised_value_optimization'):\n",
    "            supervised_value_obj = scaling*tf.reduce_mean(tf.abs(values - wrong_values), name='objective')\n",
    "            wrong_value_update = optimizer.minimize(supervised_value_obj, var_list=value_function.parameters)\n",
    "\n",
    "    if train_policy:\n",
    "        with tf.name_scope('supervised_policy_optimization'):\n",
    "            supervised_policy_obj = tf.reduce_mean(tf.abs(actions - wrong_actions), name='objective')\n",
    "            wrong_policy_update = optimizer.minimize(supervised_policy_obj, var_list=policy.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning: Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_value_function and supervise:\n",
    "    # Allow this cell to be run repeatedly to continue training if desired\n",
    "    if not initialized:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        initialized = True\n",
    "\n",
    "    # Training hyperparameters\n",
    "    max_iters = 3000\n",
    "    batch_size = 1e3\n",
    "    alpha = 0.5\n",
    "    test_set = sample_box(state_limits, 1e5)\n",
    "\n",
    "    # Record objective values over time\n",
    "    supervised_obj_eval = np.zeros(max_iters + 1)\n",
    "    supervised_obj_eval[0] = session.run(supervised_value_obj, {states: test_set})\n",
    "\n",
    "    # For checking convergence\n",
    "    converged = False\n",
    "    iter_memory = 100\n",
    "    tol = 1e-1\n",
    "    param_changes = np.zeros(max_iters)\n",
    "    old_params = session.run(value_function.parameters)\n",
    "\n",
    "    for i in tqdm(range(max_iters)):\n",
    "        batch = sample_box(state_limits, batch_size)\n",
    "        session.run(wrong_value_update, feed_dict={states: batch, supervised_learning_rate: alpha})\n",
    "    #     supervised_obj_eval[i+1] = session.run(supervised_value_obj, {states: test_set})\n",
    "\n",
    "        new_params = session.run(value_function.parameters)\n",
    "        param_changes[i] = get_max_parameter_change(old_params, new_params)\n",
    "        old_params = new_params\n",
    "\n",
    "        # Break if converged\n",
    "#         if i >= iter_memory:\n",
    "#             params_converged = np.all(param_changes[i-iter_memory+1:i+1] <= tol)\n",
    "#             if params_converged:\n",
    "#                 converged = True\n",
    "#                 break\n",
    "            \n",
    "    final_iter = i+1\n",
    "    if converged:\n",
    "        print('Converged after {} iterations.'.format(final_iter))\n",
    "    else:\n",
    "        print('Did not converge!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_value_function and supervise:\n",
    "    fig = plt.figure(figsize=(10, 5), dpi=100)\n",
    "    fig.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.plot(supervised_obj_eval[:final_iter+1], '.-r')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Supervised Learning Objective')\n",
    "\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.plot(param_changes[:final_iter+1], '.-r')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Max. Value Param. Change')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    #\n",
    "    fixed_state = [0., 0., 0., 0.]\n",
    "    plot_value_function(values, wrong_values, [51, 51], fixed_state, ['r', 'g'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning: Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_policy and supervise:\n",
    "    # Allow this cell to be run repeatedly to continue training if desired\n",
    "    if not initialized:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        initialized = True\n",
    "\n",
    "    # Training hyperparameters\n",
    "    max_iters = 500\n",
    "    batch_size = 1e3\n",
    "    alpha = 0.07\n",
    "    test_set = sample_box(state_limits, 1e5)\n",
    "\n",
    "    # Record objective values over time\n",
    "    supervised_obj_eval = np.zeros(max_iters + 1)\n",
    "    supervised_obj_eval[0] = session.run(supervised_policy_obj, {states: test_set})\n",
    "\n",
    "    # For checking convergence\n",
    "    converged = False\n",
    "    iter_memory = 50\n",
    "    tol = 1e-3\n",
    "    param_changes = np.zeros(max_iters)\n",
    "    old_params = session.run(policy.parameters)\n",
    "\n",
    "    for i in tqdm(range(max_iters)):\n",
    "        batch = sample_box(state_limits, batch_size)\n",
    "        session.run(wrong_policy_update, feed_dict={states: batch, supervised_learning_rate: alpha})\n",
    "#         supervised_obj_eval[i+1] = session.run(supervised_policy_obj, {states: test_set})\n",
    "\n",
    "        # Value function parameter changes\n",
    "        new_params = session.run(policy.parameters)\n",
    "        param_changes[i] = get_max_parameter_change(old_params, new_params)\n",
    "        old_params = new_params\n",
    "\n",
    "        # Break if converged\n",
    "#         if i >= iter_memory:\n",
    "#             params_converged = np.all(param_changes[i-iter_memory+1:i+1] <= tol)\n",
    "#             if params_converged:\n",
    "#                 converged = True\n",
    "#                 break\n",
    "\n",
    "    final_iter = i+1\n",
    "    if converged:\n",
    "        print('Converged after {} iterations.'.format(final_iter))\n",
    "    else:\n",
    "        print('Did not converge!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_policy and supervise:\n",
    "    fig = plt.figure(figsize=(10, 5), dpi=100)\n",
    "    fig.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.plot(supervised_obj_eval[:final_iter+1], '.-r')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Supervised Learning Objective')\n",
    "\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.plot(param_changes[:final_iter+1], '.-r')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Max. Policy Param. Change')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    #\n",
    "    fixed_state = [0., 0., 0., 0.]\n",
    "    n_points = [51, 51]\n",
    "    plot_policy(actions, wrong_actions, n_points, fixed_state, colors=['r','g'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Objectives\n",
    "\n",
    "Now back to the true cart-pole system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bellman error objective for value update\n",
    "if train_value_function:\n",
    "    with tf.name_scope('value_optimization'):\n",
    "        value_learning_rate = tf.placeholder(tf_dtype, shape=[], name='learning_rate')\n",
    "        target = tf.stop_gradient(rewards + gamma*future_values, name='target')\n",
    "        value_obj = scaling*tf.reduce_mean(tf.abs(values - target), name='objective')\n",
    "        optimizer = tf.train.GradientDescentOptimizer(value_learning_rate)\n",
    "        value_update = optimizer.minimize(value_obj, var_list=value_function.parameters)\n",
    "\n",
    "# Pseudo-integration objective for policy update\n",
    "if train_policy:\n",
    "    with tf.name_scope('policy_optimization'):\n",
    "        policy_learning_rate = tf.placeholder(tf_dtype, shape=[], name='learning_rate')\n",
    "        policy_obj = -scaling*tf.reduce_mean(rewards + gamma*future_values, name='objective')\n",
    "        optimizer = tf.train.GradientDescentOptimizer(policy_learning_rate)\n",
    "        policy_update = optimizer.minimize(policy_obj, var_list=policy.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow this cell to be run repeatedly to continue training if desired\n",
    "if not initialized:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    initialized = True\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 5\n",
    "value_iters = 100  # 2000\n",
    "policy_iters = 10  # 500\n",
    "\n",
    "value_alpha = 0.05 # 0.5\n",
    "value_tol = 1e-1\n",
    "\n",
    "policy_alpha = 0.01 # 0.5\n",
    "policy_tol = 1e-3\n",
    "\n",
    "batch_size = 1e3\n",
    "zero_batch = np.zeros((1, cart_pole.state_dim))     # always sample zero-state\n",
    "test_set = sample_box(state_limits, 1e5)\n",
    "\n",
    "# Record objective values over time\n",
    "value_obj_eval = np.zeros(max_iters + 1)\n",
    "policy_obj_eval = np.zeros(max_iters + 1)\n",
    "value_obj_eval[0] = session.run(value_obj, {states: test_set})\n",
    "policy_obj_eval[0] = session.run(policy_obj, {states: test_set})\n",
    "\n",
    "# For convergence, check the parameter values\n",
    "converged = False\n",
    "iter_memory = 5\n",
    "value_param_changes = np.zeros(max_iters)\n",
    "policy_param_changes = np.zeros(max_iters)\n",
    "old_value_params = session.run(value_function.parameters)\n",
    "old_policy_params = session.run(policy.parameters)\n",
    "\n",
    "for i in tqdm(range(max_iters)):\n",
    "    \n",
    "    # Policy evaluation (value update)\n",
    "    for _ in range(value_iters):\n",
    "        batch = sample_box(state_limits, batch_size)\n",
    "        batch = np.concatenate((batch, zero_batch), axis=0)\n",
    "        session.run(value_update, feed_dict={states: batch, value_learning_rate: value_alpha})\n",
    "    new_value_params = session.run(value_function.parameters)\n",
    "    value_param_changes[i] = get_max_parameter_change(old_value_params, new_value_params)\n",
    "    old_value_params = new_value_params\n",
    "\n",
    "    # Policy improvement (policy update)\n",
    "    for _ in range(policy_iters):\n",
    "        batch = sample_box(state_limits, batch_size)\n",
    "        batch = np.concatenate((batch, zero_batch), axis=0)      \n",
    "        session.run(policy_update, feed_dict={states: batch, policy_learning_rate: policy_alpha})\n",
    "    new_policy_params = session.run(policy.parameters)\n",
    "    policy_param_changes[i] = get_max_parameter_change(old_policy_params, new_policy_params)\n",
    "    old_policy_params = new_policy_params\n",
    "    \n",
    "#     # TODO debugging\n",
    "    value_obj_eval[i+1] = session.run(value_obj, {states: test_set})\n",
    "    policy_obj_eval[i+1] = session.run(policy_obj, {states: test_set})\n",
    "    if np.isnan(value_obj_eval[i+1]) or np.isnan(policy_obj_eval[i+1]):\n",
    "        raise ValueError('Encountered NAN value after {} iterations!'.format(i+1))\n",
    "    \n",
    "    # Break if converged\n",
    "#     if i >= iter_memory:\n",
    "#         value_params_converged = np.all(value_param_changes[i-iter_memory+1:i+1] <= value_tol)\n",
    "#         policy_params_converged = np.all(policy_param_changes[i-iter_memory+1:i+1] <= policy_tol)\n",
    "#         if value_params_converged and policy_params_converged:\n",
    "#             converged = True\n",
    "#             break\n",
    "\n",
    "if converged:\n",
    "    print('Converged after {} iterations.'.format(i+1))\n",
    "else:\n",
    "    print('Did not converge!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_iter = i+1\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5), dpi=100)\n",
    "fig.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "ax = fig.add_subplot(221)\n",
    "ax.plot(value_obj_eval[:final_iter+1], '.-r')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Value Function Objective')\n",
    "\n",
    "ax = fig.add_subplot(223)\n",
    "ax.plot(value_param_changes[:final_iter+1], '.-r')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Max. Value Param. Change')\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(222)\n",
    "ax.plot(policy_obj_eval[:final_iter+1], '.-r')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Policy Objective')\n",
    "\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(policy_param_changes[:final_iter+1], '.-r')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Max. Policy Param. Change')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Input and Step Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic = np.array([0.1, 0.2, 0.1, 0.1])\n",
    "const = 0.1\n",
    "steps = 3000\n",
    "\n",
    "# Zero-input response\n",
    "plot_closedloop_response(dynamics, policy, lqr_policy, steps, dt, 'zero', ic=None, labels=['Learned','True'])\n",
    "\n",
    "# Step response\n",
    "plot_closedloop_response(dynamics, policy, lqr_policy, steps, dt, 'step', const=const, labels=['Learned','True'])\n",
    "\n",
    "# Impulse response\n",
    "# plot_closedloop_response(dynamics, policy, lqr_policy, steps, dt, 'impulse', labels=['Learned','True'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function and Policy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_state = [0., 0., 0., 0.]\n",
    "n_points = [51, 51]\n",
    "\n",
    "true_values = lqr_value_function(states)\n",
    "true_actions = lqr_policy(states)\n",
    "\n",
    "plot_value_function(values, true_values, n_points, fixed_state, colors=['r','b'])\n",
    "plot_policy(actions, true_actions, n_points, fixed_state, colors=['r','b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting.show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
