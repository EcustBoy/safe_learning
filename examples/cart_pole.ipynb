{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart-Pole Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import cont2discrete\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "import safe_learning\n",
    "import plotting\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x\n",
    "    \n",
    "from tqdm import tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_state_sampler(n_samples, state_space, scale=False):\n",
    "    \"\"\"\n",
    "    Samples uniformly from a box in the state space.\n",
    "    \"\"\"\n",
    "    dim_state = len(state_space)\n",
    "    n_samps = int(n_samples)\n",
    "    for i in range(dim_state):\n",
    "        delta = state_space[i][1] - state_space[i][0]\n",
    "        offset = state_space[i][0]\n",
    "        single_dim = delta*np.random.rand(n_samps, 1) + offset\n",
    "        if scale:\n",
    "            single_dim = single_dim / np.max(np.abs(state_space[i]))\n",
    "        if i == 0:\n",
    "            samples = single_dim\n",
    "        else:\n",
    "            samples = np.concatenate((samples, single_dim), axis=1)\n",
    "    return samples\n",
    "\n",
    "def compute_trajectory(dynamics, policy, initial_condition, steps, input_type=None, dim_input=1):\n",
    "    dim_state = len(initial_condition)\n",
    "    if input_type == None:\n",
    "        u = np.zeros((1, dim_input))\n",
    "    elif input_type == 'step':\n",
    "        u = np.ones((1, dim_input))    \n",
    "    with tf.name_scope('compute_trajectory'):\n",
    "        trajectory = np.zeros((steps+1, dim_state), dtype=np.float)\n",
    "        trajectory = np.zeros((steps+1, dim_state), dtype=np.float)\n",
    "        trajectory[0, :] = initial_condition\n",
    "\n",
    "        current_state = tf.placeholder(dtype, shape=[1, dim_state])\n",
    "        next_state = dynamics(current_state, policy(current_state) + u)\n",
    "\n",
    "        for i in range(steps):\n",
    "            trajectory[i+1, :] = session.run(next_state, feed_dict={current_state: trajectory[[i], :]})\n",
    "    return trajectory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete-time Linearized Cart-Pole Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System parameters\n",
    "m = 10      # pendulum mass\n",
    "M = 10      # cart mass\n",
    "L = 100     # pole length\n",
    "g = 9.81    # gravitational constant\n",
    "\n",
    "# Linearized continuous-time system matrices for state vector:\n",
    "# s = (q, q_dot) = (x, theta, x_dot, theta_dot)\n",
    "dim_state = 4\n",
    "dim_input = 1\n",
    "A = np.array([[0, 0,             1, 0],\n",
    "              [0, 0,             0, 1],\n",
    "              [0, g*m/M,         0, 0],\n",
    "              [0, g*(m+M)/(L*M), 0, 0]])\n",
    "B = np.array([0, 0, 1/M, 1/(M*L)]).reshape((-1, dim_input))\n",
    "\n",
    "# Discretized system matrices\n",
    "dt = 0.01    # sampling time\n",
    "Ad, Bd, _, _, _ = cont2discrete((A, B, 0, 0), dt, method='zoh')\n",
    "\n",
    "# State and action spaces\n",
    "x_max = 10\n",
    "theta_max = np.deg2rad(45)\n",
    "x_dot_max = 5\n",
    "u_max = g*m*theta_max\n",
    "theta_dot_max = (g*(m+M)*theta_max + u_max)/(M*L)\n",
    "\n",
    "state_space = [[-x_max, x_max], [-theta_max, theta_max],\n",
    "               [-x_dot_max, x_dot_max], [-theta_dot_max, theta_dot_max]]\n",
    "input_space = [[-u_max, u_max],]\n",
    "\n",
    "# Scale the dynamics\n",
    "Ts = np.diag([np.max(np.abs(i)) for i in state_space])\n",
    "Tu = np.diag([np.max(np.abs(i)) for i in input_space])\n",
    "Ts_inv = np.diag([1/np.max(np.abs(i)) for i in state_space])\n",
    "Tu_inv = np.diag([1/np.max(np.abs(i)) for i in input_space])\n",
    "A_scl = Ts_inv.dot(Ad).dot(Ts)\n",
    "B_scl = Ts_inv.dot(Bd).dot(Tu)\n",
    "\n",
    "# TensorFlow wrapper for the dynamics\n",
    "@safe_learning.utilities.with_scope('scaled_dynamics')\n",
    "def scaled_dynamics(states, actions):\n",
    "    future_states = tf.matmul(states, A_scl.T) + tf.matmul(actions, B_scl.T)\n",
    "    return future_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Matrices and Exact LQR Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State cost matrix\n",
    "Q = 2*np.identity(dim_state)\n",
    "\n",
    "# Input cost matrix\n",
    "R = np.identity(dim_input)\n",
    "\n",
    "# Solve Lyapunov equation for the exact value function and optimal feedback law u = -K.dot(x)\n",
    "K, P = safe_learning.utilities.dlqr(A_scl, B_scl, Q, R)\n",
    "\n",
    "# TensorFlow wrapper for the LQR policy\n",
    "@safe_learning.utilities.with_scope('LQR_policy')\n",
    "def lqr_policy(states):\n",
    "    actions = tf.matmul(states, -K.T)\n",
    "    return actions\n",
    "\n",
    "# Use the state and input spaces to compute the \"maximum norm\" reward\n",
    "# max_state = np.array([np.max(np.abs(i)) for i in state_space]).reshape((1, dim_state))\n",
    "# max_input = np.array([np.max(np.abs(i)) for i in input_space]).reshape((1, dim_input))\n",
    "max_state = np.ones((1, dim_state))\n",
    "max_input = np.ones((1, dim_input))\n",
    "max_reward = max_state.dot(Q).dot(max_state.T) + max_input.dot(R).dot(max_input.T)\n",
    "\n",
    "print('Maximum reward is: {}'.format(max_reward))\n",
    "\n",
    "# TensorFlow wrapper for the reward function with scaling\n",
    "@safe_learning.utilities.with_scope('reward_function')\n",
    "def reward_function(states, actions):\n",
    "    rewards = -tf.reduce_sum(tf.matmul(states, Q)*states + tf.matmul(actions, R)*actions,\n",
    "                             axis=1,\n",
    "                             keep_dims=True) / 1000\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Approximators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value function\n",
    "layer_dims = [64, 64, 1]\n",
    "activations = [tf.nn.relu, tf.nn.relu, None]\n",
    "value_function = safe_learning.functions.NeuralNetwork(layer_dims,\n",
    "                                                       activations,\n",
    "                                                       name='value_function')\n",
    "\n",
    "# Policy\n",
    "layer_dims = [1]\n",
    "activations = [None]\n",
    "# initializer = tf.constant_initializer(-K.T)\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "policy = safe_learning.functions.NeuralNetwork(layer_dims,\n",
    "                                               activations,\n",
    "                                               name='policy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States, Actions, Rewards, and Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dtype = safe_learning.config.dtype\n",
    "\n",
    "states = tf.placeholder(dtype, shape=[None, dim_state], name='states')\n",
    "actions = policy(states)\n",
    "\n",
    "rewards = reward_function(states, actions)\n",
    "future_states = scaled_dynamics(states, actions)\n",
    "\n",
    "values = value_function(states)\n",
    "future_values = value_function(future_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.98\n",
    "value_learning_rate = 1e-1\n",
    "policy_learning_rate = 1e-1\n",
    "\n",
    "# TODO scale objectives for value and policy updates appropriately\n",
    "\n",
    "# Bellman error objective for value update\n",
    "with tf.name_scope('value_optimization'):\n",
    "    target = tf.stop_gradient(rewards + gamma*future_values, name='target')\n",
    "    # Normalize by the infinite series\n",
    "    value_obj = (1 - gamma)*tf.reduce_mean(tf.square(values - target), name='objective')\n",
    "    value_summary = tf.summary.scalar('value_objective',\n",
    "                                      value_obj)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(value_learning_rate)\n",
    "    value_update = optimizer.minimize(value_obj,\n",
    "                                      var_list=value_function.parameters)\n",
    "\n",
    "# Pseudo-integration objective for policy update\n",
    "with tf.name_scope('policy_optimization'):\n",
    "    policy_obj = -tf.reduce_mean(rewards + gamma*future_values, name='objective')\n",
    "    policy_summary = tf.summary.scalar('policy_objective',\n",
    "                                       policy_obj)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(policy_learning_rate)\n",
    "    policy_update = optimizer.minimize(policy_obj,\n",
    "                                       var_list=policy.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "max_iters = 100\n",
    "batch_size = 1e3\n",
    "test_set = uniform_state_sampler(1e5, state_space, scale=False)\n",
    "\n",
    "value_iters = 100\n",
    "value_tol = 1e-3\n",
    "\n",
    "policy_iters = 50\n",
    "policy_tol = 1e-3\n",
    "\n",
    "temp = session.run(rewards, {states: test_set})\n",
    "print(np.max(np.abs(temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converged = False\n",
    "value_obj_values = np.zeros(max_iters+1)\n",
    "policy_obj_values = np.zeros(max_iters+1)\n",
    "value_obj_values[0] = session.run(value_obj, {states: test_set})\n",
    "policy_obj_values[0] = session.run(policy_obj, {states: test_set})\n",
    "\n",
    "old_value_params = session.run(value_function.parameters)\n",
    "old_policy_params = session.run(policy.parameters)\n",
    "\n",
    "for i in tnrange(max_iters):\n",
    "\n",
    "    for _ in range(value_iters):\n",
    "        batch = uniform_state_sampler(batch_size, state_space, scale=False)\n",
    "        session.run(value_update, feed_dict={states: batch})\n",
    "\n",
    "    for _ in range(policy_iters):\n",
    "        batch = uniform_state_sampler(batch_size, state_space, scale=False)\n",
    "        session.run(policy_update, feed_dict={states: batch})\n",
    "\n",
    "    # Get new objective values\n",
    "    value_obj_values[i+1] = session.run(value_obj, {states: test_set})\n",
    "    policy_obj_values[i+1] = session.run(policy_obj, {states: test_set})\n",
    "\n",
    "    # Get new parameters\n",
    "    value_params = session.run(value_function.parameters)\n",
    "    policy_params = session.run(policy.parameters)\n",
    "\n",
    "    if np.isnan(value_obj_values[i+1]) or np.isnan(policy_obj_values[i+1]):\n",
    "        session.close()\n",
    "        raise ValueError('Encountered NAN value on iteration {}!'.format(i))\n",
    "\n",
    "#     # Compute objective changes\n",
    "#     value_change = value_obj_values[i+1] - value_obj_values[i]\n",
    "#     policy_change = policy_obj_values[i+1] - policy_obj_values[i]\n",
    "\n",
    "#     # Stop conditions\n",
    "#     minimal_change = ((np.abs(value_change) <= value_tol) and\n",
    "#                       (np.abs(policy_change) <= policy_tol))\n",
    "#     decrease = (value_change <= 0) and (policy_change <= 0)\n",
    "\n",
    "    # Compute maximum parameter changes\n",
    "    max_val_change = 0\n",
    "    for new, old in zip(value_params, old_value_params):\n",
    "        temp = np.max(np.abs(new - old))\n",
    "        if temp > max_val_change:\n",
    "            max_val_change = temp\n",
    "    max_pol_change = 0\n",
    "    for new, old in zip(policy_params, old_policy_params):\n",
    "        temp = np.max(np.abs(new - old))\n",
    "        if temp > max_pol_change:\n",
    "            max_pol_change = temp\n",
    "\n",
    "    # Break if converged\n",
    "    if max_val_change <= value_tol and max_pol_change <= policy_tol:\n",
    "        converged = True\n",
    "        break\n",
    "\n",
    "final_iter = i+1\n",
    "if converged:\n",
    "    print('Converged after {} iterations.'.format(final_iter))\n",
    "else:\n",
    "    print('Did not converge!')\n",
    "print('value objective: {} \\npolicy objective: {}'.format(value_obj_values[i+1], policy_obj_values[i+1]))\n",
    "print('value param change: {} \\npolicy param change: {}'.format(max_val_change, max_pol_change))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(value_obj_values[:final_iter+1])\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.plot(policy_obj_values[:final_iter+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximum reward: {}'.format(np.max(np.abs(session.run([rewards], {states: test_set})))))\n",
    "print('Maximum value: {}'.format(np.max(np.abs(session.run([values], {states: test_set})))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 2500\n",
    "initial_condition = np.array([0., 0.01, 0., 0.]).dot(Ts_inv)\n",
    "\n",
    "trajectory_rl = compute_trajectory(scaled_dynamics, policy, initial_condition, steps, None, dim_input)\n",
    "trajectory_lqr = compute_trajectory(scaled_dynamics, lqr_policy, initial_condition, steps, None, dim_input)\n",
    "    \n",
    "# Return to original dynamics scale\n",
    "trajectory_rl = trajectory_rl.dot(Ts)\n",
    "trajectory_lqr = trajectory_lqr.dot(Ts)\n",
    "    \n",
    "fig = plt.figure(1, figsize=(15, 10), dpi=70)\n",
    "for i in range(dim_state):\n",
    "    ax = fig.add_subplot(2, 2, i+1)\n",
    "    ax.plot(trajectory_rl[:, i], 'r')\n",
    "    ax.plot(trajectory_lqr[:, i], 'b')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 2500\n",
    "initial_condition = np.array([0., 0.01, 0., 0.]).dot(Ts_inv)\n",
    "\n",
    "trajectory_rl = compute_trajectory(scaled_dynamics, policy, initial_condition, steps, 'step', dim_input)\n",
    "trajectory_lqr = compute_trajectory(scaled_dynamics, lqr_policy, initial_condition, steps, 'step', dim_input)\n",
    "    \n",
    "# Return to original dynamics scale\n",
    "trajectory_rl = trajectory_rl.dot(Ts)\n",
    "trajectory_lqr = trajectory_lqr.dot(Ts)\n",
    "    \n",
    "fig = plt.figure(1, figsize=(15, 10), dpi=70)\n",
    "for i in range(dim_state):\n",
    "    ax = fig.add_subplot(2, 2, i+1)\n",
    "    ax.plot(trajectory_rl[:, i], 'r')\n",
    "    ax.plot(trajectory_lqr[:, i], 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting.show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
