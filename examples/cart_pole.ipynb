{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for a Cart-Pole System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.linalg import block_diag\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib notebook\n",
    "\n",
    "import safe_learning\n",
    "from utilities import uniform_state_sampler, compute_closedloop_response, CartPole, get_max_parameter_change\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x\n",
    "    \n",
    "np_dtype = safe_learning.config.np_dtype\n",
    "tf_dtype = safe_learning.config.dtype\n",
    "\n",
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "session = tf.InteractiveSession()\n",
    "initialized = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System parameters\n",
    "m = 0.1     # pendulum mass\n",
    "M = 5.      # cart mass\n",
    "L = 0.5     # pole length\n",
    "dt = 0.01   # sampling time\n",
    "g = 9.81    # gravity\n",
    "\n",
    "# State and action normalizers\n",
    "x_max = 10\n",
    "theta_max = np.deg2rad(10)\n",
    "x_dot_max = 5\n",
    "theta_dot_max = np.deg2rad(5)\n",
    "u_max = 5*g*m*theta_max\n",
    "\n",
    "state_norm = (x_max, theta_max, x_dot_max, theta_dot_max)\n",
    "input_norm = (u_max,)\n",
    "\n",
    "# Define system and dynamics\n",
    "cart_pole = CartPole(m, M, L, dt, [state_norm, input_norm])\n",
    "state_space = [[-1, 1]]*cart_pole.state_dim\n",
    "\n",
    "Ad, Bd = cart_pole.linearize()   \n",
    "dynamics = safe_learning.functions.LinearSystem((Ad, Bd), name='dynamics')\n",
    "\n",
    "print(Ad)\n",
    "print(Bd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State cost matrix\n",
    "Q = np.identity(cart_pole.state_dim)\n",
    "\n",
    "# Action cost matrix\n",
    "R = np.identity(cart_pole.input_dim)\n",
    "\n",
    "# Quadratic reward (-cost) function\n",
    "reward_function = safe_learning.QuadraticFunction(block_diag(-Q, -R), name='reward_function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact Optimal Policy and Value Function for the True Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve Lyapunov equation for the exact value function matrix P and optimal feedback law u = -K.dot(s)\n",
    "K, P = safe_learning.utilities.dlqr(Ad, Bd, Q, R)\n",
    "print('Optimal linear gain:\\n{}\\n'.format(-K))\n",
    "print('Quadratic value function:\\n{}'.format(P))\n",
    "\n",
    "# LQR policy\n",
    "lqr_policy = safe_learning.functions.LinearSystem(-K, name='LQR_policy')\n",
    "\n",
    "# Optimal value function\n",
    "lqr_value_function = safe_learning.functions.QuadraticFunction(-P, name='LQR_value_function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Approximators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value function\n",
    "layer_dims = [64, 64, 1]\n",
    "activations = [tf.nn.relu, tf.nn.relu, None]\n",
    "value_function = safe_learning.functions.NeuralNetwork(layer_dims, activations, name='value_function')\n",
    "\n",
    "# Policy\n",
    "layer_dims = [64, 64, 1]\n",
    "activations = [tf.nn.relu, tf.nn.relu, tf.nn.tanh]\n",
    "policy = safe_learning.functions.NeuralNetwork(layer_dims, activations, scaling=10., name='policy')\n",
    "\n",
    "# For testing\n",
    "# value_function = lqr_value_function\n",
    "policy = lqr_policy\n",
    "\n",
    "# TensorFlow graph\n",
    "states = tf.placeholder(tf_dtype, shape=[None, cart_pole.state_dim], name='states')\n",
    "actions = policy(states)\n",
    "rewards = reward_function(states, actions)\n",
    "values = value_function(states)\n",
    "future_values = value_function(dynamics(states, actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning on a \"Wrong\" Model\n",
    "\n",
    "Create a cart-pole system with \"wrong\" parameters. Fit our parametric value function to the corresponding LQR solution in a supervised fashion as a starting point before tackling policy iteration for the real system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Wrong\" system\n",
    "m = 0.2     # pendulum mass\n",
    "M = 4.5     # cart mass\n",
    "L = 0.5     # pole length\n",
    "wrong_cart_pole = CartPole(m, M, L, dt, [state_norm, input_norm])\n",
    "Ad_wrong, Bd_wrong = wrong_cart_pole.linearize()\n",
    "wrong_dynamics = safe_learning.LinearSystem((Ad_wrong, Bd_wrong), name='wrong_dynamics')\n",
    "K_wrong, P_wrong = safe_learning.utilities.dlqr(Ad_wrong, Bd_wrong, Q, R)\n",
    "\n",
    "# wrong_lqr_policy = safe_learning.functions.LinearSystem(-K, name='wrong_LQR_policy')\n",
    "wrong_lqr_value_function = safe_learning.functions.QuadraticFunction(-P, name='wrong_LQR_value_function')\n",
    "\n",
    "with tf.name_scope('supervised_learning'):\n",
    "    supervised_learning_rate = tf.placeholder(tf_dtype, shape=[], name='learning_rate')\n",
    "    wrong_values = wrong_lqr_value_function(states)\n",
    "    \n",
    "    scaling = np.abs(1 / session.run(wrong_values, {states: 0.1*np.ones((1, wrong_cart_pole.state_dim))}))\n",
    "    scaling = 1.\n",
    "    \n",
    "    supervised_obj = scaling*tf.reduce_mean(tf.abs(values - wrong_values), name='objective')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(supervised_learning_rate)\n",
    "    wrong_value_update = optimizer.minimize(supervised_obj, var_list=value_function.parameters)\n",
    "    \n",
    "print(scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow this cell to be run repeatedly to continue training if desired\n",
    "if not initialized:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    initialized = True\n",
    "    \n",
    "# Training hyperparameters\n",
    "max_iters = 100\n",
    "batch_size = 1e3\n",
    "alpha = 1e-4\n",
    "test_set = uniform_state_sampler(1e5, state_space)\n",
    "\n",
    "# Record objective values over time\n",
    "supervised_obj_eval = np.zeros(max_iters + 1)\n",
    "supervised_obj_eval[0] = session.run(supervised_obj, {states: test_set})\n",
    "\n",
    "# TODO convergence check\n",
    "converged = False\n",
    "tol = 1e-2\n",
    "\n",
    "for i in tqdm(range(max_iters)):\n",
    "    batch = uniform_state_sampler(batch_size, state_space)\n",
    "    session.run(wrong_value_update, feed_dict={states: batch, supervised_learning_rate: alpha})\n",
    "    supervised_obj_eval[i+1] = session.run(supervised_obj, {states: test_set})\n",
    "\n",
    "final_iter = i+1\n",
    "if converged:\n",
    "    print('Converged after {} iterations.'.format(final_iter))\n",
    "else:\n",
    "    print('Did not converge!')\n",
    "    \n",
    "fig = plt.figure(figsize=(10, 5), dpi=100)\n",
    "fig.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(supervised_obj_eval[:final_iter+1], '.-r')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Supervised Learning Objective')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5), dpi=100)\n",
    "fig.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "n_points = [51, 51]\n",
    "xx, yy = np.mgrid[-1:1:np.complex(0, n_points[0]), -1:1:np.complex(0, n_points[1])]\n",
    "\n",
    "#\n",
    "grid = np.column_stack((xx.ravel(), yy.ravel(), np.zeros_like(xx.ravel()), np.zeros_like(xx.ravel())))\n",
    "true_values = session.run(wrong_values, feed_dict={states: grid}).reshape(n_points)\n",
    "learned_values = session.run(values, feed_dict={states: grid}).reshape(n_points)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax.plot_surface(xx, yy, true_values, color='red', alpha=0.75)\n",
    "ax.plot_surface(xx, yy, learned_values, color='blue', alpha=0.75)\n",
    "ax.set_title(r'Value function, $\\dot{x} = \\dot{\\theta} = 0$', fontsize=18)\n",
    "ax.set_xlabel(r'$x$', fontsize=14)\n",
    "ax.set_ylabel(r'$\\theta$', fontsize=14)\n",
    "ax.set_zlabel(r'$V(s)$', fontsize=14)\n",
    "\n",
    "#\n",
    "grid = np.column_stack((np.zeros_like(xx.ravel()), np.zeros_like(xx.ravel()), xx.ravel(), yy.ravel()))\n",
    "true_values = session.run(wrong_values, feed_dict={states: grid}).reshape(n_points)\n",
    "learned_values = session.run(values, feed_dict={states: grid}).reshape(n_points)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax.plot_surface(xx, yy, true_values, color='red', alpha=0.75)\n",
    "ax.plot_surface(xx, yy, learned_values, color='blue', alpha=0.75)\n",
    "ax.set_title(r'Value function, $x = \\theta = 0$', fontsize=18)\n",
    "ax.set_xlabel(r'$\\dot{x}$', fontsize=14)\n",
    "ax.set_ylabel(r'$\\dot{\\theta}$', fontsize=14)\n",
    "ax.set_zlabel(r'$V(s)$', fontsize=14)\n",
    "\n",
    "lipschitz = session.run(value_function.lipschitz())\n",
    "print(lipschitz)\n",
    "print(session.run(values, {states: np.array([-1., -1., -1., -1.]).reshape(1, -1)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Objectives\n",
    "\n",
    "Now back to the true cart-pole system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the state and input spaces to compute the maximum absolute reward\n",
    "max_state = np.ones(cart_pole.state_dim).reshape((1, -1))\n",
    "input_multiplier = 1.\n",
    "max_input = input_multiplier*np.ones(cart_pole.input_dim).reshape((1, -1))\n",
    "\n",
    "gamma = 1.\n",
    "r_max = max_state.dot(Q).dot(max_state.T) + max_input.dot(R).dot(max_input.T)\n",
    "scaling = 1.\n",
    "\n",
    "print('Maximum reward (abs): {}'.format(r_max))\n",
    "print('Scaling: {}'.format(scaling))\n",
    "\n",
    "# Bellman error objective for value update\n",
    "with tf.name_scope('value_optimization'):\n",
    "    value_learning_rate = tf.placeholder(tf_dtype, shape=[], name='learning_rate')\n",
    "    target = tf.stop_gradient(rewards + gamma*future_values, name='target')\n",
    "    value_obj = scaling*tf.reduce_mean(tf.abs(values - target), name='objective')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(value_learning_rate)\n",
    "    value_update = optimizer.minimize(value_obj, var_list=value_function.parameters)\n",
    "\n",
    "# Pseudo-integration objective for policy update\n",
    "with tf.name_scope('policy_optimization'):\n",
    "    policy_learning_rate = tf.placeholder(tf_dtype, shape=[], name='learning_rate')\n",
    "    policy_obj = -scaling*tf.reduce_mean(rewards + gamma*future_values, name='objective')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(policy_learning_rate)\n",
    "#     policy_update = optimizer.minimize(policy_obj, var_list=policy.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow this cell to be run repeatedly to continue training if desired\n",
    "if not initialized:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    initialized = True\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 50\n",
    "batch_size = 1e3\n",
    "test_set = uniform_state_sampler(1e5, state_space)\n",
    "\n",
    "value_iters = 1\n",
    "value_alpha = 1e-3\n",
    "value_tol = 1e-1\n",
    "\n",
    "policy_iters = 50\n",
    "policy_alpha = 1e-1\n",
    "policy_tol = 1e-1\n",
    "\n",
    "# Record objective values over time\n",
    "value_obj_eval = np.zeros(max_iters + 1)\n",
    "value_obj_eval[0] = session.run(value_obj, {states: test_set})\n",
    "\n",
    "policy_obj_eval = np.zeros(max_iters + 1)\n",
    "policy_obj_eval[0] = session.run(policy_obj, {states: test_set})\n",
    "\n",
    "# For convergence, check the parameter values\n",
    "converged = False\n",
    "tol = 1e-2\n",
    "value_param_changes = np.zeros(max_iters)\n",
    "policy_param_changes = np.zeros(max_iters)\n",
    "old_value_params = session.run(value_function.parameters)\n",
    "# old_policy_params = session.run(policy.parameters)\n",
    "\n",
    "for i in tqdm(range(max_iters)):\n",
    "    # Value update\n",
    "    for _ in range(value_iters):\n",
    "        batch = uniform_state_sampler(batch_size, state_space)\n",
    "        session.run(value_update, feed_dict={states: batch, value_learning_rate: value_alpha})\n",
    "\n",
    "    # Policy update\n",
    "#     for _ in range(policy_iters):\n",
    "#         batch = uniform_state_sampler(batch_size, state_space)\n",
    "#         session.run(policy_update, feed_dict={states: batch, policy_learning_rate: policy_alpha})\n",
    "        \n",
    "    value_obj_eval[i+1] = session.run(value_obj, {states: test_set})\n",
    "    policy_obj_eval[i+1] = session.run(policy_obj, {states: test_set})\n",
    "    \n",
    "    # To save time during debugging ...\n",
    "    if np.isnan(value_obj_eval[i+1]) or np.isnan(policy_obj_eval[i+1]):\n",
    "        raise ValueError('Encountered NAN value after {} iterations!'.format(i+1))\n",
    "    \n",
    "    # Value function parameter changes\n",
    "    new_value_params = session.run(value_function.parameters)\n",
    "    value_param_changes[i] = get_max_parameter_change(old_value_params, new_value_params)\n",
    "    old_value_params = new_value_params\n",
    "    \n",
    "    # Policy parameter changes\n",
    "#     new_policy_params = session.run(policy.parameters)\n",
    "#     policy_param_changes[i] = get_max_parameter_change(old_policy_params, new_policy_params)\n",
    "#     old_policy_params = new_policy_params\n",
    "    \n",
    "    # Break if converged\n",
    "#     if value_param_changes[i] <= value_tol and policy_param_changes[i] <= policy_tol:\n",
    "#         converged = True\n",
    "#         break\n",
    "\n",
    "final_iter = i+1\n",
    "if converged:\n",
    "    print('Converged after {} iterations.'.format(final_iter))\n",
    "else:\n",
    "    print('Did not converge!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5), dpi=100)\n",
    "fig.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "ax = fig.add_subplot(221)\n",
    "ax.plot(value_obj_eval[:final_iter+1], '.-r')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Value Function Objective')\n",
    "\n",
    "# ax = fig.add_subplot(222)\n",
    "# ax.plot(policy_obj_eval[:final_iter+1], '.-r')\n",
    "# ax.set_xlabel('Iteration')\n",
    "# ax.set_ylabel('Policy Objective')\n",
    "\n",
    "ax = fig.add_subplot(223)\n",
    "ax.plot(value_param_changes[:final_iter+1], '.-b')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Max. Value Param. Change')\n",
    "\n",
    "# ax = fig.add_subplot(224)\n",
    "# ax.plot(policy_param_changes[:final_iter+1], '.-b')\n",
    "# ax.set_xlabel('Iteration')\n",
    "# ax.set_ylabel('Max. Policy Param. Change')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learned Policy: Zero-Input and Step Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_names = [r'$x$', r'$\\theta$', r'$\\dot{x}$', r'$\\dot{\\theta}$']\n",
    "plot_idx = (1, 2, 4, 5)\n",
    "steps = 2000\n",
    "\n",
    "# Zero-input response to initial angle offset\n",
    "initial_condition = np.array([0.9, 0.3, -0.1, 0.1])\n",
    "true_state_traj_zero, true_input_traj_zero = compute_closedloop_response(dynamics, lqr_policy, initial_condition, \n",
    "                                                                         steps, 0)\n",
    "learned_state_traj_zero, learned_input_traj_zero = compute_closedloop_response(dynamics, policy, initial_condition, \n",
    "                                                                               steps, 0)\n",
    "fig = plt.figure(figsize=(10, 5), dpi=100)\n",
    "fig.suptitle('Zero-Input Response', fontsize=18)\n",
    "fig.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "for i in range(cart_pole.state_dim):\n",
    "    ax = fig.add_subplot(2, 3, plot_idx[i])\n",
    "    ax.plot(learned_state_traj_zero[:, i], 'b')\n",
    "    ax.plot(true_state_traj_zero[:, i], 'r')\n",
    "    ax.set_xlabel(r'$t$', fontsize=14)\n",
    "    ax.set_ylabel(state_names[i], fontsize=14)\n",
    "ax = fig.add_subplot(2, 3, 3)\n",
    "learned = ax.plot(learned_input_traj_zero, 'b')\n",
    "true = ax.plot(true_input_traj_zero, 'r')\n",
    "ax.set_xlabel(r'$t$', fontsize=14)\n",
    "ax.set_ylabel(r'$u$', fontsize=14)\n",
    "fig.legend((learned[0], true[0]), ('Learned', 'True'), loc=(0.75, 0.2), fontsize=14)\n",
    "\n",
    "\n",
    "# Step response, zero initial condition\n",
    "initial_condition = np.array([0., 0., 0., 0.])\n",
    "r = 1\n",
    "learned_state_traj_step, learned_input_traj_step = compute_closedloop_response(dynamics, policy, initial_condition,\n",
    "                                                                               steps, r)\n",
    "true_state_traj_step, true_input_traj_step = compute_closedloop_response(dynamics, lqr_policy, initial_condition,\n",
    "                                                                         steps, r)\n",
    "fig = plt.figure(figsize=(10, 5), dpi=100)\n",
    "fig.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "fig.suptitle('Step Response', fontsize=18)\n",
    "for i in range(cart_pole.state_dim):\n",
    "    ax = fig.add_subplot(2, 3, plot_idx[i])\n",
    "    ax.plot(learned_state_traj_step[:, i], 'b')\n",
    "    ax.plot(true_state_traj_step[:, i], 'r')\n",
    "    ax.set_xlabel(r'$t$', fontsize=14)\n",
    "    ax.set_ylabel(state_names[i], fontsize=14)\n",
    "ax = fig.add_subplot(2, 3, 3)\n",
    "learned = ax.plot(learned_input_traj_step, 'b')\n",
    "true = ax.plot(true_input_traj_step, 'r')\n",
    "ax.set_xlabel(r'$t$', fontsize=14)\n",
    "ax.set_ylabel(r'$u$', fontsize=14)\n",
    "fig.legend((learned[0], true[0]), ('Learned', 'True'), loc=(0.75, 0.2), fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function and Policy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 4), dpi=100)\n",
    "fig.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "n_points = [51, 51]\n",
    "xx, yy = np.mgrid[-1:1:np.complex(0, n_points[0]), -1:1:np.complex(0, n_points[1])]\n",
    "# scaling = r_max / (1 - gamma)\n",
    "scaling = 1.\n",
    "\n",
    "#\n",
    "grid = np.column_stack((xx.ravel(), yy.ravel(), np.zeros_like(xx.ravel()), np.zeros_like(yy.ravel())))\n",
    "true_values = session.run(lqr_value_function(states), feed_dict={states: grid}).reshape(n_points)\n",
    "learned_values = session.run(values, feed_dict={states: grid}).reshape(n_points)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax.plot_surface(xx, yy, true_values, color='red', alpha=0.75)\n",
    "ax.plot_surface(xx, yy, learned_values, color='blue', alpha=0.75)\n",
    "ax.set_title(r'Value function, $\\dot{x} = \\dot{\\theta} = 0$', fontsize=18)\n",
    "ax.set_xlabel(r'$x$', fontsize=14)\n",
    "ax.set_ylabel(r'$\\theta$', fontsize=14)\n",
    "ax.set_zlabel(r'$V(s)$', fontsize=14)\n",
    "\n",
    "#\n",
    "grid = np.column_stack((np.zeros_like(xx.ravel()), np.zeros_like(yy.ravel()), xx.ravel(), yy.ravel()))\n",
    "true_values = session.run(lqr_value_function(states), feed_dict={states: grid}).reshape(n_points)\n",
    "learned_values = session.run(values, feed_dict={states: grid}).reshape(n_points)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax.plot_surface(xx, yy, true_values, color='red', alpha=0.75)\n",
    "ax.plot_surface(xx, yy, learned_values, color='blue', alpha=0.75)\n",
    "ax.set_title(r'Value function, $x = \\theta = 0$', fontsize=18)\n",
    "ax.set_xlabel(r'$\\dot{x}$', fontsize=14)\n",
    "ax.set_ylabel(r'$\\dot{\\theta}$', fontsize=14)\n",
    "ax.set_zlabel(r'$V(s)$', fontsize=14)\n",
    "\n",
    "\n",
    "# #\n",
    "# true_control = session.run(lqr_policy(states), feed_dict={states: grid})\n",
    "# learned_control = session.run(policy(states), feed_dict={states: grid})\n",
    "# true_effort = -np.sum(true_control.dot(R)*true_control, axis=1, keepdims=True).reshape(n_points)\n",
    "# learned_effort = -np.sum(learned_control.dot(R)*learned_control, axis=1, keepdims=True).reshape(n_points)\n",
    "\n",
    "# ax = fig.add_subplot(2, 2, 3, projection='3d')\n",
    "# ax.plot_surface(xx, yy, true_effort, color='red', alpha=0.75)\n",
    "# ax.plot_surface(xx, yy, learned_effort, color='blue', alpha=0.75)\n",
    "# ax.set_title(r'Control effort, $\\dot{x} = \\dot{\\theta} = 0$', fontsize=18)\n",
    "# ax.set_xlabel(r'$x$', fontsize=14)\n",
    "# ax.set_ylabel(r'$\\theta$', fontsize=14)\n",
    "# ax.set_zlabel(r'$r(0,u)$', fontsize=14)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #\n",
    "# true_control = session.run(lqr_policy(states), feed_dict={states: grid})\n",
    "# learned_control = session.run(policy(states), feed_dict={states: grid})\n",
    "# true_effort = -np.sum(true_control.dot(R)*true_control, axis=1, keepdims=True).reshape(n_points)\n",
    "# learned_effort = -np.sum(learned_control.dot(R)*learned_control, axis=1, keepdims=True).reshape(n_points)\n",
    "\n",
    "# ax = fig.add_subplot(2, 2, 4, projection='3d')\n",
    "# ax.plot_surface(xx, yy, true_effort, color='red', alpha=0.75)\n",
    "# ax.plot_surface(xx, yy, learned_effort, color='blue', alpha=0.75)\n",
    "# ax.set_title(r'Control effort, $x = \\theta = 0$', fontsize=18)\n",
    "# ax.set_xlabel(r'$\\dot{x}$', fontsize=14)\n",
    "# ax.set_ylabel(r'$\\dot{\\theta}$', fontsize=14)\n",
    "# ax.set_zlabel(r'$r(0,u)$', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting.show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
