{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import cont2discrete\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "import safe_learning\n",
    "import plotting\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_state_sampler(n_samples, domain, scaling=None):\n",
    "    \"\"\"\n",
    "    Samples uniformly from a box in the state space.\n",
    "    \"\"\"\n",
    "    dim_state = len(domain)\n",
    "    n_samps = int(n_samples)\n",
    "    for i in range(dim_state):\n",
    "        delta = domain[i][1]-domain[i][0]\n",
    "        offset = domain[i][0]\n",
    "        single_dim = delta*np.random.rand(n_samps, 1) + offset\n",
    "        if scaling is not None:\n",
    "            single_dim = single_dim / scaling[i]\n",
    "        if i == 0:\n",
    "            samples = single_dim\n",
    "        else:\n",
    "            samples = np.concatenate((samples, single_dim), axis=1)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Linearized cart-pole system\n",
    "\n",
    "# System parameters\n",
    "m = 10      # pendulum mass\n",
    "M = 10      # cart mass\n",
    "L = 100       # pole length\n",
    "g = 9.81    # gravitational constant\n",
    "\n",
    "# Linearized continuous-time system matrices for state vector:\n",
    "# s = (q, q_dot) = (x, theta, x_dot, theta_dot)\n",
    "dim_state = 4\n",
    "dim_input = 1\n",
    "A = np.array([[0, 0,             1, 0],\n",
    "              [0, 0,             0, 1],\n",
    "              [0, g*m/M,         0, 0],\n",
    "              [0, g*(m+M)/(L*M), 0, 0]])\n",
    "B = np.array([0, 0, 1/M, 1/(M*L)]).reshape(-1, dim_input)\n",
    "\n",
    "# Discretized system matrices\n",
    "Ts = 0.01    # sampling time\n",
    "Ad, Bd, _, _, _ = cont2discrete((A, B, 0, 0), Ts, method='zoh')\n",
    "\n",
    "# Domain and scaling\n",
    "x_max = 10\n",
    "theta_max = np.deg2rad(45)\n",
    "x_dot_max = 5\n",
    "u_max = g*m*theta_max\n",
    "theta_dot_max = (g*(m+M)*theta_max + u_max)/(M*L)\n",
    "\n",
    "domain = [[-x_max, x_max], [-theta_max, theta_max],\n",
    "          [-x_dot_max, x_dot_max], [-theta_dot_max, theta_dot_max]]\n",
    "state_scaling = [x_max, theta_max, x_dot_max, theta_dot_max]\n",
    "action_scaling = [u_max]\n",
    "\n",
    "Ts = np.diag(state_scaling)\n",
    "Tu = np.diag(action_scaling)\n",
    "Ts_inv = np.diag([1/i for i in state_scaling])\n",
    "Tu_inv = np.diag([1/i for i in action_scaling])\n",
    "A_scl = Ts_inv.dot(A).dot(Ts)\n",
    "B_scl = Ts_inv.dot(B).dot(Tu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost matrices and exact value function\n",
    "\n",
    "# State cost matrix\n",
    "Q = 2*np.identity(dim_state)\n",
    "\n",
    "# Input cost matrix\n",
    "R = np.identity(dim_input)\n",
    "\n",
    "# Solve Lyapunov equation for the exact value function and optimal feedback law u = -K.dot(x)\n",
    "K, P = safe_learning.utilities.dlqr(A_scl, B_scl, Q, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow functions for the cost function and the scaled, discretized, linearized dynamics\n",
    "\n",
    "@safe_learning.utilities.with_scope('cost_function')\n",
    "def cost_function(states, actions):\n",
    "    costs = tf.reduce_sum(tf.matmul(states, Q)*states + tf.matmul(actions, R)*actions,\n",
    "                          axis=1,\n",
    "                          keep_dims=True)\n",
    "    return costs\n",
    "\n",
    "\n",
    "@safe_learning.utilities.with_scope('dynamics')\n",
    "def dynamics(states, actions):\n",
    "    future_states = tf.matmul(states, A_scl.T) + tf.matmul(actions, B_scl.T)\n",
    "    return future_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function approximators\n",
    "\n",
    "# Value function\n",
    "layer_dims = [64, 64, 1]\n",
    "activations = [tf.nn.relu, tf.nn.relu, None]\n",
    "value_function = safe_learning.functions.NeuralNetwork(layer_dims,\n",
    "                                                       activations,\n",
    "                                                       name='value_function')\n",
    "\n",
    "# Policy\n",
    "layer_dims = [1]\n",
    "activations = [None]\n",
    "# initializer = tf.constant_initializer(-K.T)\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "policy = safe_learning.functions.NeuralNetwork(layer_dims,\n",
    "                                               activations,\n",
    "                                               name='policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# States, actions, stage costs, and values\n",
    "\n",
    "tf.reset_default_graph()\n",
    "dtype = safe_learning.config.dtype\n",
    "\n",
    "states = tf.placeholder(dtype, shape=[None, dim_state], name='states')\n",
    "actions = policy(states)\n",
    "\n",
    "costs = cost_function(states, actions)\n",
    "future_states = dynamics(states, actions)\n",
    "\n",
    "values = value_function(states)\n",
    "future_values = value_function(future_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "gamma = 0.98\n",
    "value_learning_rate = 1e-5\n",
    "policy_learning_rate = 1e-3\n",
    "\n",
    "# TODO scale objectives for value and policy updates appropriately\n",
    "\n",
    "# Bellman error objective for value update\n",
    "with tf.name_scope('value_optimization'):\n",
    "    target = tf.stop_gradient(costs + gamma*future_values, name='target')\n",
    "    value_obj = tf.reduce_mean(tf.square(values - target), name='objective')\n",
    "    value_summary = tf.summary.scalar('value_objective',\n",
    "                                      value_obj)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(value_learning_rate)\n",
    "    value_update = optimizer.minimize(value_obj,\n",
    "                                      var_list=value_function.parameters)\n",
    "\n",
    "# Pseudo-integration objective for policy update\n",
    "with tf.name_scope('policy_optimization'):\n",
    "    policy_obj = tf.reduce_mean(costs + gamma*future_values, name='objective')\n",
    "    policy_summary = tf.summary.scalar('policy_objective',\n",
    "                                       policy_obj)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(policy_learning_rate)\n",
    "    policy_update = optimizer.minimize(policy_obj,\n",
    "                                       var_list=policy.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow session\n",
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "session = tf.InteractiveSession()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "max_iters = 25\n",
    "batch_size = 1e3\n",
    "test_set = uniform_state_sampler(1e5, domain, state_scaling)\n",
    "\n",
    "value_iters = 200\n",
    "value_tol = 1e-1\n",
    "\n",
    "policy_iters = 50\n",
    "policy_tol = 1e-1\n",
    "\n",
    "converged = False\n",
    "value_obj_values = np.zeros(max_iters+1)\n",
    "policy_obj_values = np.zeros(max_iters+1)\n",
    "value_obj_values[0] = session.run(value_obj, {states: test_set})\n",
    "policy_obj_values[0] = session.run(policy_obj, {states: test_set})\n",
    "\n",
    "for i in tqdm(range(max_iters)):\n",
    "\n",
    "    for _ in range(value_iters):\n",
    "        batch = uniform_state_sampler(batch_size, domain, state_scaling)\n",
    "        session.run(value_update, feed_dict={states: batch})\n",
    "\n",
    "    for _ in range(policy_iters):\n",
    "        batch = uniform_state_sampler(batch_size, domain, state_scaling)\n",
    "        session.run(policy_update, feed_dict={states: batch})\n",
    "\n",
    "    # Get new objective values\n",
    "    value_obj_values[i+1] = session.run(value_obj, {states: test_set})\n",
    "    policy_obj_values[i+1] = session.run(policy_obj, {states: test_set})\n",
    "\n",
    "\n",
    "    if np.isnan(value_obj_values[i+1]) or np.isnan(policy_obj_values[i+1]):\n",
    "        session.close()\n",
    "        raise ValueError('Encountered NAN value on iteration {}!'.format(i))\n",
    "\n",
    "    # Compute objective changes\n",
    "    value_change = value_obj_values[i+1] - value_obj_values[i]\n",
    "    policy_change = policy_obj_values[i+1] - policy_obj_values[i]\n",
    "\n",
    "    # Stop conditions\n",
    "    minimal_change = ((np.abs(value_change) <= value_tol) and\n",
    "                      (np.abs(policy_change) <= policy_tol))\n",
    "    decrease = (value_change <= 0) and (policy_change <= 0)\n",
    "\n",
    "    # Break if converged\n",
    "    if minimal_change:\n",
    "        converged = True\n",
    "        break\n",
    "\n",
    "final_iter = i+1\n",
    "if converged:\n",
    "    print('Converged after {} iterations.'.format(final_iter))\n",
    "else:\n",
    "    print('\\nDid not converge!')\n",
    "print('value objective: {} \\npolicy objective: {}'.format(value_obj_values[i+1], policy_obj_values[i+1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(value_obj_values[:final_iter+1])\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.plot(policy_obj_values[:final_iter+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotting.show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
