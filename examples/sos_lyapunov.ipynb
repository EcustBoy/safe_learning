{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyapunov Neural Network vs. SOS Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import safe_learning\n",
    "from scipy.linalg import solve_discrete_lyapunov\n",
    "from utilities import VanDerPol, LyapunovNetwork\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TODO testing ****************************************#\n",
    "class Options(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Options, self).__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "OPTIONS = Options(np_dtype           = safe_learning.config.np_dtype,\n",
    "                  tf_dtype           = safe_learning.config.dtype,\n",
    "                  eps                = 1e-8,\n",
    "                  use_zero_threshold = True,\n",
    "                  pre_train          = False,\n",
    "                  dpi                = 150,\n",
    "                  fontproperties     = FontProperties(size=10),\n",
    "                  save_figs          = False,\n",
    "                  fig_path           = 'figures/sos_lyapunov/')\n",
    "#******************************************************#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOS Lyapunov Function (from SOSTOOLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monomials(x, deg):\n",
    "    x = np.atleast_2d(x)\n",
    "    # 1-D features (x, y)\n",
    "    Z = np.copy(x)\n",
    "    if deg >= 2:\n",
    "        # 2-D features (x^2, x * y, y^2)\n",
    "        temp = np.empty([len(x), 3])\n",
    "        temp[:, 0] = x[:, 0] ** 2 \n",
    "        temp[:, 1] = x[:, 0] * x[:, 1]\n",
    "        temp[:, 2] = x[:, 1] ** 2\n",
    "        Z = np.hstack((Z, temp))\n",
    "    if deg >= 3:\n",
    "        # 3-D features (x^3, x^2 * y, x * y^2, y^3)\n",
    "        temp = np.empty([len(x), 4])\n",
    "        temp[:, 0] = x[:, 0] ** 3\n",
    "        temp[:, 1] = (x[:, 0] ** 2) * x[:, 1]\n",
    "        temp[:, 2] = x[:, 0] * (x[:, 1] ** 2)\n",
    "        temp[:, 3] = x[:, 1] ** 3\n",
    "        Z = np.hstack((Z, temp))\n",
    "    if deg >= 4:\n",
    "        # 4-D features (x^4, x^3 * y, x^2 * y^2, x * y^3, y^4)\n",
    "        temp = np.empty([len(x), 5])\n",
    "        temp[:, 0] = x[:, 0] ** 4\n",
    "        temp[:, 1] = (x[:, 0] ** 3) * x[:, 1]\n",
    "        temp[:, 2] = (x[:, 0] ** 2) * (x[:, 1] ** 2)\n",
    "        temp[:, 3] = x[:, 0] * (x[:, 1] ** 3)\n",
    "        temp[:, 4] = x[:, 1] ** 4\n",
    "        Z = np.hstack((Z, temp))\n",
    "    return Z\n",
    "\n",
    "\n",
    "def sos_lyapunov(x, deg):\n",
    "    Z = monomials(x, deg)\n",
    "    if deg==1:\n",
    "        Q = np.array([[  2.706, -1.012],\n",
    "                      [ -1.012,  2.675]])\n",
    "    elif deg==2:\n",
    "        Q = np.array([[     3.546,    -1.654,   2.75e-9, -2.662e-9,  1.46e-9],\n",
    "                      [    -1.654,     3.136, -2.662e-9,   1.46e-9, 1.309e-9],\n",
    "                      [   2.75e-9, -2.662e-9,      1.13,  -0.01511,    1.064],\n",
    "                      [ -2.662e-9,   1.46e-9,  -0.01511,     1.064,   -1.318],\n",
    "                      [   1.46e-9,  1.309e-9,     1.064,    -1.318,   0.9461]])\n",
    "    elif deg==3:\n",
    "        Q = np.array([\n",
    "            [      6.301,     -3.172,   1.053e-7, -1.463e-10,    1.0e-8,   -0.2146,  -0.01918,    0.6756,   -0.4285],\n",
    "            [     -3.172,      4.386, -1.463e-10,     1.0e-8, -6.955e-9,  -0.01918,    0.6756,   -0.4285,    0.2466],\n",
    "            [   1.053e-7, -1.463e-10,    -0.2146,   -0.01918,    0.6756, -2.935e-8,  3.347e-9, -1.397e-8,  7.877e-9],\n",
    "            [ -1.463e-10,     1.0e-8,   -0.01918,     0.6756,   -0.4285,  3.347e-9, -1.397e-8,  7.877e-9, -6.379e-9],\n",
    "            [     1.0e-8,  -6.955e-9,     0.6756,    -0.4285,    0.2466, -1.397e-8,  7.877e-9, -6.379e-9,   2.45e-9],\n",
    "            [    -0.2146,   -0.01918,  -2.935e-8,   3.347e-9, -1.397e-8,     0.341,    0.2417,   -0.1313,   -0.1159],\n",
    "            [   -0.01918,     0.6756,   3.347e-9,  -1.397e-8,  7.877e-9,    0.2417,   -0.1313,   -0.1159,     0.193],\n",
    "            [     0.6756,    -0.4285,  -1.397e-8,   7.877e-9, -6.379e-9,   -0.1313,   -0.1159,     0.193,   -0.1061],\n",
    "            [    -0.4285,     0.2466,   7.877e-9,  -6.379e-9,   2.45e-9,   -0.1159,     0.193,   -0.1061,   0.01121]])\n",
    "    elif deg==4:\n",
    "        Q = np.array([\n",
    "            [      24.26,      -14.7,   7.798e-9,  -2.459e-8,   2.035e-8,     -1.386,     0.8215,      0.292,   -0.06087,   2.203e-9,   3.081e-9,  -1.233e-9, -3.724e-10,  2.847e-11],\n",
    "            [      -14.7,      15.45,  -2.459e-8,   2.035e-8,   -1.38e-8,     0.8215,      0.292,   -0.06087,   0.003462,   3.081e-9,  -1.233e-9, -3.724e-10,  2.847e-11, -1.962e-11],\n",
    "            [   7.798e-9,  -2.459e-8,     -1.386,     0.8215,      0.292,   2.203e-9,   3.081e-9,  -1.233e-9, -3.724e-10,     0.1009,      0.072,    0.01911,   -0.01406,   0.002982],\n",
    "            [  -2.459e-8,   2.035e-8,     0.8215,      0.292,   -0.06087,   3.081e-9,  -1.233e-9, -3.724e-10,  2.847e-11,      0.072,    0.01911,   -0.01406,   0.002982,  0.0002575],\n",
    "            [   2.035e-8,   -1.38e-8,      0.292,   -0.06087,   0.003462,  -1.233e-9, -3.724e-10,  2.847e-11, -1.962e-11,    0.01911,   -0.01406,   0.002982,  0.0002575, -0.0001809],\n",
    "            [     -1.386,     0.8215,   2.203e-9,   3.081e-9,  -1.233e-9,     0.1009,      0.072,    0.01911,   -0.01406,  -2.083e-9, -7.041e-10, -1.186e-10, -3.768e-10,  2.868e-10],\n",
    "            [     0.8215,      0.292,   3.081e-9,  -1.233e-9, -3.724e-10,      0.072,    0.01911,   -0.01406,   0.002982, -7.041e-10, -1.186e-10, -3.768e-10,  2.868e-10, -1.317e-10],\n",
    "            [      0.292,   -0.06087,  -1.233e-9, -3.724e-10,  2.847e-11,    0.01911,   -0.01406,   0.002982,  0.0002575, -1.186e-10, -3.768e-10,  2.868e-10, -1.317e-10,  4.177e-11],\n",
    "            [   -0.06087,   0.003462, -3.724e-10,  2.847e-11, -1.962e-11,   -0.01406,   0.002982,  0.0002575, -0.0001809, -3.768e-10,  2.868e-10, -1.317e-10,  4.177e-11, -1.496e-10],\n",
    "            [   2.203e-9,   3.081e-9,     0.1009,      0.072,    0.01911,  -2.083e-9, -7.041e-10, -1.186e-10, -3.768e-10,    0.07419,    0.03174,   -0.01252,    0.00105,   0.000431],\n",
    "            [   3.081e-9,  -1.233e-9,      0.072,    0.01911,   -0.01406, -7.041e-10, -1.186e-10, -3.768e-10,  2.868e-10,    0.03174,   -0.01252,    0.00105,   0.000431,  0.0001552],\n",
    "            [  -1.233e-9, -3.724e-10,    0.01911,   -0.01406,   0.002982, -1.186e-10, -3.768e-10,  2.868e-10, -1.317e-10,   -0.01252,    0.00105,   0.000431,  0.0001552, -0.0003108],\n",
    "            [ -3.724e-10,  2.847e-11,   -0.01406,   0.002982,  0.0002575, -3.768e-10,  2.868e-10, -1.317e-10,  4.177e-11,    0.00105,   0.000431,  0.0001552, -0.0003108,   9.085e-5],\n",
    "            [  2.847e-11, -1.962e-11,   0.002982,  0.0002575, -0.0001809,  2.868e-10, -1.317e-10,  4.177e-11, -1.496e-10,   0.000431,  0.0001552, -0.0003108,   9.085e-5,  -1.182e-5]])\n",
    "    linear_form = np.matmul(Z, Q)\n",
    "    quadratic = np.sum(linear_form * Z, axis=1, keepdims=True)\n",
    "    return quadratic\n",
    "\n",
    "\n",
    "gamma = [0, 2.938, 11.99, 23.17, 65.57]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cmap(color='red', alpha=1.):\n",
    "    if color=='red':\n",
    "        color_code = (1., 0., 0., alpha)\n",
    "    elif color=='green':\n",
    "        color_code = (0., 1., 0., alpha)\n",
    "    elif color=='blue':\n",
    "        color_code = (0., 0., 1., alpha)\n",
    "    else:\n",
    "        color_code = color\n",
    "    transparent_code = (1., 1., 1., 0.)\n",
    "    return ListedColormap([transparent_code, color_code])\n",
    "\n",
    "HEAT_MAP = plt.get_cmap('inferno', lut=None)\n",
    "HEAT_MAP.set_over('white')\n",
    "HEAT_MAP.set_under('black')\n",
    "\n",
    "LEVEL_MAP = plt.get_cmap('viridis', lut=21)\n",
    "LEVEL_MAP.set_over('gold')\n",
    "LEVEL_MAP.set_under('white')\n",
    "\n",
    "\n",
    "def balanced_confusion_weights(y, y_true, scale_by_total=True):\n",
    "    y = y.astype(np.bool)\n",
    "    y_true = y_true.astype(np.bool)\n",
    "    \n",
    "    # Assuming labels in {0, 1}, count entries from confusion matrix\n",
    "    TP = ( y &  y_true).sum()\n",
    "    TN = (~y & ~y_true).sum()\n",
    "    FP = ( y & ~y_true).sum()\n",
    "    FN = (~y &  y_true).sum()\n",
    "    confusion_counts = np.array([[TN, FN], [FP, TP]])\n",
    "    \n",
    "    # Scale up each sample by inverse of confusion weight\n",
    "    weights = np.ones_like(y, dtype=float)\n",
    "    weights[ y &  y_true] /= TP\n",
    "    weights[~y & ~y_true] /= TN\n",
    "    weights[ y & ~y_true] /= FP\n",
    "    weights[~y &  y_true] /= FN\n",
    "    if scale_by_total:\n",
    "        weights *= y.size\n",
    "    \n",
    "    return weights, confusion_counts\n",
    "\n",
    "\n",
    "def balanced_class_weights(y_true, scale_by_total=True):\n",
    "    y = y_true.astype(np.bool)\n",
    "    nP = y.sum()\n",
    "    nN = y.size - y.sum()\n",
    "    class_counts = np.array([nN, nP])\n",
    "    \n",
    "    weights = np.ones_like(y, dtype=float)\n",
    "    weights[ y] /= nP\n",
    "    weights[~y] /= nN\n",
    "    if scale_by_total:\n",
    "        weights *= y.size\n",
    "    \n",
    "    return weights, class_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CPU_COUNT = os.cpu_count()\n",
    "NUM_CORES = 8\n",
    "NUM_SOCKETS = 2\n",
    "\n",
    "os.environ[\"KMP_BLOCKTIME\"]    = str(0)\n",
    "os.environ[\"KMP_SETTINGS\"]     = str(1)\n",
    "os.environ[\"KMP_AFFINITY\"]     = 'granularity=fine,noverbose,compact,1,0'\n",
    "os.environ[\"OMP_NUM_THREADS\"]  = str(NUM_CORES)\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads  = NUM_CORES,\n",
    "                        inter_op_parallelism_threads  = NUM_SOCKETS,\n",
    "                        allow_soft_placement          = False,\n",
    "#                         log_device_placement          = True,\n",
    "                        device_count                  = {'CPU': MAX_CPU_COUNT},\n",
    "                       )\n",
    "\n",
    "# TODO manually for CPU-only?\n",
    "config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n",
    "\n",
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "session = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt        = 0.01\n",
    "state_dim = 2\n",
    "x_max     = 2.5\n",
    "y_max     = 3\n",
    "\n",
    "state_norm   = (x_max, y_max)\n",
    "state_limits = np.array([[-1., 1.]] * state_dim)\n",
    "plot_limits  = np.array([[- x_max, x_max], [- y_max, y_max]])\n",
    "vanderpol    = VanDerPol(dt, state_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridify(norms, maxes=None, num_points=25):    \n",
    "    norms = np.asarray(norms).ravel()\n",
    "    if maxes is None:\n",
    "        maxes = norms\n",
    "    else:\n",
    "        maxes = np.asarray(maxes).ravel()\n",
    "    limits = np.column_stack((- maxes / norms, maxes / norms))\n",
    "    if isinstance(num_points, int):\n",
    "        num_points = [num_points, ] * len(norms)\n",
    "    grid = safe_learning.GridWorld(limits, num_points)\n",
    "    return grid\n",
    "\n",
    "\n",
    "grid = gridify(state_norm, num_points=201)\n",
    "if OPTIONS.use_zero_threshold:\n",
    "    tau = 0\n",
    "else:\n",
    "    tau = np.sum(grid.unit_maxes) / 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Lyapunov Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims             = [64, 64, 64]\n",
    "activations            = [tf.tanh, tf.tanh, tf.tanh]\n",
    "lyapunov_function      = LyapunovNetwork(state_dim, layer_dims, activations, OPTIONS.eps)\n",
    "grad_lyapunov_function = lambda X: tf.gradients(lyapunov_function(X), X)[0]\n",
    "\n",
    "L_f = lambda X: tf.maximum( np.abs(1 - 2 * dt), np.abs(1 + dt * (2 * tf.reduce_prod(X, axis=1, keepdims=True) + 1)) )\n",
    "L_v = lambda X: tf.norm(grad_lyapunov_function(X), ord=1, axis=1, keepdims=True)\n",
    "\n",
    "policy = lambda X: 0.0 * tf.reduce_sum(X, axis=1, keepdims=True)\n",
    "initial_safe_set = np.linalg.norm(grid.all_points, axis=1) <= 0.1\n",
    "\n",
    "# TODO need to use template before variables exist in the graph\n",
    "tf_states = tf.placeholder(OPTIONS.tf_dtype, shape=[None, state_dim], name='states')\n",
    "temp = lyapunov_function(tf_states)\n",
    "session.run(tf.variables_initializer(lyapunov_function.parameters))\n",
    "\n",
    "lyapunov = safe_learning.Lyapunov(grid, lyapunov_function, vanderpol, L_f, L_v, tau, policy, initial_safe_set)\n",
    "\n",
    "_STORAGE = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = safe_learning.utilities.get_storage(_STORAGE)\n",
    "if storage is None:\n",
    "    tf_states        = tf.placeholder(OPTIONS.tf_dtype, shape=[None, state_dim], name='states')\n",
    "    tf_actions       = policy(tf_states) # dummy variable\n",
    "    tf_future_states = vanderpol(tf_states, tf_actions)\n",
    "    tf_values        = lyapunov.lyapunov_function(tf_states)\n",
    "    tf_future_values = lyapunov.lyapunov_function(tf_future_states)\n",
    "    tf_dv            = tf_future_values - tf_values\n",
    "    tf_threshold     = lyapunov.threshold(tf_states, lyapunov.tau)\n",
    "    tf_negative      = tf.squeeze(tf.less(tf_dv, tf_threshold), axis=1)\n",
    "    \n",
    "    storage = [('states', tf_states), \n",
    "               ('future_states', tf_future_states), \n",
    "               ('values', tf_values),\n",
    "               ('future_values', tf_future_values),\n",
    "               ('dv', tf_dv),\n",
    "               ('threshold', tf_threshold), \n",
    "               ('negative', tf_negative)]\n",
    "    safe_learning.utilities.set_storage(_STORAGE, storage)\n",
    "else:\n",
    "    tf_states, tf_future_states, tf_values, tf_future_values, tf_dv, tf_threshold, tf_negative = storage.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True ROA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roa(grid, closed_loop_dynamics, horizon=250, tol=1e-3, cutoff=None):\n",
    "    if isinstance(grid, np.ndarray):\n",
    "        all_points = grid\n",
    "        nindex = grid.shape[0]\n",
    "        ndim = grid.shape[1]\n",
    "    else:\n",
    "        all_points = grid.all_points\n",
    "        nindex = grid.nindex\n",
    "        ndim = grid.ndim\n",
    "\n",
    "    # Forward-simulate all trajectories from initial points in the discretization\n",
    "    trajectories = np.empty((nindex, ndim, horizon))\n",
    "    trajectories[:, :, 0] = all_points\n",
    "    for t in range(1, horizon):\n",
    "        trajectories[:, :, t] = closed_loop_dynamics(trajectories[:, :, t - 1])        \n",
    "        if cutoff is not None:\n",
    "            np.clip(trajectories[:, :, t], - cutoff, cutoff, out=trajectories[:, :, t])\n",
    "    end_states = trajectories[:, :, -1]\n",
    "    \n",
    "    # Compute an approximate ROA as all states that end up \"close\" to 0\n",
    "    dists = np.linalg.norm(end_states, ord=2, axis=1, keepdims=True).ravel()\n",
    "    roa = (dists <= tol)\n",
    "    return roa, trajectories\n",
    "\n",
    "\n",
    "horizon  = 500\n",
    "tol      = 0.3\n",
    "cutoff   = 20\n",
    "dynamics = lambda x: tf_future_states.eval({tf_states: x})\n",
    "roa, trajectories = compute_roa(grid, dynamics, horizon, tol, cutoff)\n",
    "\n",
    "# Sub-sample discretization for faster and clearer plotting later\n",
    "N_traj = 11\n",
    "skip = int(grid.num_points[0] / N_traj)\n",
    "sub_idx = np.arange(grid.nindex).reshape(grid.num_points)\n",
    "sub_idx = sub_idx[::skip, ::skip].ravel()\n",
    "sub_trajectories = trajectories[sub_idx, :, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('supervised_lyapunov_learning'):\n",
    "#     Q            = np.array([[1, -10], [-10, 1]])\n",
    "#     P            = solve_discrete_lyapunov(vanderpol.linearize(), Q)\n",
    "    P            = np.array([[1, 0], [-0.5, 1]])\n",
    "    quad         = safe_learning.QuadraticFunction(P)\n",
    "    tf_target    = quad(tf_states)\n",
    "    tf_dv_lqr    = quad(tf_future_states) - tf_target\n",
    "    \n",
    "    tf_costs     = tf.abs(tf_values - tf_target) / tf.stop_gradient(tf_target + OPTIONS.eps)\n",
    "    tf_objective = tf.reduce_mean(tf_costs, name='objective')\n",
    "    \n",
    "    tf_learning_rate = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='learning_rate')\n",
    "    optimizer        = tf.train.GradientDescentOptimizer(tf_learning_rate)\n",
    "    training_update  = optimizer.minimize(tf_objective, var_list=lyapunov.lyapunov_function.parameters)\n",
    "    \n",
    "    tf_batch_size = tf.placeholder(tf.int32, [], 'batch_size')\n",
    "    tf_batch = tf.random_uniform([tf_batch_size, ], 0, lyapunov.initial_safe_set.sum(), dtype=tf.int32, name='batch_sample')\n",
    "\n",
    "session.run(tf.variables_initializer(lyapunov_function.parameters))\n",
    "lyapunov.update_values()\n",
    "lyapunov.update_safe_set()\n",
    "\n",
    "obj = []\n",
    "level_states = grid.all_points[lyapunov.initial_safe_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "test_size = int(1e3)\n",
    "idx = tf_batch.eval({tf_batch_size: int(1e3)})\n",
    "test_set = level_states[idx, :]\n",
    "    \n",
    "feed_dict = {\n",
    "    tf_states:         level_states,\n",
    "    tf_learning_rate:  1e-3,\n",
    "    tf_batch_size:     int(1e3),\n",
    "}\n",
    "max_iters = 100\n",
    "\n",
    "if OPTIONS.pre_train:\n",
    "    for i in tqdm(range(max_iters)):\n",
    "        idx = tf_batch.eval(feed_dict)\n",
    "        feed_dict[tf_states] = level_states[idx, :]\n",
    "        session.run(training_update, feed_dict)\n",
    "\n",
    "        feed_dict[tf_states] = test_set\n",
    "        obj.append(tf_objective.eval(feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=OPTIONS.dpi)\n",
    "ax.set_xlabel(r'iteration')\n",
    "ax.set_ylabel(r'objective')\n",
    "ax.plot(obj, '.-r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, values_lqr, dv, dv_lqr = session.run([tf_values, tf_target, tf_dv, tf_dv_lqr], {tf_states: grid.all_points})\n",
    "value_norm = np.amax([values.max(), values_lqr.max()])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(7, 6), dpi=OPTIONS.dpi)\n",
    "fig.subplots_adjust(wspace=0.3, hspace=0.2)\n",
    "for ax in axes.ravel():\n",
    "    ax.set_xlabel(r'$x$')\n",
    "    ax.set_ylabel(r'$y$')\n",
    "\n",
    "z = values_lqr.reshape(grid.num_points) / values_lqr.max()\n",
    "ax = axes[0, 0]\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=x_max / y_max, cmap=LEVEL_MAP, vmin=0, vmax=1)  \n",
    "ax.set_title('LQR Lyapunov function')\n",
    "cbar = fig.colorbar(im, ax=ax, label=r'$v(x)$')\n",
    "\n",
    "z = values.reshape(grid.num_points) / values.max()\n",
    "ax = axes[1, 0]\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=x_max / y_max, cmap=LEVEL_MAP, vmin=0, vmax=1)   \n",
    "ax.set_title('NN Lyapunov function')\n",
    "cbar = fig.colorbar(im, ax=ax, label=r'$v(x)$')\n",
    "\n",
    "z = dv_lqr.reshape(grid.num_points)\n",
    "ax = axes[0, 1]\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=x_max / y_max, cmap=HEAT_MAP, vmax=0.0)   \n",
    "ax.set_title('LQR Lyapunov function')\n",
    "cbar = fig.colorbar(im, ax=ax, label=r'$v(f(x)) - v(x)$')\n",
    "\n",
    "z = dv.reshape(grid.num_points)\n",
    "ax = axes[1, 1]\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=x_max / y_max, cmap=HEAT_MAP, vmax=0.0)   \n",
    "ax.set_title('NN Lyapunov function')\n",
    "cbar = fig.colorbar(im, ax=ax, label=r'$v(f(x)) - v(x)$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint for neural net weights\n",
    "saver = tf.train.Saver(var_list=lyapunov.lyapunov_function.parameters)\n",
    "ckpt_path = saver.save(session, \"/tmp/spencerr_sos_lyapunov.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('roa_classification'):\n",
    "    # Current maximum level set we want to push the ROA in to\n",
    "    tf_c_max            = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='c_max')\n",
    "    tf_level_multiplier = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='level_multiplier')\n",
    "    \n",
    "    # True class labels, converted from ROA booleans {0, 1} to data labels {-1, 1}\n",
    "    tf_roa     = tf.placeholder(OPTIONS.tf_dtype, shape=[None, 1], name='roa')\n",
    "    tf_labels  = 2 * tf_roa - 1\n",
    "\n",
    "    # Classifier output (signed distance to decision boundary c_max = c)\n",
    "    tf_decision_dist = tf_c_max - tf_values\n",
    "    tf_y_est = 0.5 * (tf.sign(tf_decision_dist) + 1)\n",
    "    \n",
    "    \n",
    "    # Use perceptron / hinge / logistic loss with class weights\n",
    "    tf_weights         = tf.placeholder(OPTIONS.tf_dtype, shape=[None, 1], name='class_weights')\n",
    "    tf_classifier_loss = tf_weights * tf.maximum(- tf_labels * tf_decision_dist, 0, name='perceptron_loss')\n",
    "#     tf_classifier_loss = tf_weights * tf.maximum(1 - tf_labels * tf_decision_dist, 0, name='hinge_loss')\n",
    "#     tf_classifier_loss = tf_weights * tf.log(1 + tf.exp(- tf_labels * tf_decision_dist), name='logistic_loss')\n",
    "        \n",
    "    # Enforce decrease constraint with Lagrangian relaxation\n",
    "    tf_lagrange_multiplier = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='lagrange_multiplier')\n",
    "    tf_decrease_loss       = tf_roa * tf.maximum(tf_dv - tf_threshold, 0) / tf.stop_gradient(tf_values + OPTIONS.eps)\n",
    "#     tf_decrease_loss       = tf_y_est * tf.maximum(tf_dv - tf_threshold, 0) / tf.stop_gradient(tf_values + OPTIONS.eps)\n",
    "    \n",
    "    # Define update step\n",
    "    tf_objective     = tf.reduce_mean(tf_classifier_loss + tf_lagrange_multiplier * tf_decrease_loss, name='objective')\n",
    "    tf_learning_rate = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='learning_rate')\n",
    "    optimizer        = tf.train.GradientDescentOptimizer(tf_learning_rate)\n",
    "    training_update  = optimizer.minimize(tf_objective, var_list=lyapunov.lyapunov_function.parameters)\n",
    "    \n",
    "    # TODO\n",
    "#     tf_dec_obj  = tf.reduce_mean(tf_decrease_loss, name='decrease_objective')\n",
    "#     tf_dec_rate = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='learning_rate')\n",
    "#     optimizer   = tf.train.GradientDescentOptimizer(tf_dec_rate)\n",
    "#     dec_update  = optimizer.minimize(tf_dec_obj, var_list=lyapunov.lyapunov_function.parameters)\n",
    "    \n",
    "\n",
    "with tf.name_scope('sampling'):\n",
    "    tf_batch_size = tf.placeholder(tf.int32, [], 'batch_size')\n",
    "    tf_idx_range  = tf.placeholder(tf.int32, shape=[], name='indices_to_sample')\n",
    "    tf_idx_batch  = tf.random_uniform([tf_batch_size, ], 0, tf_idx_range, dtype=tf.int32, name='batch_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore checkpoint\n",
    "saver.restore(session, ckpt_path)\n",
    "lyapunov.update_values()\n",
    "lyapunov.update_safe_set()\n",
    "# session.run(tf.variables_initializer(optimizer.variables()))\n",
    "\n",
    "obj          = []\n",
    "loss_class   = []\n",
    "loss_dec     = []\n",
    "roa_estimate = np.copy(lyapunov.safe_set)\n",
    "idx_visited  = np.zeros_like(lyapunov.safe_set)\n",
    "\n",
    "c_max = [lyapunov.feed_dict[lyapunov.c_max], ]\n",
    "safe_size = [lyapunov.safe_set.sum() / lyapunov.discretization.nindex, ]\n",
    "grid = lyapunov.discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_iters = 10\n",
    "inner_iters = 30\n",
    "\n",
    "# dec_iters = 100\n",
    "horizon   = 250\n",
    "\n",
    "feed_dict = {\n",
    "    tf_states:               np.zeros((1, lyapunov.discretization.ndim)), # placeholder\n",
    "    tf_batch_size:           int(1e3),\n",
    "    tf_c_max:                1,\n",
    "    tf_lagrange_multiplier:  350,\n",
    "    tf_idx_range:            grid.nindex,\n",
    "    #\n",
    "    tf_learning_rate:        3e-4,\n",
    "    tf_level_multiplier:     3.,\n",
    "    #\n",
    "#     tf_dec_rate:             1e-3,\n",
    "}\n",
    "\n",
    "test_set = grid.all_points\n",
    "test_labels = roa.reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Current metrics ...')\n",
    "c = lyapunov.feed_dict[lyapunov.c_max]\n",
    "num_safe = lyapunov.safe_set.sum()\n",
    "print('c_max: {}'.format(c))\n",
    "print('grid size: {}'.format(grid.nindex))\n",
    "print('safe set size: {} ({:.2f}% of grid, {:.2f}% of ROA)\\n'.format(int(num_safe), 100 * num_safe / grid.nindex, 100 * num_safe / roa.sum()))\n",
    "print('')\n",
    "time.sleep(0.5)\n",
    "\n",
    "for i in range(outer_iters):\n",
    "    # Identify current safe set and gap states around it\n",
    "    c         = lyapunov.feed_dict[lyapunov.c_max]\n",
    "    idx_small = lyapunov.values.ravel() <= c\n",
    "    idx_big   = lyapunov.values.ravel() <= feed_dict[tf_level_multiplier] * c\n",
    "    idx_gap   = np.logical_and(idx_big, ~idx_small)\n",
    "    \n",
    "    # Update ROA estimate by propagating gap states forward\n",
    "    propagated_states = grid.all_points[idx_gap]\n",
    "    for _ in range(horizon):\n",
    "        propagated_states = tf_future_states.eval({tf_states: propagated_states})\n",
    "        np.clip(propagated_states, -1, 1, out=propagated_states)\n",
    "    safe_in_future = (tf_values.eval({tf_states: propagated_states}) <= c).ravel()\n",
    "    roa_estimate[idx_gap] |= safe_in_future\n",
    "    \n",
    "    # Train classifier on current ROA estimate and any states from scaled level set\n",
    "    target_idx              = np.logical_or(idx_big, roa_estimate)\n",
    "    target_set              = grid.all_points[target_idx]\n",
    "    target_labels           = roa_estimate[target_idx].astype(OPTIONS.np_dtype).reshape([-1, 1])\n",
    "    feed_dict[tf_idx_range] = target_set.shape[0]\n",
    "    idx_visited |= target_idx\n",
    "    \n",
    "    # Test set\n",
    "#     test_set = target_set\n",
    "#     test_labels = target_labels\n",
    "\n",
    "    # SGD for classification\n",
    "    for _ in tqdm(range(inner_iters)):\n",
    "        # Training step\n",
    "        idx_batch                     = tf_idx_batch.eval(feed_dict)\n",
    "        feed_dict[tf_states]          = target_set[idx_batch]\n",
    "        feed_dict[tf_roa]             = target_labels[idx_batch]\n",
    "#         feed_dict[tf_weights], counts = balanced_confusion_weights(tf_values.eval(feed_dict) <= feed_dict[tf_c_max], feed_dict[tf_roa].astype(bool))\n",
    "        feed_dict[tf_weights], counts = balanced_class_weights(feed_dict[tf_roa].astype(bool))\n",
    "        session.run(training_update, feed_dict=feed_dict)\n",
    "\n",
    "        # Record objectives\n",
    "        feed_dict[tf_states]          = test_set\n",
    "        feed_dict[tf_roa]             = test_labels\n",
    "#         feed_dict[tf_weights], counts = balanced_confusion_weights(tf_values.eval(feed_dict) <= feed_dict[tf_c_max], feed_dict[tf_roa].astype(bool))\n",
    "        feed_dict[tf_weights], counts = balanced_class_weights(feed_dict[tf_roa].astype(bool))\n",
    "\n",
    "        results = session.run([tf_classifier_loss, tf_decrease_loss], feed_dict)\n",
    "        loss_class.append(results[0].mean())\n",
    "        loss_dec.append(results[1].mean())\n",
    "        obj.append(loss_class[-1] + feed_dict[tf_lagrange_multiplier] * loss_dec[-1])\n",
    "    \n",
    "\n",
    "    # TODO\n",
    "#     lyapunov.update_values()\n",
    "#     lyapunov.update_safe_set()\n",
    "#     for _ in tqdm(range(dec_iters)):\n",
    "#         idx_edge                      = lyapunov.values.ravel() >= lyapunov.feed_dict[lyapunov.c_max]\n",
    "#         target_idx                    = np.logical_and(idx_edge, roa_estimate)\n",
    "#         target_set                    = grid.all_points[target_idx]\n",
    "#         target_labels                 = roa_estimate[target_idx].astype(OPTIONS.np_dtype).reshape([-1, 1])\n",
    "#         feed_dict[tf_idx_range]       = target_set.shape[0]\n",
    "#         idx_batch                     = tf_idx_batch.eval(feed_dict)\n",
    "#         feed_dict[tf_states]          = target_set[idx_batch]\n",
    "#         feed_dict[tf_roa]             = target_labels[idx_batch]\n",
    "#         feed_dict[tf_weights], counts = balanced_class_weights(feed_dict[tf_roa].astype(bool))\n",
    "#         session.run(dec_update, feed_dict=feed_dict)\n",
    "    \n",
    "\n",
    "    lyapunov.update_values()\n",
    "    lyapunov.update_safe_set()\n",
    "    roa_estimate |= lyapunov.safe_set\n",
    "    safe_size.append(lyapunov.safe_set.sum() / grid.nindex)\n",
    "    c_max.append(lyapunov.feed_dict[lyapunov.c_max])\n",
    "    \n",
    "    print(counts)\n",
    "    print('c_max: {}'.format(c_max[-1]))\n",
    "    print('safe set size: {} ({:.2f}% of grid, {:.2f}% of ROA)\\n'.format(int(safe_size[-1] * grid.nindex), \n",
    "                                                                         100 * safe_size[-1], \n",
    "                                                                         100 * safe_size[-1] * roa.size / roa.sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3), dpi=OPTIONS.dpi)\n",
    "\n",
    "ax.plot(loss_class, '.-r')\n",
    "ax.plot(feed_dict[tf_lagrange_multiplier] * np.asarray(loss_dec), '.-b')\n",
    "\n",
    "ax.set_xlabel(r'SGD iteration (accumulated)')\n",
    "ax.set_ylabel(r'Training loss')\n",
    "# ax.set_xticks(list(range(0, len(loss_class) + 1, inner_iters)))\n",
    "\n",
    "proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in ['red', 'blue']]    \n",
    "legend = ax.legend(proxy, ['Classification loss', 'Lyapunov decrease loss'], loc='upper right', fontsize=8)\n",
    "legend.get_frame().set_alpha(0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3), dpi=OPTIONS.dpi)\n",
    "roa_fraction = roa.sum() / roa.size\n",
    "\n",
    "ax.plot(c_max, '.-r')\n",
    "ax.set_ylabel(r'$c_k$')\n",
    "ax.tick_params('y', colors='r')\n",
    "# ax.set_ylim([0, 1])\n",
    "\n",
    "ax.set_xlabel(r'Safe set update iteration $k$')\n",
    "# ax.set_xticks(list(range(0, len(c_max) + 1, 1)))\n",
    "\n",
    "ax = ax.twinx()\n",
    "ax.plot(np.array(safe_size) / roa_fraction, '.-b')\n",
    "ax.set_ylabel(r'$|\\mathcal{V}(c_k) \\cap \\mathcal{X}_\\tau|\\ /\\ |\\mathcal{R} \\cap \\mathcal{X}_\\tau|$')\n",
    "ax.tick_params('y', colors='b')\n",
    "# ax.set_ylim([0, 1])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(np.array(safe_size) / roa_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4), dpi=OPTIONS.dpi)\n",
    "ax.set_aspect(x_max / y_max)\n",
    "ax.set_xlim([- x_max, x_max])\n",
    "ax.set_ylim([- y_max, y_max])\n",
    "    \n",
    "# True ROA\n",
    "z = roa.reshape(grid.num_points)\n",
    "# im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=x_max / y_max, cmap=binary_cmap('blue', 0.5), vmin=0)\n",
    "# im = ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), colors='darkgreen', linestyles='dashed')\n",
    "\n",
    "# Safe set\n",
    "z = lyapunov.safe_set.reshape(grid.num_points)\n",
    "# im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=x_max / y_max, cmap=binary_cmap('blue', 1), vmin=0)\n",
    "im = ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), cmap=ListedColormap(['blue']))\n",
    "\n",
    "# Decrease region\n",
    "z = tf_dv.eval({tf_states: grid.all_points}).reshape(grid.num_points) < 0\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=x_max / y_max, cmap=binary_cmap('blue', 0.3), vmin=0)\n",
    "# im = ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), cmap=ListedColormap(['blue']), levels=0, linestyles='dashed')\n",
    "\n",
    "# Estimated ROA\n",
    "alpha = 0.5\n",
    "cmap = ListedColormap([(1., 1., 1., 0.), (1., 0., 0., alpha), (0., 1., 0., alpha)])\n",
    "# z = tf_negative.eval({tf_states: grid.all_points}).reshape(grid.num_points)\n",
    "z = roa_estimate.astype(int)\n",
    "z[idx_visited] += 1\n",
    "z = z.reshape(grid.num_points)\n",
    "# im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=x_max / y_max, cmap=cmap, vmin=0)\n",
    "im = ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), cmap=ListedColormap(['blue', 'red', 'green']))\n",
    "\n",
    "# Neural-network level sets\n",
    "z = tf_values.eval({tf_states: grid.all_points}).reshape(grid.num_points)\n",
    "im = ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), colors='darkviolet', levels=1, linestyles='dashed')\n",
    "\n",
    "# SOS Lyapunov function\n",
    "deg = 4\n",
    "temp = vanderpol.denormalize(grid.all_points).eval()\n",
    "z = sos_lyapunov(temp, deg).reshape(grid.num_points)\n",
    "# im = ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), cmap=plt.get_cmap('plasma'), levels=np.linspace(0, z.max(), 41))\n",
    "# im = ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), colors='blue', levels=gamma[deg])\n",
    "\n",
    "# # Trajectories\n",
    "for n in range(sub_trajectories.shape[0]):\n",
    "    x = sub_trajectories[n, 0, :] * state_norm[0]\n",
    "    y = sub_trajectories[n, 1, :] * state_norm[1]\n",
    "    ax.plot(x, y, 'k--', linewidth=0.25)\n",
    "sub_states = grid.all_points[sub_idx]\n",
    "dx_dt = (dynamics(sub_states) - sub_states) / dt\n",
    "dx_dt = dx_dt / np.linalg.norm(dx_dt, ord=2, axis=1, keepdims=True)\n",
    "ax.quiver(sub_states[:, 0] * state_norm[0], sub_states[:, 1] * state_norm[1], dx_dt[:, 0], dx_dt[:, 1], \n",
    "          scale=None, pivot='mid', headwidth=3, headlength=6, color='k')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
