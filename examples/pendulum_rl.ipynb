{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for an Inverted Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "\n",
    "from scipy.linalg import block_diag\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import safe_learning\n",
    "import plotting\n",
    "from utilities import InvertedPendulum, compute_closedloop_response, get_parameter_change\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x\n",
    "\n",
    "# TODO testing ****************************************#\n",
    "\n",
    "class Options(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Options, self).__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "OPTIONS = Options(np_dtype              = safe_learning.config.np_dtype,\n",
    "                  tf_dtype              = safe_learning.config.dtype,\n",
    "                  use_linear_dynamics   = False,\n",
    "                  saturate              = True,\n",
    "                  eps                   = 1e-8,\n",
    "                  dpi                   = 200,\n",
    "                  fontproperties        = FontProperties(size=10),\n",
    "                  save_figs             = False,\n",
    "                  fig_path              = 'figures/pendulum_rl/')\n",
    "\n",
    "HEAT_MAP = plt.get_cmap('inferno', lut=None)\n",
    "HEAT_MAP.set_over('white')\n",
    "HEAT_MAP.set_under('black')\n",
    "\n",
    "LEVEL_MAP = plt.get_cmap('viridis', lut=21)\n",
    "LEVEL_MAP.set_over('gold')\n",
    "LEVEL_MAP.set_under('white')\n",
    "\n",
    "BINARY_MAP = ListedColormap([(1., 1., 1., 0.), (0., 1., 0., 0.65)])\n",
    "\n",
    "#******************************************************#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CPU_COUNT = os.cpu_count()\n",
    "NUM_CORES = 8\n",
    "NUM_SOCKETS = 2\n",
    "\n",
    "os.environ[\"KMP_BLOCKTIME\"]    = str(0)\n",
    "os.environ[\"KMP_SETTINGS\"]     = str(1)\n",
    "os.environ[\"KMP_AFFINITY\"]     = 'granularity=fine,noverbose,compact,1,0'\n",
    "os.environ[\"OMP_NUM_THREADS\"]  = str(NUM_CORES)\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads  = NUM_CORES,\n",
    "                        inter_op_parallelism_threads  = NUM_SOCKETS,\n",
    "                        allow_soft_placement          = False,\n",
    "#                         log_device_placement          = True,\n",
    "                        device_count                  = {'CPU': MAX_CPU_COUNT})\n",
    "\n",
    "# TODO manually for CPU-only?\n",
    "config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n",
    "\n",
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "session = tf.InteractiveSession(config=config)\n",
    "\n",
    "# print('Found MAX_CPU_COUNT =', MAX_CPU_COUNT)\n",
    "# for dev in session.list_devices():\n",
    "#     print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridify(norms, maxes=None, num_points=25):    \n",
    "    norms = np.asarray(norms).ravel()\n",
    "    if maxes is None:\n",
    "        maxes = norms\n",
    "    else:\n",
    "        maxes = np.asarray(maxes).ravel()\n",
    "    limits = np.column_stack((- maxes / norms, maxes / norms))\n",
    "    \n",
    "    if isinstance(num_points, int):\n",
    "        num_points = [num_points, ] * len(norms)\n",
    "    grid = safe_learning.GridWorld(limits, num_points)\n",
    "    return grid\n",
    "\n",
    "\n",
    "def compute_roa(grid, closed_loop_dynamics, horizon=250, tol=1e-3, equilibrium=None):\n",
    "    # Forward-simulate all trajectories from initial points in the discretization\n",
    "    trajectories = np.empty((grid.nindex, grid.ndim, horizon))\n",
    "    for t in range(horizon):\n",
    "        if t == 0:\n",
    "            trajectories[:, :, t] = grid.all_points\n",
    "        else:\n",
    "            trajectories[:, :, t] = closed_loop_dynamics(trajectories[:, :, t - 1])\n",
    "            \n",
    "    if equilibrium is None:\n",
    "        equilibrium = np.zeros((1, grid.ndim))\n",
    "    \n",
    "    # Compute an approximate ROA as all states that end up \"close\" to 0\n",
    "    dist = np.linalg.norm(trajectories[:, :, -1] - equilibrium, ord=2, axis=1, keepdims=True)\n",
    "    roa = (dist <= tol)\n",
    "    return trajectories, roa.reshape(grid.num_points), dist.reshape(grid.num_points)\n",
    "\n",
    "\n",
    "def estimate_cost(grid, closed_loop_dynamics, cost_function, discount, horizon=250, tol=1e-3):\n",
    "    # Estimate true cost function using a finite-horizon rollout\n",
    "    converged = False\n",
    "    rollout = np.zeros(grid.nindex)\n",
    "    current_states = grid.all_points\n",
    "    for t in range(horizon):\n",
    "        temp = (discount ** t) * cost_function(current_states).ravel()\n",
    "        rollout += temp\n",
    "        if np.max(np.abs(temp)) < tol:\n",
    "            converged = True\n",
    "            break\n",
    "        current_states = closed_loop_dynamics(current_states)\n",
    "    if converged:\n",
    "        print('Cost converged after {} steps!'.format(t + 1))\n",
    "    else:\n",
    "        print('Cost did not converge!')\n",
    "    return rollout.reshape(grid.num_points)\n",
    "\n",
    "\n",
    "def plot_function_3D(tf_function_A, tf_function_B, grid, norms, colors=['r','b'], title=None, show=True, view=(None, None)):\n",
    "    scaled_discrete_points = [np.rad2deg(norm) * points for norm, points in zip(norms, grid.discrete_points)]\n",
    "    xx, yy = np.meshgrid(*scaled_discrete_points)\n",
    "    if isinstance(tf_function_A, np.ndarray):\n",
    "        f_vals_A = tf_function_A.reshape(grid.num_points)\n",
    "    else:\n",
    "        f_vals_A = - tf_function_A(grid.all_points).eval().reshape(grid.num_points)\n",
    "        \n",
    "    if isinstance(tf_function_B, np.ndarray):\n",
    "        f_vals_B = tf_function_B.reshape(grid.num_points)\n",
    "    else:\n",
    "        f_vals_B = - tf_function_B(grid.all_points).eval().reshape(grid.num_points)\n",
    "     \n",
    "    fig = plt.figure(figsize=(6, 5), dpi=200)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    surf_B = ax.plot_surface(xx, yy, f_vals_B, color=colors[1], alpha=0.65) #, label=r'$J_{LQR}({\\bf x})$')\n",
    "    surf_A = ax.plot_surface(xx, yy, f_vals_A, color=colors[0], alpha=0.65) #, label=r'$J_{\\bf \\theta}({\\bf x})$')\n",
    "    \n",
    "    for surf in (surf_A, surf_B):\n",
    "        surf._facecolors2d = surf._facecolors3d\n",
    "        surf._edgecolors2d = surf._edgecolors3d\n",
    "        \n",
    "    if title is not None:\n",
    "        ax.set_title(title, fontproperties=OPTIONS.fontproperties)\n",
    "    ax.set_xlabel(r'$\\phi$ [deg]', labelpad=10, fontproperties=OPTIONS.fontproperties)\n",
    "    ax.set_ylabel(r'$\\dot{\\phi}$ [deg/s]', labelpad=10, fontproperties=OPTIONS.fontproperties)\n",
    "#     ax.set_zlabel(r'$J({\\bf x})$', fontproperties=OPTIONS.fontproperties)\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels() + ax.get_zticklabels()):\n",
    "        label.set_fontproperties(OPTIONS.fontproperties)\n",
    "        \n",
    "    proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in [(0., 0., 1., 0.65), (1., 0., 0., 0.65)]]    \n",
    "    ax.legend(proxy, [r'$J_{\\pi}({\\bf x})$', r'$J_{\\bf \\theta}({\\bf x})$'], prop=OPTIONS.fontproperties)\n",
    "    \n",
    "#     ax.contourf(xx, yy, roa, cmap=BINARY_MAP, zdir='z', offset=0)\n",
    "#     proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in [(0., 0., 1., 0.65), (1., 0., 0., 0.65), (0., 1., 0., 0.65)]]    \n",
    "#     ax.legend(proxy, [r'$J_{\\pi}({\\bf x})$', r'$J_{\\bf \\theta}({\\bf x})$', 'ROA'], prop=OPTIONS.fontproperties)\n",
    "\n",
    "    ax.view_init(view[0], view[1])\n",
    "    fig.tight_layout()\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_closedloop_response(dynamics, policy1, policy2, steps, dt, reference='zero', const=1.0, ic=None,\n",
    "                             labels=['Policy 1','Policy 2'], colors=['r','b'], show=True):\n",
    "    state_dim = 2\n",
    "    state_traj1, action_traj1, t, _ = compute_closedloop_response(dynamics, policy1, state_dim, steps, dt, reference, const, ic)\n",
    "    state_traj2, action_traj2, _, _ = compute_closedloop_response(dynamics, policy2, state_dim, steps, dt, reference, const, ic)\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 3), dpi=100)\n",
    "    fig.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "    \n",
    "    if reference=='zero':\n",
    "        title_string = r'Zero-Input Response'\n",
    "    elif reference=='impulse':\n",
    "        title_string = r'Impulse Response'\n",
    "    elif reference=='step':\n",
    "        title_string = r'Step Response, $r = %.1g$' % const\n",
    "    \n",
    "    if ic is not None:\n",
    "        ic_tuple = (ic[0], ic[1])\n",
    "    else:\n",
    "        ic_tuple = (0, 0)\n",
    "    title_string = title_string + r', $s_0 = (%.1g, %.1g)$' % ic_tuple\n",
    "    fig.suptitle(title_string, fontsize=18)         \n",
    "    \n",
    "    state_names = [r'$\\theta$', r'$\\omega$']\n",
    "    plot_idx = (1, 2)\n",
    "    for i in range(2):\n",
    "        ax = fig.add_subplot(1, 3, plot_idx[i])\n",
    "        ax.plot(t, state_traj1[:, i], colors[0])\n",
    "        ax.plot(t, state_traj2[:, i], colors[1])\n",
    "        ax.set_xlabel(r'$t$', fontsize=14)\n",
    "        ax.set_ylabel(state_names[i], fontsize=14, rotation=0)\n",
    "    ax = fig.add_subplot(1, 3, 3)\n",
    "    plot1 = ax.plot(t, action_traj1, color=colors[0])\n",
    "    plot2 = ax.plot(t, action_traj2, color=colors[1])\n",
    "    ax.set_xlabel(r'$t$', fontsize=14)\n",
    "    ax.set_ylabel(r'$u$', fontsize=14, rotation=0)\n",
    "    fig.legend((plot1[0], plot2[0]), (labels[0], labels[1]), loc=(0.9, 0.2), fontsize=14)\n",
    "\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dt = 0.01   # sampling time\n",
    "g = 9.81    # gravity\n",
    "\n",
    "# System parameters\n",
    "m = 0.15    # pendulum mass [kg], 0.5\n",
    "L = 0.5     # pole length [m], 1.\n",
    "b = 0.1     # rotational friction [kg * (m ** 2) / s], 0.\n",
    "\n",
    "# State and action normalizers\n",
    "theta_max = np.deg2rad(30)\n",
    "omega_max = np.sqrt(g / L)\n",
    "u_max = m * g * L * np.sin(theta_max)\n",
    "\n",
    "state_norm = (theta_max, omega_max)\n",
    "action_norm = (u_max, )\n",
    "\n",
    "# Dimensions and domains\n",
    "state_dim = 2\n",
    "action_dim = 1\n",
    "# state_limits = np.array([[-1., 1.]] * state_dim)\n",
    "# action_limits = np.array([[-1., 1.]] * action_dim)\n",
    "\n",
    "# Define dynamic system\n",
    "pendulum = InvertedPendulum(m, L, b, dt, [state_norm, action_norm])\n",
    "A, B = pendulum.linearize()\n",
    "\n",
    "if OPTIONS.use_linear_dynamics:\n",
    "    dynamics = safe_learning.functions.LinearSystem((A, B), name='dynamics')\n",
    "else:\n",
    "    dynamics = pendulum.__call__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State cost matrix\n",
    "Q = np.diag([0.1, 0.1])\n",
    "\n",
    "# Action cost matrix\n",
    "R = np.diag([0.1])\n",
    "\n",
    "# Normalize cost matrices\n",
    "# max_state = np.ones((1, state_dim))\n",
    "# max_action = np.ones((1, action_dim))\n",
    "# cost_norm = np.linalg.multi_dot((max_state, Q, max_state.T)) + np.linalg.multi_dot((max_action, R, max_action.T))\n",
    "# # cost_norm = np.max([Q.max(), R.max()])\n",
    "# Q = Q / cost_norm\n",
    "# R = R / cost_norm\n",
    "\n",
    "# Quadratic reward (-cost) function\n",
    "reward_function = safe_learning.QuadraticFunction(block_diag(- Q, - R), name='reward_function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric Policy and Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy\n",
    "layer_dims = [64, 64, action_dim]\n",
    "activations = [tf.nn.relu, tf.nn.relu, None]\n",
    "if OPTIONS.saturate:\n",
    "    activations[-1] = tf.nn.tanh\n",
    "policy = safe_learning.functions.NeuralNetwork(layer_dims, activations, name='policy', use_bias=False)\n",
    "\n",
    "# Value function\n",
    "layer_dims = [64, 64, 1]\n",
    "activations = [tf.nn.relu, tf.nn.relu, None]\n",
    "value_function = safe_learning.functions.NeuralNetwork(layer_dims, activations, name='value_function', use_bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Approximate Policy Evaluation\n",
    "\n",
    "Fix the policy, and learn the corresponding value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Policy\n",
    "\n",
    "Set the policy to the LQR solution, possibly with saturation constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K, P = safe_learning.utilities.dlqr(A, B, Q, R)\n",
    "policy_lqr = safe_learning.functions.LinearSystem((-K, ), name='policy_lqr')\n",
    "if OPTIONS.saturate:\n",
    "    policy_lqr = safe_learning.Saturation(policy_lqr, -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Graph and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear graph\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "# print(session is tf.get_default_session())\n",
    "# print(session.graph is tf.get_default_graph())\n",
    "\n",
    "# print(len([n.name for n in session.graph.as_graph_def().node]))\n",
    "# print(len([n.name for n in tf.get_default_graph().as_graph_def().node]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new graph\n",
    "states = tf.placeholder(OPTIONS.tf_dtype, shape=[None, state_dim], name='states')\n",
    "actions = policy_lqr(states)\n",
    "rewards = reward_function(states, actions)\n",
    "future_states = dynamics(states, actions)\n",
    "\n",
    "# Use the parametric value function\n",
    "values = value_function(states)\n",
    "future_values = value_function(future_states)\n",
    "\n",
    "with tf.name_scope('approximate_policy_evaluation'):\n",
    "    # Discount factor and scaling\n",
    "    gamma = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='discount_factor')\n",
    "    max_state = np.ones((1, state_dim))\n",
    "    max_action = np.ones((1, action_dim))\n",
    "    r_max = np.linalg.multi_dot((max_state, Q, max_state.T)) + np.linalg.multi_dot((max_action, R, max_action.T))\n",
    "#     scaling = (1 - gamma) / r_max.ravel()\n",
    "    scaling = 1 / r_max.ravel()\n",
    "    \n",
    "    # Objective function\n",
    "    target = tf.stop_gradient(rewards + gamma * future_values, name='target')\n",
    "    objective = scaling * tf.reduce_mean(tf.abs(values - target), name='objective')\n",
    "    \n",
    "    # Optimizer settings\n",
    "    learning_rate = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='learning_rate')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#     training_update = optimizer.minimize(objective, var_list=value_function.parameters)\n",
    "    grads_and_vars = optimizer.compute_gradients(objective, value_function.parameters)\n",
    "    training_update = optimizer.apply_gradients(grads_and_vars)\n",
    "    \n",
    "with tf.name_scope('state_sampler'):\n",
    "    batch_size = tf.placeholder(tf.int32, shape=[], name='batch_size')\n",
    "    batch = tf.random_uniform([batch_size, state_dim], -1, 1, dtype=OPTIONS.tf_dtype, name='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "session.run(tf.variables_initializer(value_function.parameters))\n",
    "try:\n",
    "    del obj\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed dict with hyperparameters\n",
    "feed_dict = {\n",
    "    states:         np.zeros((1, state_dim)), # placeholder\n",
    "    gamma:          0.95,\n",
    "    learning_rate:  0.005,\n",
    "    batch_size:     100,   \n",
    "}\n",
    "max_iters = 500\n",
    "test_size = 1e3\n",
    "\n",
    "# Uniformly-distributed test set\n",
    "grid_length = np.power(test_size, 1 / state_dim)        # test_size = N^d, solve for N\n",
    "grid_length = int(2 * np.floor(grid_length / 2) + 1)    # round N to the nearest odd integer to include 0 in grid\n",
    "state_limits = np.array([[-1., 1.]] * state_dim)        # states are normalized to [-1, 1]^d\n",
    "num_points = [grid_length, ] * state_dim\n",
    "test_set = safe_learning.GridWorld(state_limits, num_points).all_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of objective value and parameter changes during training\n",
    "if 'obj' not in locals():\n",
    "    obj = []\n",
    "    param_changes = []\n",
    "    \n",
    "old_params = session.run(value_function.parameters)\n",
    "\n",
    "for i in tqdm(range(max_iters)):\n",
    "    feed_dict[states] = batch.eval(feed_dict)\n",
    "    session.run(training_update, feed_dict)\n",
    "    \n",
    "    new_params = session.run(value_function.parameters)\n",
    "    param_changes.append(get_parameter_change(old_params, new_params, 'inf'))\n",
    "    old_params = list(new_params)\n",
    "    \n",
    "    feed_dict[states] = test_set\n",
    "    obj.append(objective.eval(feed_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5), dpi=OPTIONS.dpi)\n",
    "fig.subplots_adjust(wspace=0.5, hspace=0.35)\n",
    "\n",
    "ax = fig.add_subplot(211)\n",
    "ax.plot(obj, '.-r')\n",
    "ax.set_xlabel(r'SGD iteration $k$', fontproperties=OPTIONS.fontproperties)\n",
    "ax.set_ylabel(r'$G(\\mathcal{X}_v, {\\bf \\theta}_k)$', fontproperties=OPTIONS.fontproperties)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontproperties(OPTIONS.fontproperties)\n",
    "# ax.set_ylim([0, 0.025])\n",
    "\n",
    "ax = fig.add_subplot(212)\n",
    "ax.plot(param_changes, '.-r')\n",
    "ax.set_xlabel(r'SGD iteration $k$', fontproperties=OPTIONS.fontproperties)\n",
    "ax.set_ylabel(r'$||{\\bf \\theta}_k - {\\bf \\theta}_{k-1}||_\\infty$', fontproperties=OPTIONS.fontproperties)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontproperties(OPTIONS.fontproperties)\n",
    "# ax.set_ylim([0, 0.03])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if OPTIONS.save_figs:\n",
    "    if OPTIONS.saturate and not OPTIONS.use_linear_dynamics:\n",
    "        gamma_string = str(feed_dict[gamma])[2:]\n",
    "        fig.savefig(OPTIONS.fig_path + 'pendulum_policyeval_training_gamma' + gamma_string + '.pdf', bbox_inches='tight')\n",
    "    elif not OPTIONS.saturate and OPTIONS.use_linear_dynamics:\n",
    "        fig.savefig(OPTIONS.fig_path + 'pendulum_policyeval_training_linear_.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obj[-1])\n",
    "temp = session.run(grads_and_vars, feed_dict)\n",
    "\n",
    "grad_list = []\n",
    "for tup in temp:\n",
    "    grad_list.append(tup[0].ravel())\n",
    "#     print(tup[0].shape)\n",
    "\n",
    "grad = np.concatenate(grad_list)\n",
    "print(grad.shape)\n",
    "print(np.linalg.norm(grad))\n",
    "print(np.abs(grad).max())\n",
    "print(np.abs(grad).max() * feed_dict[learning_rate])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Dynamics: Trajectories, ROA, and Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_loop_dynamics = lambda x: future_states.eval({states: x})\n",
    "cost_function = lambda x: - rewards.eval({states: x})\n",
    "discount = feed_dict[gamma]\n",
    "\n",
    "roa_horizon = 500\n",
    "cost_horizon = 1000\n",
    "tol = 1e-2\n",
    "N = 101\n",
    "\n",
    "norms = np.rad2deg(state_norm)\n",
    "maxes = np.copy(norms)\n",
    "grid = gridify(norms, maxes, N)\n",
    "\n",
    "trajectories, roa, _ = compute_roa(grid, closed_loop_dynamics, roa_horizon, tol)\n",
    "true_cost = estimate_cost(grid, closed_loop_dynamics, cost_function, discount, cost_horizon, tol)\n",
    "\n",
    "scaled_discrete_points = [norm * points for norm, points in zip(norms, grid.discrete_points)]\n",
    "xx, yy = np.meshgrid(*scaled_discrete_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5), dpi=OPTIONS.dpi)\n",
    "plot_limits = np.column_stack((- maxes, maxes))\n",
    "ax.set_aspect(maxes[0] / maxes[1])\n",
    "ax.set_xlim(plot_limits[0])\n",
    "ax.set_ylim(plot_limits[1])\n",
    "ax.set_xlabel(r'$\\phi$ [deg]', fontproperties=OPTIONS.fontproperties)\n",
    "ax.set_ylabel(r'$\\dot{\\phi}$ [deg/s]', fontproperties=OPTIONS.fontproperties)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontproperties(OPTIONS.fontproperties)\n",
    "    \n",
    "# ROA\n",
    "im = ax.imshow(roa.T, origin='lower', extent=plot_limits.ravel(), aspect=maxes[0] / maxes[1], cmap=BINARY_MAP)\n",
    "\n",
    "# Sub-sample discretization for faster and clearer plotting\n",
    "N_traj = 13\n",
    "skip = int(N / N_traj)\n",
    "sub_idx = np.arange(grid.nindex).reshape(grid.num_points)\n",
    "sub_idx = sub_idx[::skip, ::skip].ravel()\n",
    "sub_trajectories = trajectories[sub_idx, :, :]\n",
    "\n",
    "# Trajectories\n",
    "for n in range(sub_trajectories.shape[0]):\n",
    "    theta = sub_trajectories[n, 0, :] * norms[0]\n",
    "    omega = sub_trajectories[n, 1, :] * norms[1]\n",
    "    ax.plot(theta, omega, 'k--', linewidth=0.6)\n",
    "sub_states = grid.all_points[sub_idx]\n",
    "dx_dt = (future_states.eval({states: sub_states}) - sub_states) / dt\n",
    "dx_dt = dx_dt / np.linalg.norm(dx_dt, ord=2, axis=1, keepdims=True)\n",
    "ax.quiver(sub_states[:, 0] * norms[0], sub_states[:, 1] * norms[1], dx_dt[:, 0], dx_dt[:, 1], scale=None, pivot='mid', headwidth=4, headlength=8, color='k')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if OPTIONS.save_figs:\n",
    "    fig.savefig(OPTIONS.fig_path + 'pendulum_lqr_roa.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 5), dpi=OPTIONS.dpi)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel(r'$\\phi$ [deg]', labelpad=10, fontproperties=OPTIONS.fontproperties)\n",
    "ax.set_ylabel(r'$\\dot{\\phi}$ [deg/s]', labelpad=10, fontproperties=OPTIONS.fontproperties)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels() + ax.get_zticklabels()):\n",
    "    label.set_fontproperties(OPTIONS.fontproperties)\n",
    "    \n",
    "# ROA\n",
    "ax.contourf(xx, yy, roa, cmap=BINARY_MAP, zdir='z', offset=0)\n",
    "\n",
    "# Cost function\n",
    "surf = ax.plot_surface(xx, yy, true_cost, color='b', alpha=0.65)\n",
    "surf._facecolors2d = surf._facecolors3d\n",
    "surf._edgecolors2d = surf._edgecolors3d\n",
    "\n",
    "# Legend\n",
    "proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in [(0., 0., 1., 0.65), (0., 1., 0., 0.65)]]    \n",
    "ax.legend(proxy, [r'$J_{\\pi}({\\bf x})$', 'ROA'], prop=OPTIONS.fontproperties, loc=(0.85, 0.85))\n",
    "\n",
    "ax.view_init(None, -45)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if OPTIONS.save_figs:\n",
    "    if OPTIONS.saturate and not OPTIONS.use_linear_dynamics:\n",
    "        gamma_string = str(feed_dict[gamma])[2:]\n",
    "        fig.savefig(OPTIONS.fig_path + 'pendulum_lqr_costfunc_gamma' + gamma_string + '.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 5), dpi=OPTIONS.dpi)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel(r'$\\phi$ [deg]', labelpad=10, fontproperties=OPTIONS.fontproperties)\n",
    "ax.set_ylabel(r'$\\dot{\\phi}$ [deg/s]', labelpad=10, fontproperties=OPTIONS.fontproperties)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels() + ax.get_zticklabels()):\n",
    "    label.set_fontproperties(OPTIONS.fontproperties)\n",
    "\n",
    "# ROA\n",
    "# ax.contourf(xx, yy, roa, cmap=BINARY_MAP, zdir='z', offset=0)\n",
    "\n",
    "# Cost functions\n",
    "approx_cost = - values.eval({states: grid.all_points}).reshape(grid.num_points)\n",
    "surf_true = ax.plot_surface(xx, yy, true_cost, color='b', alpha=0.65)\n",
    "surf_approx = ax.plot_surface(xx, yy, approx_cost, color='r', alpha=0.65)\n",
    "for surf in (surf_true, surf_approx):\n",
    "    surf._facecolors2d = surf._facecolors3d\n",
    "    surf._edgecolors2d = surf._edgecolors3d\n",
    "\n",
    "# Legend\n",
    "proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in [(0., 0., 1., 0.65), (1., 0., 0., 0.65)]]#, (0., 1., 0., 0.65)]]    \n",
    "ax.legend(proxy, [r'$J_{\\pi}({\\bf x})$', r'$J_{\\bf \\theta}({\\bf x})$'], prop=OPTIONS.fontproperties, loc=(0.85, 0.85))\n",
    "\n",
    "ax.view_init(None, -45)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if OPTIONS.save_figs:\n",
    "    gamma_string = str(feed_dict[gamma])[2:]\n",
    "    fig.savefig(OPTIONS.fig_path + 'pendulum_policyeval_costfunc_gamma' + gamma_string + '.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Approximate Policy Improvement\n",
    "\n",
    "Fix the value function, and learn the corresponding policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Value Function\n",
    "\n",
    "Use supervised learning to closely approximate a fixed value function with a neural network, and use it to learn the LQR policy, possibly with saturation constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_loop_dynamics = lambda x: future_states.eval({states: x})\n",
    "cost_function = lambda x: - rewards.eval({states: x})\n",
    "\n",
    "discount = 0.965\n",
    "cost_horizon = 1000\n",
    "tol = 1e-2\n",
    "\n",
    "true_cost = estimate_cost(grid, closed_loop_dynamics, cost_function, discount, cost_horizon, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('supervised_value_function_learning'):\n",
    "    true_values = tf.placeholder(OPTIONS.tf_dtype, shape=[None, 1], name='true_values')\n",
    "    loss = tf.abs(values - true_values)\n",
    "    objective = tf.reduce_mean(loss, name='objective')\n",
    "    \n",
    "    learning_rate = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='learning_rate')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_update = optimizer.minimize(objective, var_list=value_function.parameters)\n",
    "\n",
    "try:\n",
    "    del obj\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed dict with hyperparameters\n",
    "feed_dict = {\n",
    "    states:         np.zeros((1, state_dim)), # placeholder\n",
    "    true_values:    np.zeros((1, 1)),         # placeholder\n",
    "    learning_rate:  0.005\n",
    "}\n",
    "max_iters  = 1000\n",
    "batch_size = 1000\n",
    "\n",
    "test_set = grid.all_points\n",
    "test_values = - true_cost.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'obj' not in locals():\n",
    "    obj = []\n",
    "    param_changes = []\n",
    "\n",
    "true_cost  = true_cost.reshape((-1, 1))\n",
    "old_params = session.run(value_function.parameters)\n",
    "\n",
    "for i in tqdm(range(max_iters)):\n",
    "    batch_idx = np.random.randint(grid.nindex, size=batch_size)\n",
    "    feed_dict[states] = grid.all_points[batch_idx]\n",
    "    feed_dict[true_values] = - true_cost[batch_idx]\n",
    "    session.run(training_update, feed_dict)\n",
    "    \n",
    "    new_params = session.run(value_function.parameters)\n",
    "    param_changes.append(get_parameter_change(old_params, new_params, 'inf'))\n",
    "    old_params = list(new_params)\n",
    "    \n",
    "    feed_dict[states] = test_set\n",
    "    feed_dict[true_values] = test_values\n",
    "    obj.append(objective.eval(feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_cost = true_cost.reshape(grid.num_points)\n",
    "approx_cost = - values.eval({states: grid.all_points}).reshape(grid.num_points)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5), dpi=OPTIONS.dpi)\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(obj, 'r')\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.plot_surface(xx, yy, true_cost, color='b', alpha=0.65)\n",
    "ax.plot_surface(xx, yy, approx_cost, color='r', alpha=0.65)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Graph and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the parametric policy\n",
    "states = tf.placeholder(OPTIONS.tf_dtype, shape=[None, state_dim], name='states')\n",
    "actions = policy(states)\n",
    "rewards = reward_function(states, actions)\n",
    "future_states = dynamics(states, actions)\n",
    "\n",
    "# Future values according to parametric policy and true value function\n",
    "future_values = value_function(future_states)\n",
    "\n",
    "with tf.name_scope('approximate_policy_evaluation'):\n",
    "    # Discount factor and scaling\n",
    "    gamma = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='discount_factor')\n",
    "    max_state = np.ones((1, state_dim))\n",
    "    max_action = np.ones((1, action_dim))\n",
    "    r_max = np.linalg.multi_dot((max_state, Q, max_state.T)) + np.linalg.multi_dot((max_action, R, max_action.T))\n",
    "    scaling = (1 - gamma) / r_max.ravel()\n",
    "    \n",
    "    # Objective function    \n",
    "    objective = - scaling * tf.reduce_mean(rewards + gamma * future_values, name='objective')\n",
    "    \n",
    "    # Optimizer settings\n",
    "    learning_rate = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='learning_rate')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_update = optimizer.minimize(objective, var_list=policy.parameters)\n",
    "    \n",
    "with tf.name_scope('state_sampler'):\n",
    "    batch_size = tf.placeholder(tf.int32, shape=[], name='batch_size')\n",
    "    batch = tf.random_uniform([batch_size, state_dim], -1, 1, dtype=OPTIONS.tf_dtype, name='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "session.run(tf.variables_initializer(policy.parameters))\n",
    "try:\n",
    "    del obj\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed dict with hyperparameters\n",
    "feed_dict = {\n",
    "    states:         np.zeros((1, state_dim)), # placeholder\n",
    "    gamma:          discount,                 # use the same discount factor from the value function!\n",
    "    learning_rate:  0.6,\n",
    "    batch_size:     100,   \n",
    "}\n",
    "max_iters = 1000\n",
    "test_size = 1000\n",
    "\n",
    "# Uniformly-distributed test set\n",
    "grid_length = np.power(test_size, 1 / state_dim)        # test_size = N^d, solve for N\n",
    "grid_length = int(2 * np.floor(grid_length / 2) + 1)    # round N to the nearest odd integer to include 0 in grid\n",
    "state_limits = np.array([[-1., 1.]] * state_dim)        # states are normalized to [-1, 1]^d\n",
    "num_points = [grid_length, ] * state_dim\n",
    "test_set = safe_learning.GridWorld(state_limits, num_points).all_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of objective value and parameter changes during training\n",
    "if 'obj' not in locals():\n",
    "    obj = []\n",
    "    param_changes = []\n",
    "    \n",
    "old_params = session.run(policy.parameters)\n",
    "\n",
    "for i in tqdm(range(max_iters)):\n",
    "    feed_dict[states] = batch.eval(feed_dict)\n",
    "    session.run(training_update, feed_dict)\n",
    "    \n",
    "    new_params = session.run(policy.parameters)\n",
    "    param_changes.append(get_parameter_change(old_params, new_params, 'inf'))\n",
    "    old_params = list(new_params)\n",
    "    \n",
    "    feed_dict[states] = test_set\n",
    "    obj.append(objective.eval(feed_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5), dpi=OPTIONS.dpi)\n",
    "fig.subplots_adjust(wspace=0.5, hspace=0.35)\n",
    "\n",
    "ax = fig.add_subplot(211)\n",
    "ax.plot(obj, '.-r')\n",
    "ax.set_xlabel(r'SGD iteration $k$', fontproperties=OPTIONS.fontproperties)\n",
    "ax.set_ylabel(r'$H(\\mathcal{X}_v, {\\bf \\delta}_k)$', fontproperties=OPTIONS.fontproperties)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontproperties(OPTIONS.fontproperties)\n",
    "# ax.set_ylim([0.62, 0.635])\n",
    "\n",
    "ax = fig.add_subplot(212)\n",
    "ax.plot(param_changes, '.-r')\n",
    "ax.set_xlabel(r'SGD iteration $k$', fontproperties=OPTIONS.fontproperties)\n",
    "ax.set_ylabel(r'$||{\\bf \\delta}_k - {\\bf \\delta}_{k-1}||_\\infty$', fontproperties=OPTIONS.fontproperties)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontproperties(OPTIONS.fontproperties)\n",
    "# ax.set_ylim([0, 0.002])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if OPTIONS.save_figs:\n",
    "    if OPTIONS.saturate and not OPTIONS.use_linear_dynamics:\n",
    "        gamma_string = str(feed_dict[gamma])[2:]\n",
    "        fig.savefig(OPTIONS.fig_path + 'pendulum_policyimpv_training_gamma' + gamma_string + '.pdf', bbox_inches='tight')\n",
    "    elif not OPTIONS.saturate and OPTIONS.use_linear_dynamics:\n",
    "        fig.savefig(OPTIONS.fig_path + 'pendulum_policyimpv_training_linear_.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 5), dpi=OPTIONS.dpi)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel(r'$\\phi$ [deg]', labelpad=10, fontproperties=OPTIONS.fontproperties)\n",
    "ax.set_ylabel(r'$\\dot{\\phi}$ [deg/s]', labelpad=10, fontproperties=OPTIONS.fontproperties)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels() + ax.get_zticklabels()):\n",
    "    label.set_fontproperties(OPTIONS.fontproperties)\n",
    "    \n",
    "# Policies\n",
    "approx = u_max * actions.eval({states: grid.all_points}).reshape(grid.num_points)\n",
    "truth = u_max * policy_lqr(grid.all_points).eval().reshape(grid.num_points)\n",
    "for zz, c in zip([approx, truth], ['r', 'b']):\n",
    "    surf = ax.plot_surface(xx, yy, zz, color=c, alpha=0.65)\n",
    "    surf._facecolors2d = surf._facecolors3d\n",
    "    surf._edgecolors2d = surf._edgecolors3d\n",
    "\n",
    "# Legend\n",
    "proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in [(0., 0., 1., 0.65), (1., 0., 0., 0.65)]]    \n",
    "ax.legend(proxy, [r'$\\pi({\\bf x})$ [N]', r'$\\pi_{\\bf \\delta}({\\bf x})$ [N]'], prop=OPTIONS.fontproperties, loc=(0.85, 0.85))\n",
    "\n",
    "ax.set_zlim([-0.4, 0.4])\n",
    "ax.view_init(None, -45)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if OPTIONS.save_figs:\n",
    "    gamma_string = str(feed_dict[gamma])[2:]\n",
    "    fig.savefig(OPTIONS.fig_path + 'pendulum_policyimpv_policy_gamma' + gamma_string + '.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New ROA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_loop_dynamics = lambda x: future_states.eval({states: x})\n",
    "roa_horizon = 600\n",
    "tol = 0.1\n",
    "new_trajectories, new_roa, new_dists = compute_roa(grid, closed_loop_dynamics, roa_horizon, tol, equilibrium=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5), dpi=OPTIONS.dpi)\n",
    "plot_limits = np.column_stack((- maxes, maxes))\n",
    "ax.set_aspect(maxes[0] / maxes[1])\n",
    "ax.set_xlim(plot_limits[0])\n",
    "ax.set_ylim(plot_limits[1])\n",
    "ax.set_xlabel(r'$\\phi$ [deg]', fontproperties=OPTIONS.fontproperties)\n",
    "ax.set_ylabel(r'$\\dot{\\phi}$ [deg/s]', fontproperties=OPTIONS.fontproperties)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontproperties(OPTIONS.fontproperties)\n",
    "    \n",
    "# Compare ROAs\n",
    "cmap = ListedColormap([(1., 1., 1., 0.), (0., 1., 0., 0.65), (1., 0., 0., 0.65)])\n",
    "z = roa.astype(int) + new_roa.astype(int)\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=maxes[0] / maxes[1], cmap=cmap)\n",
    "\n",
    "# Sub-sample discretization for faster and clearer plotting\n",
    "N_traj = 13\n",
    "skip = int(N / N_traj)\n",
    "sub_idx = np.arange(grid.nindex).reshape(grid.num_points)\n",
    "sub_idx = sub_idx[::skip, ::skip].ravel()\n",
    "sub_trajectories = new_trajectories[sub_idx, :, :]\n",
    "\n",
    "# Trajectories\n",
    "for n in range(sub_trajectories.shape[0]):\n",
    "    theta = sub_trajectories[n, 0, :] * norms[0]\n",
    "    omega = sub_trajectories[n, 1, :] * norms[1]\n",
    "    ax.plot(theta, omega, 'k--', linewidth=0.6)\n",
    "sub_states = grid.all_points[sub_idx]\n",
    "dx_dt = (future_states.eval({states: sub_states}) - sub_states) / dt\n",
    "dx_dt = dx_dt / np.linalg.norm(dx_dt, ord=2, axis=1, keepdims=True)\n",
    "ax.quiver(sub_states[:, 0] * norms[0], sub_states[:, 1] * norms[1], dx_dt[:, 0], dx_dt[:, 1], scale=None, pivot='mid', headwidth=4, headlength=8, color='k')\n",
    "\n",
    "# Legend\n",
    "proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in [(0., 1., 0., 0.65), (1., 0., 0., 0.65)]]    \n",
    "legend = ax.legend(proxy, [r'ROA for $\\pi$', r'ROA for $\\pi_{\\bf \\delta}$'], prop=OPTIONS.fontproperties, loc='upper right')\n",
    "legend.get_frame().set_alpha(1.)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if OPTIONS.save_figs:\n",
    "    gamma_string = str(feed_dict[gamma])[2:]\n",
    "    fig.savefig(OPTIONS.fig_path + 'pendulum_policyimpv_roa_gamma' + gamma_string + '.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Approximate Policy Iteration\n",
    "\n",
    "Train a policy and value function in tandem with approximate policy iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Graph and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use parametric policy and value function\n",
    "states = tf.placeholder(OPTIONS.tf_dtype, shape=[None, state_dim], name='states')\n",
    "actions = policy(states)\n",
    "rewards = reward_function(states, actions)\n",
    "values = value_function(states)\n",
    "future_states = dynamics(states, actions)\n",
    "future_values = value_function(future_states)\n",
    "\n",
    "# Compare with LQR solution, possibly with saturation constraints\n",
    "actions_lqr = policy_lqr(states)\n",
    "rewards_lqr = reward_function(states, actions_lqr)\n",
    "future_states_lqr = dynamics(states, actions_lqr)\n",
    "\n",
    "# Discount factor and scaling\n",
    "max_state = np.ones((1, state_dim))\n",
    "max_action = np.ones((1, action_dim))\n",
    "r_max = np.linalg.multi_dot((max_state, Q, max_state.T)) + np.linalg.multi_dot((max_action, R, max_action.T))\n",
    "gamma = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='discount_factor')\n",
    "eval_scaling = 1 / r_max.ravel()\n",
    "impv_scaling = (1 - gamma) / r_max.ravel()\n",
    "\n",
    "# Policy evaluation\n",
    "with tf.name_scope('value_optimization'):\n",
    "    value_learning_rate = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='learning_rate')\n",
    "    target = tf.stop_gradient(rewards + gamma * future_values, name='target')\n",
    "    value_objective = eval_scaling * tf.reduce_mean(tf.abs(values - target), name='objective')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(value_learning_rate)\n",
    "    value_update = optimizer.minimize(value_objective, var_list=value_function.parameters)\n",
    "\n",
    "# Policy improvement\n",
    "with tf.name_scope('policy_optimization'):\n",
    "    policy_learning_rate = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='learning_rate')\n",
    "    policy_objective = - impv_scaling * tf.reduce_mean(rewards + gamma * future_values, name='objective')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(policy_learning_rate)\n",
    "    policy_update = optimizer.minimize(policy_objective, var_list=policy.parameters)\n",
    "\n",
    "# Sampling    \n",
    "with tf.name_scope('state_sampler'):\n",
    "    batch_size = tf.placeholder(tf.int32, shape=[], name='batch_size')\n",
    "    batch = tf.random_uniform([batch_size, state_dim], -1, 1, dtype=OPTIONS.tf_dtype, name='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "session.run(tf.variables_initializer(value_function.parameters))\n",
    "session.run(tf.variables_initializer(policy.parameters))\n",
    "try:\n",
    "    del value_obj\n",
    "    del policy_obj\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed dict with hyperparameters\n",
    "feed_dict = {\n",
    "    states:                np.zeros((1, state_dim)), # placeholder\n",
    "    gamma:                 0.965,\n",
    "    value_learning_rate:   0.005,           # 0.1 \n",
    "    policy_learning_rate:  0.6,           # 0.6\n",
    "    batch_size:            int(1e2),\n",
    "}\n",
    "max_iters    = 200\n",
    "value_iters  = 100    # 100\n",
    "policy_iters = 10    # 10\n",
    "test_size    = 1e3   # 1e3\n",
    "\n",
    "# Uniformly-distributed test set\n",
    "grid_length = np.power(test_size, 1 / state_dim)        # test_size = N^d, solve for N\n",
    "grid_length = int(2 * np.floor(grid_length / 2) + 1)    # round N to the nearest odd integer to include 0 in grid\n",
    "state_limits = np.array([[-1., 1.]] * state_dim)        # states are normalized to [-1, 1]^d\n",
    "num_points = [grid_length, ] * state_dim\n",
    "test_set = safe_learning.GridWorld(state_limits, num_points).all_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'value_obj' not in locals():\n",
    "    value_obj = []\n",
    "    policy_obj = []\n",
    "    value_param_changes = []\n",
    "    policy_param_changes = []\n",
    "    \n",
    "converged = False\n",
    "old_value_params = session.run(value_function.parameters)\n",
    "old_policy_params = session.run(policy.parameters)\n",
    "\n",
    "for i in tqdm(range(max_iters)):\n",
    "    \n",
    "    # Policy evaluation (value update)\n",
    "    for _ in range(value_iters):\n",
    "        feed_dict[states] = batch.eval(feed_dict)\n",
    "        session.run(value_update, feed_dict)\n",
    "        #\n",
    "    feed_dict[states] = test_set\n",
    "    value_obj.append(value_objective.eval(feed_dict))\n",
    "    new_value_params = session.run(value_function.parameters)\n",
    "    value_param_changes.append(get_parameter_change(old_value_params, new_value_params, 'inf'))\n",
    "    old_value_params = list(new_value_params)\n",
    "\n",
    "    # Policy improvement (policy update)\n",
    "    for _ in range(policy_iters):\n",
    "        feed_dict[states] = batch.eval(feed_dict)\n",
    "        session.run(policy_update, feed_dict)\n",
    "        #\n",
    "    feed_dict[states] = test_set\n",
    "    policy_obj.append(policy_objective.eval(feed_dict))\n",
    "    new_policy_params = session.run(policy.parameters)\n",
    "    policy_param_changes.append(get_parameter_change(old_policy_params, new_policy_params, 'inf'))\n",
    "    old_policy_params = list(new_policy_params)\n",
    "    \n",
    "    # Record objectives\n",
    "#     feed_dict[states] = test_set\n",
    "#     value_obj.append(value_objective.eval(feed_dict))\n",
    "#     policy_obj.append(policy_objective.eval(feed_dict))\n",
    "\n",
    "final_iter = i + 1\n",
    "if converged:\n",
    "    print('Converged after {} iterations.'.format(final_iter))\n",
    "else:\n",
    "    print('Did not converge!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5), dpi=OPTIONS.dpi)\n",
    "fig.subplots_adjust(wspace=0.5, hspace=0.35)\n",
    "\n",
    "ax = fig.add_subplot(211)\n",
    "ax.plot(value_obj, '.-r')\n",
    "ax.set_xlabel(r'Policy iteration $k$', fontproperties=OPTIONS.fontproperties)\n",
    "ax.set_ylabel(r'$G(\\mathcal{X}_v, {\\bf \\theta}_k)$', fontproperties=OPTIONS.fontproperties)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontproperties(OPTIONS.fontproperties)\n",
    "ax.set_ylim([0, None])\n",
    "\n",
    "ax = fig.add_subplot(212)\n",
    "ax.plot(value_param_changes, '.-r')\n",
    "ax.set_xlabel(r'Policy iteration $k$', fontproperties=OPTIONS.fontproperties)\n",
    "ax.set_ylabel(r'$||{\\bf \\theta}_k - {\\bf \\theta}_{k-1}||_\\infty$', fontproperties=OPTIONS.fontproperties)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontproperties(OPTIONS.fontproperties)\n",
    "ax.set_ylim([0, None])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if OPTIONS.save_figs:\n",
    "    gamma_string = str(feed_dict[gamma])[2:]\n",
    "    fig.savefig(OPTIONS.fig_path + 'pendulum_policyiter_costfunc_training_gamma' + gamma_string + '.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5), dpi=OPTIONS.dpi)\n",
    "fig.subplots_adjust(wspace=0.5, hspace=0.35)\n",
    "\n",
    "ax = fig.add_subplot(211)\n",
    "ax.plot(policy_obj, '.-r')\n",
    "ax.set_xlabel(r'Policy iteration $k$', fontproperties=OPTIONS.fontproperties)\n",
    "ax.set_ylabel(r'$H(\\mathcal{X}_v, {\\bf \\delta}_k)$', fontproperties=OPTIONS.fontproperties)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontproperties(OPTIONS.fontproperties)\n",
    "# ax.set_ylim([0, 2])\n",
    "\n",
    "ax = fig.add_subplot(212)\n",
    "ax.plot(policy_param_changes, '.-r')\n",
    "ax.set_xlabel(r'Policy iteration $k$', fontproperties=OPTIONS.fontproperties)\n",
    "ax.set_ylabel(r'$||{\\bf \\delta}_k - {\\bf \\delta}_{k-1}||_\\infty$', fontproperties=OPTIONS.fontproperties)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontproperties(OPTIONS.fontproperties)\n",
    "# ax.set_ylim([0, 0.025])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if OPTIONS.save_figs:\n",
    "    gamma_string = str(feed_dict[gamma])[2:]\n",
    "    fig.savefig(OPTIONS.fig_path + 'pendulum_policyiter_policy_training_gamma' + gamma_string + '.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 5), dpi=OPTIONS.dpi)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel(r'$\\phi$ [deg]', labelpad=10, fontproperties=OPTIONS.fontproperties)\n",
    "ax.set_ylabel(r'$\\dot{\\phi}$ [deg/s]', labelpad=10, fontproperties=OPTIONS.fontproperties)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels() + ax.get_zticklabels()):\n",
    "    label.set_fontproperties(OPTIONS.fontproperties)\n",
    "\n",
    "# Costs\n",
    "approx = - values.eval({states: grid.all_points}).reshape(grid.num_points)\n",
    "truth = true_cost.reshape(grid.num_points)\n",
    "for zz, c in zip([approx, truth], ['r', 'b']):\n",
    "    surf = ax.plot_surface(xx, yy, zz, color=c, alpha=0.65)\n",
    "    surf._facecolors2d = surf._facecolors3d\n",
    "    surf._edgecolors2d = surf._edgecolors3d\n",
    "    \n",
    "\n",
    "# ROA\n",
    "ax.contourf(xx, yy, roa, cmap=BINARY_MAP, zdir='z', offset=0)\n",
    "\n",
    "# Legend\n",
    "proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in [(0., 0., 1., 0.65), (1., 0., 0., 0.65), (0., 1., 0., 0.65)]]    \n",
    "ax.legend(proxy, [r'$J_{\\pi}({\\bf x})$', r'$J_{\\bf \\theta}({\\bf x})$', r'ROA for $\\pi$'], prop=OPTIONS.fontproperties, loc=(0.85, 0.85))\n",
    "\n",
    "ax.view_init(None, -45)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if OPTIONS.save_figs:\n",
    "    gamma_string = str(feed_dict[gamma])[2:]\n",
    "    fig.savefig(OPTIONS.fig_path + 'pendulum_policyiter_costfunc_gamma' + gamma_string + '.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 5), dpi=OPTIONS.dpi)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel(r'$\\phi$ [deg]', labelpad=10, fontproperties=OPTIONS.fontproperties)\n",
    "ax.set_ylabel(r'$\\dot{\\phi}$ [deg/s]', labelpad=10, fontproperties=OPTIONS.fontproperties)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels() + ax.get_zticklabels()):\n",
    "    label.set_fontproperties(OPTIONS.fontproperties)\n",
    "\n",
    "# Policies\n",
    "approx = u_max * actions.eval({states: grid.all_points}).reshape(grid.num_points)\n",
    "truth = u_max * policy_lqr(grid.all_points).eval().reshape(grid.num_points)\n",
    "for zz, c in zip([approx, truth], ['r', 'b']):\n",
    "    surf = ax.plot_surface(xx, yy, zz, color=c, alpha=0.65)\n",
    "    surf._facecolors2d = surf._facecolors3d\n",
    "    surf._edgecolors2d = surf._edgecolors3d\n",
    "\n",
    "# Legend\n",
    "proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in [(0., 0., 1., 0.65), (1., 0., 0., 0.65)]]    \n",
    "ax.legend(proxy, [r'$\\pi({\\bf x})$ [N]', r'$\\pi_{\\bf \\delta}({\\bf x})$ [N]'], prop=OPTIONS.fontproperties, loc=(0.85, 0.85))\n",
    "\n",
    "ax.set_zlim([-0.4, 0.4])\n",
    "ax.view_init(None, -45)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if OPTIONS.save_figs:\n",
    "    gamma_string = str(feed_dict[gamma])[2:]\n",
    "    fig.savefig(OPTIONS.fig_path + 'pendulum_policyiter_policy_gamma' + gamma_string + '.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New ROA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_loop_dynamics = lambda x: future_states.eval({states: x})\n",
    "roa_horizon = 500\n",
    "tol = 0.01\n",
    "new_trajectories, new_roa, new_dists = compute_roa(grid, closed_loop_dynamics, roa_horizon, tol, equilibrium=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5), dpi=OPTIONS.dpi)\n",
    "plot_limits = np.column_stack((- maxes, maxes))\n",
    "ax.set_aspect(maxes[0] / maxes[1])\n",
    "ax.set_xlim(plot_limits[0])\n",
    "ax.set_ylim(plot_limits[1])\n",
    "ax.set_xlabel(r'$\\phi$ [deg]', fontproperties=OPTIONS.fontproperties)\n",
    "ax.set_ylabel(r'$\\dot{\\phi}$ [deg/s]', fontproperties=OPTIONS.fontproperties)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontproperties(OPTIONS.fontproperties)\n",
    "    \n",
    "# Compare ROAs\n",
    "cmap = ListedColormap([(1., 1., 1., 0.), (0., 1., 0., 0.65), (1., 0., 0., 0.65)])\n",
    "z = roa.astype(int) + new_roa.astype(int)\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=maxes[0] / maxes[1], cmap=cmap)\n",
    "\n",
    "# Sub-sample discretization for faster and clearer plotting\n",
    "N_traj = 13\n",
    "skip = int(N / N_traj)\n",
    "sub_idx = np.arange(grid.nindex).reshape(grid.num_points)\n",
    "sub_idx = sub_idx[::skip, ::skip].ravel()\n",
    "sub_trajectories = new_trajectories[sub_idx, :, :]\n",
    "\n",
    "# Trajectories\n",
    "for n in range(sub_trajectories.shape[0]):\n",
    "    theta = sub_trajectories[n, 0, :] * norms[0]\n",
    "    omega = sub_trajectories[n, 1, :] * norms[1]\n",
    "    ax.plot(theta, omega, 'k--', linewidth=0.6)\n",
    "sub_states = grid.all_points[sub_idx]\n",
    "dx_dt = (future_states.eval({states: sub_states}) - sub_states) / dt\n",
    "dx_dt = dx_dt / np.linalg.norm(dx_dt, ord=2, axis=1, keepdims=True)\n",
    "ax.quiver(sub_states[:, 0] * norms[0], sub_states[:, 1] * norms[1], dx_dt[:, 0], dx_dt[:, 1], scale=None, pivot='mid', headwidth=4, headlength=8, color='k')\n",
    "\n",
    "# Legend\n",
    "proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in [(0., 1., 0., 0.65), (1., 0., 0., 0.65)]]    \n",
    "legend = ax.legend(proxy, [r'ROA for $\\pi$', r'ROA for $\\pi_{\\bf \\delta}$'], prop=OPTIONS.fontproperties, loc='upper right')\n",
    "legend.get_frame().set_alpha(1.)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if OPTIONS.save_figs:\n",
    "    gamma_string = str(feed_dict[gamma])[2:]\n",
    "    fig.savefig(OPTIONS.fig_path + 'pendulum_policyiter_roa_gamma' + gamma_string + '.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Input and Step Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic = np.array([0.1, 0.2])\n",
    "const = 0.4\n",
    "steps = 1000\n",
    "\n",
    "# Zero-input response\n",
    "plot_closedloop_response(dynamics, policy, policy_lqr, steps, dt, 'zero', ic=ic, labels=['Learned','True'])\n",
    "\n",
    "# Step response\n",
    "plot_closedloop_response(dynamics, policy, policy_lqr, steps, dt, 'step', const=const, labels=['Learned','True'])\n",
    "\n",
    "# Impulse response\n",
    "# plot_closedloop_response(dynamics, policy, lqr_policy, steps, dt, 'impulse', labels=['Learned','True'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting.show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
