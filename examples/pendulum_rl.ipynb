{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for an Inverted Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.linalg import block_diag\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "import safe_learning\n",
    "import plotting\n",
    "from utilities import (sample_box, sample_box_boundary, sample_ellipsoid, constrained_batch_sampler, \n",
    "                       InvertedPendulum, compute_closedloop_response, get_max_parameter_change)\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x\n",
    "    \n",
    "np_dtype = safe_learning.config.np_dtype\n",
    "tf_dtype = safe_learning.config.dtype\n",
    "\n",
    "tf.reset_default_graph()\n",
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "initialized = False\n",
    "\n",
    "\n",
    "# TODO testing ****************************************#\n",
    "\n",
    "train_policy = True\n",
    "\n",
    "train_value_function = True\n",
    "\n",
    "clip_states = False\n",
    "\n",
    "use_rollout_target = False\n",
    "\n",
    "saturate = True\n",
    "\n",
    "#******************************************************#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFNetwork(safe_learning.DeterministicFunction):\n",
    "    def __init__(self, limits, num_states, variances=None, name='rbf_network'):\n",
    "        super(RBFNetwork, self).__init__(name=name)\n",
    "        self.discretization = safe_learning.GridWorld(np.asarray(limits), num_states)\n",
    "        self._hidden_units = self.discretization.nindex\n",
    "        if variances is not None:\n",
    "            self.variances = variances\n",
    "        else:\n",
    "            self.variances = np.min(self.discretization.unit_maxes)**2    # tau**2\n",
    "        self._betas = 1 / (2*self.variances)\n",
    "        self.centres = self.discretization.all_points\n",
    "        self._centres_3D = np.reshape(self.centres.T, (1, state_dim, self._hidden_units))\n",
    "    \n",
    "    def build_evaluation(self, states):\n",
    "        initializer=tf.contrib.layers.xavier_initializer()\n",
    "#         initializer=tf.constant_initializer()\n",
    "        W = tf.get_variable('weights', dtype=tf_dtype, shape=[self._hidden_units, 1], initializer=initializer)\n",
    "        states_3D = tf.expand_dims(states, axis=2)\n",
    "        phi_X = tf.exp(-self._betas*tf.reduce_sum(tf.square(states_3D - self._centres_3D), axis=1, keep_dims=False))\n",
    "        output = tf.matmul(phi_X, W)\n",
    "        return output\n",
    "\n",
    "\n",
    "def tf_rollout(initial_states, tf_dynamics, tf_policy, tf_reward_function, tf_horizon, gamma=1.0):\n",
    "    \n",
    "    if isinstance(gamma, tf.Tensor):\n",
    "        tf_gamma = tf.cast(gamma, tf_dtype)\n",
    "    else:\n",
    "        tf_gamma = tf.constant(gamma, dtype=tf_dtype)\n",
    "    \n",
    "    def body(rollout, states, idx):\n",
    "        actions = tf_policy(states)\n",
    "        rollout = rollout + tf.pow(tf_gamma, idx)*tf_reward_function(states, actions)\n",
    "        states = tf_dynamics(states, actions)\n",
    "        idx = idx + 1\n",
    "        return rollout, states, idx\n",
    "\n",
    "    def condition(rollout, states, idx):\n",
    "        return idx < tf_horizon\n",
    "\n",
    "    initial_idx = tf.constant(0, dtype=tf_dtype)\n",
    "    initial_rollout = tf.constant(0, shape=[1, 1], dtype=tf_dtype) # broadcasting takes care of N states\n",
    "    \n",
    "    shape_invariants = [tf.TensorShape([None, 1]), initial_states.get_shape(), initial_idx.get_shape()]\n",
    "    rollout, final_states, _ = tf.while_loop(condition, body, [initial_rollout, initial_states, initial_idx], \n",
    "                                             shape_invariants)\n",
    "    return rollout, final_states\n",
    "\n",
    "\n",
    "def plot_function_3D(tf_function_A, tf_function_B, n_points, colors=['r','b'], title='Value function', show=True):\n",
    "    xx, yy = np.mgrid[-1:1:np.complex(0, n_points[0]), -1:1:np.complex(0, n_points[1])]\n",
    "    grid = np.column_stack((xx.ravel(), yy.ravel()))\n",
    "    tf_grid = tf.constant(grid, dtype=tf_dtype)\n",
    "    f_vals_A = tf_function_A(tf_grid).eval().reshape(n_points)\n",
    "    f_vals_B = tf_function_B(tf_grid).eval().reshape(n_points)\n",
    "                           \n",
    "    fig = plt.figure(figsize=(6, 5), dpi=100)\n",
    "    fig.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "    \n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(xx, yy, f_vals_A, color=colors[0], alpha=0.5)\n",
    "    ax.plot_surface(xx, yy, f_vals_B, color=colors[1], alpha=0.5)\n",
    "    \n",
    "    if isinstance(tf_function_A, RBFNetwork):\n",
    "        centre_vals = tf_function_A(tf_function_A.centres).eval()\n",
    "        ax.scatter(tf_function_A.centres[:, 0], tf_function_A.centres[:, 1], centre_vals, color=colors[0], marker='o')\n",
    "    if isinstance(tf_function_B, RBFNetwork):\n",
    "        centre_vals = tf_function_B(tf_function_B.centres).eval()\n",
    "        ax.scatter(tf_function_B.centres[:, 0], tf_function_B.centres[:, 1], centre_vals, color=colors[1], marker='o')\n",
    "    \n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel(r'$x$', fontsize=14)\n",
    "    ax.set_ylabel(r'$\\theta$', fontsize=14)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_closedloop_response(dynamics, policy1, policy2, steps, dt, reference='zero', const=1.0, ic=None,\n",
    "                             labels=['Policy 1','Policy 2'], colors=['r','b'], show=True):\n",
    "    state_dim = 2\n",
    "    state_traj1, action_traj1, t, _ = compute_closedloop_response(dynamics, policy1, state_dim, \n",
    "                                                                  steps, dt, reference, const, ic)\n",
    "    state_traj2, action_traj2, _, _ = compute_closedloop_response(dynamics, policy2, state_dim,\n",
    "                                                                  steps, dt, reference, const, ic)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 3), dpi=100)\n",
    "    fig.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "    \n",
    "    if reference=='zero':\n",
    "        title_string = r'Zero-Input Response'\n",
    "    elif reference=='impulse':\n",
    "        title_string = r'Impulse Response'\n",
    "    elif reference=='step':\n",
    "        title_string = r'Step Response, $r = %.1g$' % const\n",
    "    \n",
    "    if ic is not None:\n",
    "        ic_tuple = (ic[0], ic[1])\n",
    "    else:\n",
    "        ic_tuple = (0, 0)\n",
    "    title_string = title_string + r', $s_0 = (%.1g, %.1g)$' % ic_tuple\n",
    "    fig.suptitle(title_string, fontsize=18)         \n",
    "    \n",
    "    state_names = [r'$\\theta$', r'$\\omega$']\n",
    "    plot_idx = (1, 2)\n",
    "    for i in range(2):\n",
    "        ax = fig.add_subplot(1, 3, plot_idx[i])\n",
    "        ax.plot(t, state_traj1[:, i], colors[0])\n",
    "        ax.plot(t, state_traj2[:, i], colors[1])\n",
    "        ax.set_xlabel(r'$t$', fontsize=14)\n",
    "        ax.set_ylabel(state_names[i], fontsize=14, rotation=0)\n",
    "    ax = fig.add_subplot(1, 3, 3)\n",
    "    plot1 = ax.plot(t, action_traj1, color=colors[0])\n",
    "    plot2 = ax.plot(t, action_traj2, color=colors[1])\n",
    "    ax.set_xlabel(r'$t$', fontsize=14)\n",
    "    ax.set_ylabel(r'$u$', fontsize=14, rotation=0)\n",
    "    fig.legend((plot1[0], plot2[0]), (labels[0], labels[1]), loc=(0.85, 0.2), fontsize=14)\n",
    "\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System parameters\n",
    "m = 0.5       # pendulum mass [kg]\n",
    "L = 1.        # rod length [m]\n",
    "b = 0.        # rotational friction constant [kg.m**2/s]\n",
    "\n",
    "# Constants\n",
    "dt = 0.01   # sampling time\n",
    "g = 9.81    # gravity\n",
    "\n",
    "# Linearized dynamics\n",
    "dim_state = 2\n",
    "dim_input = 1\n",
    "A = np.array([[0, 1], [g/L, -b/(m*L**2)]], dtype=np_dtype)\n",
    "B = np.array([[0], [1/(m*L**2)]])\n",
    "\n",
    "# State and action normalizers\n",
    "theta_max = np.deg2rad(30)\n",
    "omega_max = np.sqrt(g/L)\n",
    "u_max = 1.5*m*g*L*np.sin(theta_max)\n",
    "\n",
    "state_norm = np.array([theta_max, omega_max])\n",
    "action_norm = np.array([u_max])\n",
    "\n",
    "# Define system and dynamics\n",
    "pendulum = InvertedPendulum(m, L, b, dt, [state_norm, action_norm])\n",
    "state_dim = 2\n",
    "action_dim = 1\n",
    "\n",
    "state_limits = np.array([[-1., 1.]]*state_dim)\n",
    "action_limits = np.array([[-1., 1.]]*action_dim)\n",
    "\n",
    "A, B = pendulum.linearize()   \n",
    "dynamics = safe_learning.functions.LinearSystem((A, B), name='true_dynamics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State cost matrix\n",
    "Q = np.diag([0.1, 0.1])\n",
    "\n",
    "# Action cost matrix\n",
    "R = np.diag([0.1])\n",
    "\n",
    "# Quadratic reward (-cost) function\n",
    "reward_function = safe_learning.QuadraticFunction(block_diag(-Q, -R), name='reward_function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact Optimal Policy and Value Function for the True Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve Lyapunov equation for the exact value function matrix P and optimal feedback law u = -K.dot(s)\n",
    "K, P = safe_learning.utilities.dlqr(A, B, Q, R)\n",
    "print('LQR gain:\\n{}\\n'.format(-K))\n",
    "print('Induced 2-norm of LQR gain:\\n{}\\n'.format(np.linalg.norm(K, 2)))\n",
    "print('LQR cost matrix:\\n{}\\n'.format(P))\n",
    "\n",
    "# LQR policy\n",
    "lqr_policy = safe_learning.functions.LinearSystem((-K,), name='LQR_policy')\n",
    "if saturate:\n",
    "    lqr_policy = safe_learning.Saturation(lqr_policy, -1, 1)\n",
    "\n",
    "# Optimal value function\n",
    "lqr_value_function = safe_learning.functions.QuadraticFunction(-P, name='LQR_value_function')\n",
    "\n",
    "# Approximate maximum ellipsoidal level set contained inside [-1, 1]**d via sampling\n",
    "samples = sample_box_boundary(state_limits, 1e6)\n",
    "test_values = lqr_value_function(tf.constant(samples, tf_dtype)).eval()\n",
    "c_min = np.amin(np.abs(test_values))\n",
    "c_max = np.amax(np.abs(test_values))\n",
    "print('Minimum LQR cost (approx.):\\n{}\\n'.format(c_min))\n",
    "print('Maximum LQR cost (approx.):\\n{}\\n'.format(c_max))\n",
    "\n",
    "# LQR response\n",
    "T = 100\n",
    "ic = np.array([1., 0.]).reshape(1, -1)\n",
    "x, u, t, r = compute_closedloop_response(dynamics, lqr_policy, state_dim, T, dt, 'zero', ic=ic)\n",
    "\n",
    "names = [r'$\\theta$', r'$\\omega$', r'$u$']\n",
    "plt.plot(t, np.concatenate((x, u), axis=1))\n",
    "plt.legend(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Approximators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "max_state = np.ones(state_dim).reshape((1, -1))\n",
    "max_action = np.ones(action_dim).reshape((1, -1))\n",
    "r_max = np.linalg.multi_dot((max_state, Q, max_state.T)) + np.linalg.multi_dot((max_action, R, max_action.T))\n",
    "gamma = tf.placeholder(tf_dtype, shape=[], name='discount_factor')\n",
    "scaling = (1 - gamma) / r_max\n",
    "\n",
    "# Value function\n",
    "if train_value_function:\n",
    "    layer_dims = [64, 64, 1]\n",
    "    activations = [tf.nn.relu, tf.nn.relu, None]\n",
    "    value_function = safe_learning.functions.NeuralNetwork(layer_dims, activations, name='value_function')\n",
    "else:\n",
    "    value_function = lqr_value_function\n",
    "    \n",
    "# Policy\n",
    "if train_policy:\n",
    "    layer_dims = [64, 64, action_dim]\n",
    "    activations = [tf.nn.relu, tf.nn.relu, None]\n",
    "    if saturate:\n",
    "        activations[-1] = tf.nn.tanh\n",
    "    policy = safe_learning.functions.NeuralNetwork(layer_dims, activations, scaling=1., name='policy')\n",
    "else:\n",
    "    policy = lqr_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = tf.placeholder(tf_dtype, shape=[None, state_dim], name='states')\n",
    "actions = policy(states)\n",
    "rewards = reward_function(states, actions)\n",
    "future_states = dynamics(states, actions)\n",
    "\n",
    "values = value_function(states)\n",
    "if clip_states:\n",
    "    future_values = value_function(tf.clip_by_value(future_states, -1, 1))\n",
    "else:\n",
    "    future_values = value_function(future_states)\n",
    "\n",
    "true_values = lqr_value_function(states)\n",
    "true_actions = lqr_policy(states)\n",
    "true_rewards = reward_function(states, true_actions)\n",
    "true_future_states = dynamics(states, true_actions)\n",
    "\n",
    "# Rollout\n",
    "horizon = tf.placeholder(tf_dtype, shape=[], name='rollout_horizon')\n",
    "rollout_op = tf_rollout(states, dynamics, lqr_policy, reward_function, horizon)\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "N = 100\n",
    "T = 100\n",
    "iters = 200\n",
    "initial_states = sample_box(state_limits, 1)\n",
    "initial_states = np.asarray([1., -1.]).reshape(1, -1)\n",
    "\n",
    "# TensorFlow implementation\n",
    "rollout_values = np.zeros((N, T))\n",
    "final_states = initial_states\n",
    "for i in range(T):\n",
    "    temp, final_states = session.run(rollout_op, {states: final_states, horizon: 1})\n",
    "    rollout_values[:, i] = temp.ravel()\n",
    "rollout_values = np.cumsum(rollout_values, axis=1)\n",
    "\n",
    "true = true_values.eval({states: initial_states})\n",
    "diff = np.abs(rollout_values[:, -1] - true)\n",
    "print(np.amax(diff))\n",
    "\n",
    "fig = plt.figure()    \n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(rollout_values[0, :])\n",
    "ax.plot(true[0]*np.ones(T))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bellman error or rollout objective for value update\n",
    "with tf.name_scope('value_optimization'):\n",
    "    value_learning_rate = tf.placeholder(tf_dtype, shape=[], name='learning_rate')\n",
    "\n",
    "    if use_rollout_target:\n",
    "        rollout_horizon = tf.placeholder(tf_dtype, shape=[], name='rollout_horizon')\n",
    "        target = tf_rollout(states, dynamics, policy, reward_function, rollout_horizon, gamma)[0]\n",
    "    else:\n",
    "        target = tf.stop_gradient(rewards + gamma*future_values, name='target')\n",
    "\n",
    "    value_obj = scaling*tf.reduce_mean(tf.abs(values - target), name='objective')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(value_learning_rate)\n",
    "    if train_value_function:\n",
    "        value_update = optimizer.minimize(value_obj, var_list=value_function.parameters)\n",
    "\n",
    "# Pseudo-integration objective for policy update\n",
    "with tf.name_scope('policy_optimization'):\n",
    "    policy_learning_rate = tf.placeholder(tf_dtype, shape=[], name='learning_rate')\n",
    "    policy_obj = -scaling*tf.reduce_mean(rewards + gamma*future_values, name='objective')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(policy_learning_rate)\n",
    "    if train_policy:\n",
    "        policy_update = optimizer.minimize(policy_obj, var_list=policy.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow this cell to be run repeatedly to continue training if desired\n",
    "if not initialized:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    initialized = True\n",
    "if 'value_obj_history' not in locals():\n",
    "    value_obj_history = np.zeros(0)\n",
    "    value_param_history = np.zeros(0)\n",
    "    policy_obj_history = np.zeros(0)\n",
    "    policy_param_history = np.zeros(0)\n",
    "    \n",
    "# Uniformly distributed test set\n",
    "test_size = 1e3\n",
    "grid_length = np.power(test_size, 1 / state_dim)\n",
    "grid_length = int(2*np.floor(grid_length / 2) + 1)\n",
    "test_set = safe_learning.GridWorld(state_limits, [grid_length,]*state_dim).all_points\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 200\n",
    "min_iters = 100\n",
    "level = c_max\n",
    "batch_size = 1e2\n",
    "batch = constrained_batch_sampler(dynamics, policy, state_dim, batch_size, action_limit=0.99, zero_pad=0)\n",
    "\n",
    "value_iters = 100 # 1\n",
    "policy_iters = 10 # 10\n",
    "\n",
    "value_tol = 2e-3\n",
    "policy_tol = 1e-3\n",
    "\n",
    "feed_dict = {\n",
    "    states:               test_set,\n",
    "    gamma:                0.98,\n",
    "    value_learning_rate:  0.01, # 0.1\n",
    "    policy_learning_rate: 0.5,  # 0.1\n",
    "}\n",
    "if use_rollout_target:\n",
    "    feed_dict[rollout_horizon] = 25\n",
    "\n",
    "# Record objective values over time\n",
    "value_obj_eval = np.zeros(max_iters + 1)\n",
    "policy_obj_eval = np.zeros(max_iters + 1)\n",
    "value_obj_eval[0] = value_obj.eval(feed_dict)\n",
    "policy_obj_eval[0] = policy_obj.eval(feed_dict)\n",
    "\n",
    "# For convergence, check the parameter values\n",
    "converged = False\n",
    "value_param_changes = np.zeros(max_iters)\n",
    "policy_param_changes = np.zeros(max_iters)\n",
    "old_value_params = session.run(value_function.parameters)\n",
    "old_policy_params = session.run(policy.parameters)\n",
    "\n",
    "iter_memory = 5\n",
    "col1 = np.arange(iter_memory) + 1\n",
    "col2 = np.ones(iter_memory)\n",
    "A = np.column_stack((col1, col2))\n",
    "lstsq_slope = lambda y: np.linalg.lstsq(A, y)[0][0]\n",
    "\n",
    "for i in tqdm(range(max_iters)):\n",
    "    \n",
    "    # Policy evaluation (value update)\n",
    "    if train_value_function:\n",
    "        for _ in range(value_iters):\n",
    "#             feed_dict[states] = sample_box(state_limits, batch_size)\n",
    "#             feed_dict[states] = sample_ellipsoid(P, level, batch_size)\n",
    "            feed_dict[states] = session.run(batch)\n",
    "            session.run(value_update, feed_dict)\n",
    "        new_value_params = session.run(value_function.parameters)\n",
    "        value_param_changes[i] = get_max_parameter_change(old_value_params, new_value_params)\n",
    "        old_value_params = new_value_params\n",
    "\n",
    "    # Policy improvement (policy update)\n",
    "    if train_policy:\n",
    "        for _ in range(policy_iters):\n",
    "#             feed_dict[states] = sample_box(state_limits, batch_size)\n",
    "#             feed_dict[states] = sample_ellipsoid(P, level, batch_size)\n",
    "            feed_dict[states] = session.run(batch)\n",
    "            session.run(policy_update, feed_dict)\n",
    "        new_policy_params = session.run(policy.parameters)\n",
    "        policy_param_changes[i] = get_max_parameter_change(old_policy_params, new_policy_params)\n",
    "        old_policy_params = new_policy_params\n",
    "    \n",
    "    feed_dict[states] = test_set\n",
    "    value_obj_eval[i+1] = value_obj.eval(feed_dict)\n",
    "    policy_obj_eval[i+1] = policy_obj.eval(feed_dict)\n",
    "    \n",
    "    # TODO debugging    \n",
    "    if np.isnan(value_obj_eval[i+1]) or np.isnan(policy_obj_eval[i+1]):\n",
    "        raise ValueError('Encountered NAN value after {} iterations!'.format(i+1))\n",
    "    \n",
    "    # Break if converged\n",
    "#     if i >= iter_memory and i >= min_iters:\n",
    "#         value_params_converged = np.all(value_param_changes[i-iter_memory+1:i+1] <= value_tol)\n",
    "#         policy_params_converged = np.all(policy_param_changes[i-iter_memory+1:i+1] <= policy_tol)\n",
    "#         if value_params_converged and policy_params_converged:\n",
    "#             converged = True\n",
    "#             break\n",
    "\n",
    "#     value_obj_converged = lstsq_slope(value_obj_eval[i-iter_memory+1:i+1]) <= value_tol\n",
    "#     policy_obj_converged = lstsq_slope(policy_obj_eval[i-iter_memory+1:i+1]) <= policy_tol\n",
    "#     if i > min_iters and value_obj_converged and policy_obj_converged:\n",
    "#         converged = True\n",
    "#         break\n",
    "        \n",
    "\n",
    "final_iter = i+1\n",
    "if converged:\n",
    "    print('Converged after {} iterations.'.format(final_iter))\n",
    "else:\n",
    "    print('Did not converge!')\n",
    "\n",
    "value_obj_history = np.concatenate((value_obj_history, value_obj_eval[:final_iter+1]))\n",
    "value_param_history = np.concatenate((value_param_history, value_param_changes[:final_iter+1]))\n",
    "policy_obj_history = np.concatenate((policy_obj_history, policy_obj_eval[:final_iter+1]))\n",
    "policy_param_history = np.concatenate((policy_param_history, policy_param_changes[:final_iter+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5), dpi=100)\n",
    "fig.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "cap = 20\n",
    "start = 0\n",
    "end = -1\n",
    "\n",
    "ax = fig.add_subplot(221)\n",
    "ax.plot(np.clip(value_obj_history[start:end], None, cap), '.-r')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Value Function Objective')\n",
    "\n",
    "ax = fig.add_subplot(223)\n",
    "ax.plot(np.clip(value_param_history[start:end], None, cap), '.-r')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Max. Value Param. Change')\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(222)\n",
    "ax.plot(np.clip(policy_obj_history[start:end], None, cap), '.-r')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Policy Objective')\n",
    "\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(np.clip(policy_param_history[start:end], None, cap), '.-r')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Max. Policy Param. Change')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Input and Step Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic = np.array([0.1, 0.2])\n",
    "const = 0.4\n",
    "steps = 1000\n",
    "\n",
    "# Zero-input response\n",
    "plot_closedloop_response(dynamics, policy, lqr_policy, steps, dt, 'zero', ic=ic, labels=['Learned','True'])\n",
    "\n",
    "# Step response\n",
    "plot_closedloop_response(dynamics, policy, lqr_policy, steps, dt, 'step', const=const, labels=['Learned','True'])\n",
    "\n",
    "# Impulse response\n",
    "# plot_closedloop_response(dynamics, policy, lqr_policy, steps, dt, 'impulse', labels=['Learned','True'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function and Policy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not initialized:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    initialized = True\n",
    "    time.sleep(1.5)\n",
    "    print('Initialized!')\n",
    "    \n",
    "n_points = [51, 51]\n",
    "plot_function_3D(value_function, lqr_value_function, n_points, title='Value function')\n",
    "plot_function_3D(policy, lqr_policy, n_points, title='Control')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
