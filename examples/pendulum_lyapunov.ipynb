{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability Verification for an Inverted Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "from scipy.linalg import block_diag\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import colors as mpl_colors\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "import safe_learning\n",
    "from utilities import InvertedPendulum, debug\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x\n",
    "    \n",
    "np_dtype = safe_learning.config.np_dtype\n",
    "tf_dtype = safe_learning.config.dtype\n",
    "\n",
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# TODO debug flags ***************************************#\n",
    "\n",
    "import pandas\n",
    "pandas.options.display.float_format = '{:,.4f}'.format\n",
    "pandas.set_option('expand_frame_repr', False)\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Saturate the action so that it lies in [-1, 1]\n",
    "SATURATE = True\n",
    "\n",
    "# Use the true physical parameters in the GP model\n",
    "USE_TRUE_PARAMETERS = False\n",
    "\n",
    "# Use the linearized discrete-time model as the true underlying dynamics\n",
    "USE_LINEAR_DYNAMICS = False\n",
    "\n",
    "# Use a threshold of zero when checking for stability\n",
    "USE_ZERO_THRESHOLD = False\n",
    "\n",
    "#\n",
    "USE_LINEAR_KERNELS = False\n",
    "\n",
    "#\n",
    "USE_LIPSCHITZ_SCALING = True\n",
    "\n",
    "# Scaling factor for GP confidence intervals\n",
    "BETA = 2.\n",
    "\n",
    "#\n",
    "TRAIN_HYPERPARAMETERS = False\n",
    "\n",
    "#\n",
    "GP_SCALING = 1.\n",
    "\n",
    "#\n",
    "NOISE_VAR = 0.001 ** 2\n",
    "\n",
    "#******************************************************#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_discretization(center, tau, N):\n",
    "    offset = (1 - 1 / N) * tau / 2\n",
    "    lower = center - offset\n",
    "    upper = center + offset\n",
    "    if len(N) > 1:\n",
    "        discrete_points = [np.linspace(low, up, n, dtype=config.np_dtype)\n",
    "                           for low, up, n in zip(lower, upper, N)]\n",
    "    else:\n",
    "        discrete_points = [np.linspace(low, up, N[0], dtype=config.np_dtype)\n",
    "                           for low, up in zip(lower, upper)]\n",
    "    mesh = np.meshgrid(*discrete_points, indexing='ij')\n",
    "    points = np.column_stack(col.ravel() for col in mesh)\n",
    "    local_points = points.astype(config.np_dtype)\n",
    "    return local_points\n",
    "\n",
    "\n",
    "class MultiResolutionGrid(safe_learning.GridWorld):\n",
    "    \"\"\" \"\"\"\n",
    "    def __init__(self, limits, num_points, N=None):\n",
    "        \"\"\" \"\"\"\n",
    "        super(MultiResolutionGrid, self).__init__(limits, num_points)\n",
    "        if N is not None:\n",
    "            self._N = N\n",
    "        else:\n",
    "            self._N = np.ones(nindex, dtype=np.int)\n",
    "        self._refined_points = None\n",
    "        self._refined_nindex = np.sum(self._N)\n",
    "    \n",
    "    @property\n",
    "    def refined_points(self):\n",
    "        \"\"\" \"\"\"\n",
    "        if self._refined_points is None:\n",
    "            self.update_resolution(self._N)\n",
    "        return self._refined_points\n",
    "    \n",
    "    def update_resolution(self, N):\n",
    "        \"\"\" \"\"\"\n",
    "        dim = int(self.discretization.ndim)\n",
    "        tau = self.discretization.unit_maxes\n",
    "\n",
    "        # TODO different N values per state?\n",
    "        gridify = lambda point: local_discretization(point, tau, N)\n",
    "        local_grids = np.apply_along_axis(gridify, 1, self._all_points)\n",
    "\n",
    "        self._N = N\n",
    "        self._refined_points = local_grids.reshape((-1, dim))\n",
    "        self._refined_nindex = np.sum(self._N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dt = 0.01   # sampling time\n",
    "g = 9.81    # gravity\n",
    "\n",
    "# True system parameters\n",
    "m = 0.15    # pendulum mass\n",
    "L = 0.5     # pole length\n",
    "b = 0.1     # rotational friction\n",
    "\n",
    "# State and action normalizers\n",
    "theta_max = np.deg2rad(30)\n",
    "omega_max = np.sqrt(g / L)\n",
    "u_max = g * m * L * np.sin(theta_max)\n",
    "\n",
    "state_norm = (theta_max, omega_max)\n",
    "action_norm = (u_max, )\n",
    "\n",
    "# Constraints for initial 'safe' states\n",
    "theta_safe = np.deg2rad(8)\n",
    "omega_safe = 0.5*np.sqrt(g / L)\n",
    "\n",
    "# Dimensions and domains\n",
    "state_dim = 2\n",
    "action_dim = 1\n",
    "state_limits = np.array([[-1., 1.]]*state_dim)\n",
    "action_limits = np.array([[-1., 1.]]*action_dim)\n",
    "\n",
    "print(np.rad2deg(state_norm))\n",
    "\n",
    "# True system\n",
    "true_pendulum = InvertedPendulum(m, L, b, dt, [state_norm, action_norm])\n",
    "A_true, B_true = true_pendulum.linearize()\n",
    "\n",
    "if USE_LINEAR_DYNAMICS:\n",
    "    true_dynamics = safe_learning.functions.LinearSystem((A_true, B_true), name='true_dynamics')\n",
    "else:\n",
    "    true_dynamics = true_pendulum.__call__\n",
    "\n",
    "# \"Wrong\" system\n",
    "m = 0.1     # pendulum mass\n",
    "L = 0.4     # pole length\n",
    "b = 0.0     # rotational friction\n",
    "pendulum = InvertedPendulum(m, L, b, dt, [state_norm, action_norm])\n",
    "A, B = pendulum.linearize()\n",
    "\n",
    "if USE_TRUE_PARAMETERS:\n",
    "    A = A_true\n",
    "    B = B_true\n",
    "mean_dynamics = safe_learning.LinearSystem((A, B), name='mean_dynamics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_true = np.hstack((A_true, B_true))\n",
    "m = np.hstack((A, B))\n",
    "variances = (m_true - m) ** 2\n",
    "\n",
    "# Make sure at least some non-zero prior variance is maintained\n",
    "np.clip(variances, 1e-3, None, out=variances)\n",
    "\n",
    "# Measurement noise\n",
    "noise_var = NOISE_VAR\n",
    "\n",
    "# Input to GP is of the form (x,u)\n",
    "full_dim = state_dim + action_dim\n",
    "\n",
    "# Kernels\n",
    "if USE_LINEAR_KERNELS:\n",
    "    kernel_theta = gpflow.kernels.Linear(full_dim, variance=variances[0, :], ARD=True)\n",
    "\n",
    "    kernel_omega = gpflow.kernels.Linear(full_dim, variance=variances[1, :], ARD=True)\n",
    "\n",
    "else:\n",
    "    kernel_theta = (gpflow.kernels.Linear(full_dim, variance=variances[0, :], ARD=True)\n",
    "                    + gpflow.kernels.Matern32(1, lengthscales=1, active_dims=[0])\n",
    "                    * gpflow.kernels.Linear(1, variance=variances[0, 1]))\n",
    "\n",
    "    kernel_omega = (gpflow.kernels.Linear(full_dim, variance=variances[1, :], ARD=True)\n",
    "                    + gpflow.kernels.Matern32(1, lengthscales=1, active_dims=[0])\n",
    "                    * gpflow.kernels.Linear(1, variance=variances[1, 1]))\n",
    "\n",
    "# Mean dynamics\n",
    "mean_function_theta = safe_learning.LinearSystem((A[[0], :], B[[0], :]), name='mean_dynamics_theta')\n",
    "mean_function_omega = safe_learning.LinearSystem((A[[1], :], B[[1], :]), name='mean_dynamics_omega')\n",
    "\n",
    "# Define a GP model over the dynamics\n",
    "gp_theta = safe_learning.GPRCached(np.empty((0, full_dim), dtype=np_dtype),\n",
    "                                   np.empty((0, 1), dtype=np_dtype),\n",
    "                                   kernel_theta,\n",
    "                                   mean_function_theta)\n",
    "\n",
    "gp_omega = safe_learning.GPRCached(np.empty((0, full_dim), dtype=np_dtype),\n",
    "                                   np.empty((0, 1), dtype=np_dtype),\n",
    "                                   kernel_omega,\n",
    "                                   mean_function_omega)\n",
    "\n",
    "gp_theta.likelihood.variance = noise_var\n",
    "gp_omega.likelihood.variance = noise_var\n",
    "\n",
    "#\n",
    "if TRAIN_HYPERPARAMETERS:\n",
    "    # Sample state-actions (x, u) and observations from the dynamics\n",
    "    N = 1e2\n",
    "    states = sample_box(state_limits, N)\n",
    "    actions = sample_box(action_limits, N)\n",
    "    X = np.concatenate((states, actions), axis=1)\n",
    "    Y = true_dynamics(states, actions).eval()\n",
    "\n",
    "    for i, gp in enumerate((gp_theta, gp_omega)):\n",
    "\n",
    "        # \n",
    "        print('Original parameters:', gp, '\\n')\n",
    "\n",
    "        # Optimize the parameters of each GP via MLE\n",
    "        print('Optimizing ...\\n')\n",
    "        gp.X = X\n",
    "        gp.Y = Y[:, i].reshape((-1, 1))\n",
    "        gp.optimize()\n",
    "\n",
    "        # Reset GP data matrices for safe learning\n",
    "        gp.X = np.empty((0, full_dim), dtype=np_dtype)\n",
    "        gp.Y = np.empty((0, 1), dtype=np_dtype)\n",
    "        gp.update_cache()\n",
    "\n",
    "        # \n",
    "        print('New parameters:', gp, '\\n')\n",
    "\n",
    "#\n",
    "gp_theta_fun = safe_learning.GaussianProcess(gp_theta, BETA)\n",
    "gp_omega_fun = safe_learning.GaussianProcess(gp_omega, BETA)\n",
    "\n",
    "# Stack GP functions => block-diagonal kernel matrix\n",
    "dynamics = safe_learning.FunctionStack((gp_theta_fun, gp_omega_fun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of states along each dimension\n",
    "num_states = [201, 201]\n",
    "# num_states = [2001, 2001]\n",
    "\n",
    "# State grid\n",
    "grid_limits = np.array([[-1., 1.], [-1., 1.]])\n",
    "state_discretization = safe_learning.GridWorld(grid_limits, num_states)\n",
    "\n",
    "# Discretization constant\n",
    "if USE_ZERO_THRESHOLD:\n",
    "    tau = 0.0\n",
    "else:\n",
    "    tau = np.sum(state_discretization.unit_maxes) / 2\n",
    "\n",
    "print('Grid size: {}'.format(state_discretization.nindex))\n",
    "print('Discretization constant: {}'.format(tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State cost matrix\n",
    "Q = np.diag([1., 2.])\n",
    "\n",
    "# Action cost matrix\n",
    "R = 1.2*np.identity(action_dim)\n",
    "\n",
    "# Normalize cost matrices\n",
    "cost_norm = np.amax([Q.max(), R.max()])\n",
    "Q = Q / cost_norm\n",
    "R = R / cost_norm\n",
    "\n",
    "# Quadratic cost function\n",
    "cost_function = safe_learning.QuadraticFunction(block_diag(Q, R), name='cost_function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix policy to the LQR solution for the true system\n",
    "K, P = safe_learning.utilities.dlqr(A_true, B_true, Q, R)\n",
    "policy = safe_learning.LinearSystem(-K, name='policy')\n",
    "\n",
    "if SATURATE:\n",
    "    policy = safe_learning.Saturation(policy, -1, 1)\n",
    "    \n",
    "# TensorFlow variables\n",
    "tf_states = tf.placeholder(tf_dtype, shape=[None, state_dim], name='states')\n",
    "tf_actions = policy(tf_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyapunov Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Lyapunov function corresponding to the known policy\n",
    "lyapunov_function = safe_learning.QuadraticFunction(P)\n",
    "grad_lyapunov_function = safe_learning.LinearSystem((2*P,))\n",
    "\n",
    "# Lipschitz constants\n",
    "L_pol = lambda s: tf.constant(np.linalg.norm(-K, 1), dtype=tf_dtype)\n",
    "L_dyn = lambda s: np.linalg.norm(A_true, 1) + np.linalg.norm(B_true, 1)*L_pol(s)\n",
    "\n",
    "if USE_LIPSCHITZ_SCALING:\n",
    "    L_v = lambda s: tf.abs(grad_lyapunov_function(s))\n",
    "else:\n",
    "    L_v = lambda s: tf.norm(grad_lyapunov_function(s), ord=1, axis=1, keep_dims=True)\n",
    "    \n",
    "# Set initial safe set as a level set of the Lyapunov function\n",
    "values = session.run(lyapunov_function(tf_states), {tf_states: state_discretization.all_points})\n",
    "cutoff = 1e-2 * np.max(values)\n",
    "initial_safe_set = np.squeeze(values, axis=1) <= cutoff\n",
    "\n",
    "# Set initial safe set as a hypercube in the state space\n",
    "# safe_norm = np.array([[theta_safe / theta_max, omega_safe / omega_max]])\n",
    "# norm_states = state_discretization.all_points / safe_norm\n",
    "# initial_safe_set = np.all(np.logical_and(norm_states >= -1, norm_states <= 1), axis=1, keepdims=False)\n",
    "\n",
    "# Initialize class\n",
    "lyapunov = safe_learning.Lyapunov(state_discretization, lyapunov_function, dynamics, \n",
    "                                  L_dyn, L_v, tau, policy, initial_safe_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy(lyapunov, tf_states, state_norm=None, show=True):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 5), dpi=100)\n",
    "#     fig.subplots_adjust(wspace=0.4, hspace=0.2)\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    cmap.set_under('indigo')\n",
    "    cmap.set_over('gold')\n",
    "    ticks = np.linspace(-1., 1., 9)\n",
    "    cutoff = 1. - 1e-10\n",
    "    \n",
    "    if state_norm is not None:\n",
    "        theta_max, omega_max = state_norm\n",
    "        scale = np.array([np.rad2deg(theta_max), np.rad2deg(omega_max)]).reshape((-1, 1))\n",
    "        limits = scale * lyapunov.discretization.limits\n",
    "    else:\n",
    "        limits = lyapunov.discretization.limits\n",
    "    \n",
    "    z = session.run(lyapunov.policy(tf_states), feed_dict={tf_states: lyapunov.discretization.all_points})\n",
    "    z = z.reshape(lyapunov.discretization.num_points)\n",
    "    im = ax.imshow(z.T, \n",
    "                   origin='lower', \n",
    "                   extent=limits.ravel(), \n",
    "                   aspect=limits[0, 0] / limits[1, 0],\n",
    "                   cmap=cmap,\n",
    "                   vmin=-cutoff,\n",
    "                   vmax=cutoff)\n",
    "    cbar = fig.colorbar(im, ax=ax, label=r'$u = \\pi(x)$', ticks=ticks)\n",
    "    ax.set_xlabel(r'$\\theta$ [deg]')\n",
    "    ax.set_ylabel(r'$\\omega$ [deg/s]')\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "# Visualize policy\n",
    "plot_policy(lyapunov, tf_states, state_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Safe Set Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare safe set before and after checking the decrease condition for the first time\n",
    "c_max = lyapunov.feed_dict[lyapunov.c_max]\n",
    "init_safe_set_size = np.sum(lyapunov.safe_set)\n",
    "\n",
    "print('Before update ...')\n",
    "print('c_max: {}'.format(c_max))\n",
    "print('Safe set size: {}\\n'.format(init_safe_set_size))\n",
    "\n",
    "old_safe_set = np.copy(lyapunov.safe_set)\n",
    "lyapunov.update_safe_set()\n",
    "\n",
    "c_max = lyapunov.feed_dict[lyapunov.c_max]\n",
    "init_safe_set_size = np.sum(lyapunov.safe_set)\n",
    "\n",
    "print('After update ...')\n",
    "print('c_max: {}'.format(c_max))\n",
    "print('Safe set size: {}'.format(init_safe_set_size))\n",
    "\n",
    "debug(lyapunov, true_dynamics, state_norm, plot='pendulum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug(lyapunov, true_dynamics, state_norm, do_print=True, newly_safe_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Learning and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_variation = np.array([-0.01, -0.001, 0.0, 0.001, 0.01], dtype=np_dtype).reshape((-1, 1))\n",
    "action_variation = np.array([[0.]], dtype=np_dtype)\n",
    "\n",
    "with tf.name_scope('add_new_measurement'):\n",
    "    full_dim = state_dim + action_dim \n",
    "    tf_max_state_action = tf.placeholder(tf_dtype, shape=[1, full_dim])\n",
    "    tf_measurement = true_dynamics(tf_max_state_action)\n",
    "    \n",
    "def update_gp():\n",
    "    \"\"\"Update the GP model based on an actively selected data point.\"\"\"\n",
    "    \n",
    "    # Get a new sample location\n",
    "    max_state_action, _ = safe_learning.get_safe_sample(lyapunov,\n",
    "                                                        action_variation,\n",
    "                                                        action_limits,\n",
    "                                                        positive=False,\n",
    "                                                        num_samples=1000)\n",
    "    \n",
    "    # Obtain a measurement of the true dynamics\n",
    "    lyapunov.feed_dict[tf_max_state_action] = max_state_action\n",
    "    measurement = tf_measurement.eval(feed_dict=lyapunov.feed_dict)\n",
    "    \n",
    "    # Add the measurement to our GP dynamics\n",
    "    lyapunov.dynamics.add_data_point(max_state_action, measurement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_per_update = 10\n",
    "safe_set_updates = 3\n",
    "\n",
    "if 'level' not in globals():\n",
    "    level = np.zeros(safe_set_updates + 1)\n",
    "    safe_size = np.zeros(safe_set_updates + 1)\n",
    "    level[0] = lyapunov.feed_dict[lyapunov.c_max]\n",
    "    safe_size[0] = np.sum(lyapunov.safe_set)\n",
    "    num_data = data_per_update*np.arange(safe_set_updates + 1)\n",
    "    e = 0\n",
    "else:\n",
    "    e = len(level) - 1\n",
    "    level = np.concatenate((level, np.zeros(safe_set_updates)))\n",
    "    safe_size = np.concatenate((safe_size, np.zeros(safe_set_updates)))\n",
    "    temp = data_per_update*np.arange(1, safe_set_updates + 1) + num_data[-1]\n",
    "    num_data = np.concatenate((num_data, temp))\n",
    "\n",
    "start = time.time()\n",
    "    \n",
    "for i in range(safe_set_updates):\n",
    "    \n",
    "    print('Iteration {} with c_max: {}'.format(e + i + 1, lyapunov.feed_dict[lyapunov.c_max]))\n",
    "    old_safe_set = np.copy(lyapunov.safe_set)\n",
    "\n",
    "#     for _ in tqdm(range(data_per_update)):\n",
    "    for _ in range(data_per_update):    \n",
    "        update_gp()\n",
    "\n",
    "    lyapunov.update_safe_set(batch_size=1000, adaptive=True, Nmax=8)\n",
    "#     lyapunov.update_safe_set(batch_size=10000, adaptive=False)\n",
    "    level[e + i + 1] = lyapunov.feed_dict[lyapunov.c_max]\n",
    "    safe_size[e + i + 1] = np.sum(lyapunov.safe_set)\n",
    "    \n",
    "    current_safe_set_size = np.sum(lyapunov.safe_set)\n",
    "    data = lyapunov.dynamics.functions[0].X\n",
    "    print('Discretization size: {}'.format(state_discretization.all_points.shape[0]))\n",
    "    print('Safe set size: {}'.format(current_safe_set_size))\n",
    "    print('Growth: {}'.format(current_safe_set_size - init_safe_set_size))\n",
    "    print(\"Data points collected: {}\".format(data.shape[0]))\n",
    "    print(\"New c_max: {}\\n\".format(lyapunov.feed_dict[lyapunov.c_max]))\n",
    "\n",
    "end = time.time()\n",
    "print('Duration:', end - start)\n",
    "\n",
    "debug(lyapunov, true_dynamics, state_norm, plot='pendulum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmax(num_data > 200)\n",
    "idx = 0\n",
    "if idx == 0:\n",
    "    idx = len(num_data)\n",
    "print(level, '\\n')\n",
    "print(safe_size)\n",
    "    \n",
    "fig, ax = plt.subplots(1, 2, sharex=False, figsize=(10, 3), dpi=200)\n",
    "fig.subplots_adjust(wspace=0.3, hspace=0.2)\n",
    "\n",
    "ax[0].step(num_data[:idx], level[:idx], 'o--', where='post')\n",
    "ax[0].set_xlabel(r'Number of data points collected', fontsize=12)\n",
    "ax[0].set_ylabel(r'$c_{max}$', fontsize=12)\n",
    "\n",
    "ax[1].step(num_data[:idx], safe_size[:idx], 'o--', where='post')\n",
    "ax[1].set_xlabel(r'Number of data points collected', fontsize=12)\n",
    "ax[1].set_ylabel(r'Safe set size', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accepted \"safe\" state-action pairs\n",
    "# data = lyapunov.dynamics.functions[0].X\n",
    "# print(\"Data points collected: {}\\n\".format(data.shape[0]))\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gp_theta.predict_f(data[0,:].reshape(1, -1)))\n",
    "# print(gp_theta.likelihood.variance.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug(lyapunov, true_dynamics, state_norm, do_print=True, newly_safe_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lyapunov._N.shape)\n",
    "print(lyapunov._N.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
