{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning a Lyapunov Function for an Inverted Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "import safe_learning\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas\n",
    "import mosek\n",
    "import cvxpy as cvx\n",
    "import os\n",
    "\n",
    "from scipy.linalg import block_diag\n",
    "from utilities import InvertedPendulum, debug, LyapunovNetwork\n",
    "from safe_learning.utilities import get_storage, set_storage\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.client import timeline\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# TODO testing ****************************************#\n",
    "class Options(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Options, self).__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "OPTIONS = Options(np_dtype               = safe_learning.config.np_dtype,\n",
    "                  tf_dtype               = safe_learning.config.dtype,\n",
    "                  saturate               = True,\n",
    "                  eps                    = 1e-8,\n",
    "                  use_linear_dynamics    = False,\n",
    "                  dpi                    = 150,\n",
    "                  fontproperties         = FontProperties(size=10),\n",
    "                  save_figs              = False,\n",
    "                  use_bad_lyapunov_start = True,\n",
    "                  use_zero_threshold     = True,\n",
    "                  fig_path               = 'figures/pendulum_lyapunov/')\n",
    "#******************************************************#\n",
    "    \n",
    "NP_DTYPE = safe_learning.config.np_dtype\n",
    "TF_DTYPE = safe_learning.config.dtype\n",
    "_STORAGE = {}\n",
    "EPS = 1e-8\n",
    "\n",
    "HEAT_MAP = plt.get_cmap('inferno', lut=None)\n",
    "HEAT_MAP.set_over('white')\n",
    "HEAT_MAP.set_under('black')\n",
    "\n",
    "LEVEL_MAP = plt.get_cmap('viridis', lut=21)\n",
    "LEVEL_MAP.set_over('gold')\n",
    "LEVEL_MAP.set_under('white')\n",
    "\n",
    "BINARY_MAP = ListedColormap([(1., 1., 1., 0.), (0., 1., 0., 1)])\n",
    "\n",
    "def binary_cmap(color='red', alpha=1.):\n",
    "    if color=='red':\n",
    "        color_code = (1., 0., 0., alpha)\n",
    "    elif color=='green':\n",
    "        color_code = (0., 1., 0., alpha)\n",
    "    elif color=='blue':\n",
    "        color_code = (0., 0., 1., alpha)\n",
    "    else:\n",
    "        color_code = color\n",
    "    transparent_code = (1., 1., 1., 0.)\n",
    "    return ListedColormap([transparent_code, color_code])\n",
    "\n",
    "pandas.options.display.float_format = '{:,.4f}'.format\n",
    "pandas.set_option('expand_frame_repr', False)\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "plt.rc('font', size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CPU_COUNT = os.cpu_count()\n",
    "NUM_CORES = 8\n",
    "NUM_SOCKETS = 2\n",
    "\n",
    "os.environ[\"KMP_BLOCKTIME\"]    = str(0)\n",
    "os.environ[\"KMP_SETTINGS\"]     = str(1)\n",
    "os.environ[\"KMP_AFFINITY\"]     = 'granularity=fine,noverbose,compact,1,0'\n",
    "os.environ[\"OMP_NUM_THREADS\"]  = str(NUM_CORES)\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads  = NUM_CORES,\n",
    "                        inter_op_parallelism_threads  = NUM_SOCKETS,\n",
    "                        allow_soft_placement          = False,\n",
    "#                         log_device_placement          = True,\n",
    "                        device_count                  = {'CPU': MAX_CPU_COUNT},\n",
    "                       )\n",
    "\n",
    "# TODO manually for CPU-only?\n",
    "config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n",
    "\n",
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "session = tf.InteractiveSession(config=config)\n",
    "\n",
    "# print('Found MAX_CPU_COUNT =', MAX_CPU_COUNT)\n",
    "# for dev in session.list_devices():\n",
    "#     print(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saturate the action so that it lies in [-1, 1]\n",
    "SATURATE = True\n",
    "\n",
    "# Use the true physical parameters in the GP model\n",
    "USE_TRUE_PARAMETERS = False\n",
    "\n",
    "# Use the linearized discrete-time model as the true underlying dynamics\n",
    "USE_LINEAR_DYNAMICS = False\n",
    "\n",
    "#\n",
    "USE_LIPSCHITZ_SCALING = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_function_compose(tf_input, tf_function, num_compositions, output_name='function_composition', **kwargs):\n",
    "    '''Apply a function multiple times to the input.'''\n",
    "    \n",
    "    def body(intermediate, idx):\n",
    "        intermediate = tf_function(intermediate, **kwargs)\n",
    "        idx = idx + 1\n",
    "        return intermediate, idx\n",
    "\n",
    "    def condition(rollout, states, idx):\n",
    "        return idx < num_compositions\n",
    "\n",
    "    initial_idx = tf.constant(0, dtype=TF_DTYPE)\n",
    "    initial_intermediate = tf_input\n",
    "    shape_invariants = [initial_intermediate.get_shape(), initial_idx.get_shape()]\n",
    "    tf_output, _ = tf.while_loop(condition, body, [initial_intermediate, initial_idx], shape_invariants, name=output_name)\n",
    "    \n",
    "    return tf_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dt = 0.01   # sampling time\n",
    "g = 9.81    # gravity\n",
    "\n",
    "# True system parameters\n",
    "m = 0.15    # pendulum mass\n",
    "L = 0.5     # pole length\n",
    "b = 0.1     # rotational friction\n",
    "\n",
    "# State and action normalizers\n",
    "theta_max = np.deg2rad(120)\n",
    "omega_max = np.sqrt(g / L)\n",
    "u_max = g * m * L * np.sin(theta_max)\n",
    "\n",
    "state_norm = (theta_max, omega_max)\n",
    "action_norm = (u_max, )\n",
    "\n",
    "# Constraints for initial 'safe' states\n",
    "theta_safe = 0.2 * theta_max\n",
    "omega_safe = 0.5 * omega_max\n",
    "\n",
    "# Dimensions and domains\n",
    "state_dim = 2\n",
    "action_dim = 1\n",
    "state_limits = np.array([[-1., 1.]]*state_dim)\n",
    "action_limits = np.array([[-1., 1.]]*action_dim)\n",
    "\n",
    "# True system\n",
    "pendulum = InvertedPendulum(m, L, b, dt, [state_norm, action_norm])\n",
    "A, B = pendulum.linearize()\n",
    "\n",
    "if USE_LINEAR_DYNAMICS:\n",
    "    dynamics = safe_learning.functions.LinearSystem((A, B), name='true_dynamics')\n",
    "else:\n",
    "    dynamics = pendulum.__call__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of states along each dimension\n",
    "num_states = 501\n",
    "\n",
    "# State grid\n",
    "grid_limits = np.array([[-1., 1.], [-1., 1.]])\n",
    "state_discretization = safe_learning.GridWorld(grid_limits, num_states)\n",
    "\n",
    "# Discretization constant\n",
    "if OPTIONS.use_zero_threshold:\n",
    "    tau = 0.0\n",
    "else:\n",
    "    tau = np.sum(state_discretization.unit_maxes) / 2\n",
    "\n",
    "print('Grid size: {}'.format(state_discretization.nindex))\n",
    "print('Discretization constant: {}'.format(tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State cost matrix\n",
    "Q = np.diag([0.1, 0.1])\n",
    "\n",
    "# Action cost matrix\n",
    "R = 0.1 * np.identity(action_dim)\n",
    "\n",
    "# Normalize cost matrices\n",
    "cost_norm = np.amax([Q.max(), R.max()])\n",
    "Q = Q / cost_norm\n",
    "R = R / cost_norm\n",
    "\n",
    "# Quadratic cost function\n",
    "cost_function = safe_learning.QuadraticFunction(block_diag(Q, R), name='cost_function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix policy to the LQR solution for the true system\n",
    "K, P = safe_learning.utilities.dlqr(A, B, Q, R)\n",
    "policy = safe_learning.LinearSystem(-K, name='policy')\n",
    "\n",
    "if SATURATE:\n",
    "    policy = safe_learning.Saturation(policy, -1, 1)\n",
    "\n",
    "    \n",
    "def plot_policy(policy, discretization, state_norm=None):\n",
    "    plt.rc('font', size=5)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(3, 2), dpi=300)\n",
    "    ticks = np.linspace(-1., 1., 9)\n",
    "    cutoff = 1. - 1e-10\n",
    "\n",
    "    if state_norm is not None:\n",
    "        theta_max, omega_max = state_norm\n",
    "        scale = np.array([np.rad2deg(theta_max), np.rad2deg(omega_max)]).reshape((-1, 1))\n",
    "        limits = scale * discretization.limits\n",
    "    else:\n",
    "        limits = discretization.limits\n",
    "\n",
    "    z = policy(discretization.all_points).eval()\n",
    "    z = z.reshape(discretization.num_points)\n",
    "    im = ax.imshow(z.T, origin='lower', extent=limits.ravel(), aspect=limits[0, 0] / limits[1, 0], cmap=HEAT_MAP, vmin=-cutoff, vmax=cutoff)\n",
    "    cbar = fig.colorbar(im, ax=ax, label=r'$u = \\pi(x)$', ticks=ticks)\n",
    "    ax.set_xlabel(r'$\\phi$ [deg]')\n",
    "    ax.set_ylabel(r'$\\dot{\\phi}$ [deg/s]')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# Visualize policy\n",
    "# plot_policy(policy, state_discretization, state_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LQR Lyapunov Candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIONS.use_bad_lyapunov_start:\n",
    "    P = np.eye(state_dim)\n",
    "\n",
    "# Define the Lyapunov function corresponding to the known policy\n",
    "lyapunov_function = safe_learning.QuadraticFunction(P)\n",
    "grad_lyapunov_function = safe_learning.LinearSystem((2*P,))\n",
    "\n",
    "# initial_safe_set = np.all(state_discretization.all_points == 0.0, axis=1)\n",
    "values = session.run(lyapunov_function(state_discretization.all_points))\n",
    "cutoff = 6e-2 * np.max(values)\n",
    "initial_safe_set = np.squeeze(values, axis=1) <= cutoff\n",
    "\n",
    "# Scaling\n",
    "lyapunov_function = safe_learning.QuadraticFunction(P / np.max(values))\n",
    "grad_lyapunov_function = safe_learning.LinearSystem((2 * P / np.max(values),))\n",
    "\n",
    "# Lipschitz constants\n",
    "L_pol = lambda s: tf.constant(np.linalg.norm(-K, 1), dtype=TF_DTYPE)\n",
    "L_dyn = lambda s: np.linalg.norm(A, 1) + np.linalg.norm(B, 1)*L_pol(s)\n",
    "\n",
    "if USE_LIPSCHITZ_SCALING:\n",
    "    L_v = lambda s: tf.abs(grad_lyapunov_function(s))\n",
    "else:\n",
    "    L_v = lambda s: tf.norm(grad_lyapunov_function(s), ord=1, axis=1, keep_dims=True)\n",
    "\n",
    "# Initialize class\n",
    "lyapunov_lqr = safe_learning.Lyapunov(state_discretization, lyapunov_function, dynamics, L_dyn, L_v, tau, policy, initial_safe_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare safe set before and after checking the decrease condition for the first time\n",
    "c_max = lyapunov_lqr.feed_dict[lyapunov_lqr.c_max]\n",
    "init_safe_set_size = np.sum(lyapunov_lqr.safe_set)\n",
    "\n",
    "print('Before update ...')\n",
    "print('c_max: {}'.format(c_max))\n",
    "print('Safe set size: {}\\n'.format(init_safe_set_size))\n",
    "debug(lyapunov_lqr, dynamics, state_norm, plot='pendulum')\n",
    "\n",
    "# old_safe_set = np.copy(lyapunov_lqr.safe_set)\n",
    "# lyapunov_lqr.update_safe_set()\n",
    "\n",
    "# c_max = lyapunov_lqr.feed_dict[lyapunov_lqr.c_max]\n",
    "# init_safe_set_size = np.sum(lyapunov_lqr.safe_set)\n",
    "\n",
    "# print('After update ...')\n",
    "# print('c_max: {}'.format(c_max))\n",
    "# print('Safe set size: {}'.format(init_safe_set_size))\n",
    "# debug(lyapunov_lqr, true_dynamics, state_norm, plot='pendulum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyapunov_lqr.update_values()\n",
    "lyapunov_lqr.update_safe_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Lyapunov Candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [64, 64]\n",
    "activations = [tf.tanh, tf.tanh]\n",
    "\n",
    "# leaky = lambda x, name: tf.nn.leaky_relu(x, 0.3, name)\n",
    "# activations = [leaky, leaky]\n",
    "\n",
    "lyapunov_function = LyapunovNetwork(state_dim, layer_dims, activations, OPTIONS.eps)\n",
    "\n",
    "grad_lyapunov_function = lambda s: tf.gradients(lyapunov_function(s), s)[0]\n",
    "if USE_LIPSCHITZ_SCALING:\n",
    "    L_v = lambda s: tf.abs(grad_lyapunov_function(s))\n",
    "else:\n",
    "    L_v = lambda s: tf.norm(grad_lyapunov_function(s), ord=1, axis=1, keepdims=True)\n",
    "\n",
    "# TODO need to use template before variables exist in the graph\n",
    "tf_states = tf.placeholder(TF_DTYPE, shape=[None, state_dim], name='states')\n",
    "temp = lyapunov_function(tf_states)\n",
    "session.run(tf.variables_initializer(lyapunov_function.parameters))\n",
    "\n",
    "lyapunov = safe_learning.Lyapunov(state_discretization, lyapunov_function, dynamics, L_dyn, L_v, tau, policy, initial_safe_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for p in lyapunov_function.parameters:\n",
    "#     count += p.shape[0].value * p.shape[1].value\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = get_storage(_STORAGE)\n",
    "if storage is None:\n",
    "    tf_states = tf.placeholder(TF_DTYPE, shape=[None, state_dim], name='states')\n",
    "    tf_actions = policy(tf_states)\n",
    "    tf_future_states = dynamics(tf_states, tf_actions)\n",
    "    \n",
    "    tf_values_lqr = lyapunov_lqr.lyapunov_function(tf_states)\n",
    "    tf_future_values_lqr = lyapunov_lqr.lyapunov_function(tf_future_states)\n",
    "    tf_dv_lqr = tf_future_values_lqr - tf_values_lqr\n",
    "\n",
    "    tf_values = lyapunov.lyapunov_function(tf_states)\n",
    "    tf_future_values = lyapunov.lyapunov_function(tf_future_states)\n",
    "    tf_dv = tf_future_values - tf_values\n",
    "    \n",
    "    tf_threshold = lyapunov.threshold(tf_states, lyapunov.tau)\n",
    "    tf_negative = tf.squeeze(tf.less(tf_dv, tf_threshold), axis=1)\n",
    "    \n",
    "    storage = [('states', tf_states), \n",
    "               ('future_states', tf_future_states), \n",
    "               ('values_lqr', tf_values_lqr), \n",
    "               ('values', tf_values), \n",
    "               ('future_values_lqr', tf_future_values_lqr), \n",
    "               ('future_values', tf_future_values),\n",
    "               ('dv_lqr', tf_dv_lqr),\n",
    "               ('dv', tf_dv),\n",
    "               ('threshold', tf_threshold), \n",
    "               ('negative', tf_negative)]\n",
    "    set_storage(_STORAGE, storage)\n",
    "else:\n",
    "    (tf_states, tf_future_states, tf_values_lqr, tf_values, tf_future_values_lqr, tf_future_values, \n",
    "     tf_dv_lqr, tf_dv, tf_threshold, tf_negative)  = storage.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Region of Attraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridify(norms, maxes=None, num_points=25):    \n",
    "    norms = np.asarray(norms).ravel()\n",
    "    if maxes is None:\n",
    "        maxes = norms\n",
    "    else:\n",
    "        maxes = np.asarray(maxes).ravel()\n",
    "    limits = np.column_stack((- maxes / norms, maxes / norms))\n",
    "    \n",
    "    if isinstance(num_points, int):\n",
    "        num_points = [num_points, ] * len(norms)\n",
    "    grid = safe_learning.GridWorld(limits, num_points)\n",
    "    return grid\n",
    "\n",
    "\n",
    "def compute_roa(grid, closed_loop_dynamics, horizon=250, tol=1e-3, equilibrium=None, no_traj=True):\n",
    "    if isinstance(grid, np.ndarray):\n",
    "        all_points = grid\n",
    "        nindex = grid.shape[0]\n",
    "        ndim = grid.shape[1]\n",
    "    else:\n",
    "        all_points = grid.all_points\n",
    "        nindex = grid.nindex\n",
    "        ndim = grid.ndim\n",
    "    \n",
    "    # Forward-simulate all trajectories from initial points in the discretization\n",
    "    if no_traj:\n",
    "        end_states = all_points\n",
    "        for t in range(1, horizon):\n",
    "            end_states = closed_loop_dynamics(end_states)\n",
    "    else:\n",
    "        trajectories = np.empty((nindex, ndim, horizon))\n",
    "        trajectories[:, :, 0] = all_points\n",
    "        for t in range(1, horizon):\n",
    "            trajectories[:, :, t] = closed_loop_dynamics(trajectories[:, :, t - 1])\n",
    "        end_states = trajectories[:, :, -1]\n",
    "            \n",
    "    if equilibrium is None:\n",
    "        equilibrium = np.zeros((1, ndim))\n",
    "    \n",
    "    # Compute an approximate ROA as all states that end up \"close\" to 0\n",
    "    dists = np.linalg.norm(end_states - equilibrium, ord=2, axis=1, keepdims=True).ravel()\n",
    "    roa = (dists <= tol)\n",
    "    if no_traj:\n",
    "        return roa, dists\n",
    "    else:\n",
    "        return roa, dists, trajectories\n",
    "\n",
    "\n",
    "norms = np.rad2deg(state_norm)\n",
    "maxes = np.copy(norms)\n",
    "plot_limits = np.column_stack((- maxes, maxes))\n",
    "# N = 501\n",
    "# grid = gridify(norms, maxes, N)\n",
    "grid = lyapunov.discretization\n",
    "\n",
    "closed_loop_dynamics = lambda x: tf_future_states.eval({tf_states: x})\n",
    "horizon = 500\n",
    "tol = 0.1\n",
    "\n",
    "roa, dists, trajectories = compute_roa(grid, closed_loop_dynamics, horizon, tol, no_traj=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5), dpi=OPTIONS.dpi)\n",
    "ax.set_aspect(maxes[0] / maxes[1])\n",
    "ax.set_xlim(plot_limits[0])\n",
    "ax.set_ylim(plot_limits[1])\n",
    "ax.set_xlabel(r'$\\phi$ [deg]', fontproperties=OPTIONS.fontproperties)\n",
    "ax.set_ylabel(r'$\\dot{\\phi}$ [deg/s]', fontproperties=OPTIONS.fontproperties)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontproperties(OPTIONS.fontproperties)\n",
    "    \n",
    "# ROA\n",
    "z = roa.reshape(grid.num_points)\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=maxes[0] / maxes[1], cmap=binary_cmap('green'), vmin=0)\n",
    "\n",
    "# Sub-sample discretization for faster and clearer plotting\n",
    "N_traj = 14\n",
    "skip = int(lyapunov.discretization.num_points[0] / N_traj)\n",
    "sub_idx = np.arange(grid.nindex).reshape(grid.num_points)\n",
    "sub_idx = sub_idx[::skip, ::skip].ravel()\n",
    "sub_trajectories = trajectories[sub_idx, :, :]\n",
    "\n",
    "# Trajectories\n",
    "for n in range(sub_trajectories.shape[0]):\n",
    "    theta = sub_trajectories[n, 0, :] * norms[0]\n",
    "    omega = sub_trajectories[n, 1, :] * norms[1]\n",
    "    ax.plot(theta, omega, 'k--', linewidth=0.6)\n",
    "sub_states = grid.all_points[sub_idx]\n",
    "dx_dt = (tf_future_states.eval({tf_states: sub_states}) - sub_states) / dt\n",
    "dx_dt = dx_dt / np.linalg.norm(dx_dt, ord=2, axis=1, keepdims=True)\n",
    "ax.quiver(sub_states[:, 0] * norms[0], sub_states[:, 1] * norms[1], dx_dt[:, 0], dx_dt[:, 1], \n",
    "          scale=None, pivot='mid', headwidth=4, headlength=8, color='k')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network: Supervised Training with LQR Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('supervised_lyapunov_learning'):\n",
    "    tf_costs = tf.abs(tf_values_lqr - tf_values) / tf.stop_gradient(tf_values_lqr + EPS)\n",
    "    tf_objective = tf.reduce_mean(tf_costs, name='objective')\n",
    "    \n",
    "    tf_learning_rate = tf.placeholder(TF_DTYPE, shape=[], name='learning_rate')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(tf_learning_rate)\n",
    "    lyapunov_update = optimizer.minimize(tf_objective, var_list=lyapunov.lyapunov_function.parameters)\n",
    "\n",
    "session.run(tf.variables_initializer(lyapunov_function.parameters))\n",
    "lyapunov.update_values()\n",
    "lyapunov.update_safe_set()\n",
    "debug(lyapunov, dynamics, state_norm, plot='pendulum')\n",
    "\n",
    "obj = []\n",
    "level_states = lyapunov_lqr.discretization.all_points[lyapunov.initial_safe_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training batch from level set\n",
    "tf_batch_size = tf.placeholder(tf.int32, [], 'batch_size')\n",
    "tf_batch = tf.random_uniform([tf_batch_size, ], 0, level_states.shape[0], dtype=tf.int32, name='batch_sample')\n",
    "\n",
    "# Test set\n",
    "test_size = int(1e3)\n",
    "idx = tf_batch.eval({tf_batch_size: int(1e3)})\n",
    "test_set = level_states[idx, :]\n",
    "    \n",
    "feed_dict = {\n",
    "    tf_states:         level_states,\n",
    "    tf_learning_rate:  1e-3,\n",
    "    tf_batch_size:     int(1e3),\n",
    "}\n",
    "max_iters = 200\n",
    "\n",
    "for i in tqdm(range(max_iters)):\n",
    "    idx = tf_batch.eval(feed_dict)\n",
    "    feed_dict[tf_states] = level_states[idx, :]\n",
    "    session.run(lyapunov_update, feed_dict)\n",
    "\n",
    "    feed_dict[tf_states] = test_set\n",
    "    obj.append(tf_objective.eval(feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 2), dpi=OPTIONS.dpi)\n",
    "ax.set_xlabel(r'iteration')\n",
    "ax.set_ylabel(r'objective')\n",
    "ax.plot(obj, '.-r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, values_lqr, dv, dv_lqr = session.run([tf_values, tf_values_lqr, tf_dv, tf_dv_lqr], {tf_states: grid.all_points})\n",
    "value_max = np.amax([values.max(), values_lqr.max()])\n",
    "value_min = 0.0\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(7, 6), dpi=OPTIONS.dpi)\n",
    "fig.subplots_adjust(wspace=0.3, hspace=0.2)\n",
    "for ax in axes.ravel():\n",
    "    ax.set_xlabel(r'$\\phi$ [deg]')\n",
    "    ax.set_ylabel(r'$\\dot{\\phi}$ [deg/s]')\n",
    "\n",
    "z = values_lqr.reshape(grid.num_points)\n",
    "ax = axes[0, 0]\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=maxes[0] / maxes[1], cmap=LEVEL_MAP, vmin=value_min, vmax=value_max)  \n",
    "ax.set_title('LQR Lyapunov function')\n",
    "cbar = fig.colorbar(im, ax=ax, label=r'$v(x)$')\n",
    "\n",
    "z = values.reshape(grid.num_points)\n",
    "ax = axes[1, 0]\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=maxes[0] / maxes[1], cmap=LEVEL_MAP, vmin=value_min, vmax=value_max)   \n",
    "ax.set_title('NN Lyapunov function')\n",
    "cbar = fig.colorbar(im, ax=ax, label=r'$v(x)$')\n",
    "\n",
    "z = dv_lqr.reshape(grid.num_points)\n",
    "ax = axes[0, 1]\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=maxes[0] / maxes[1], cmap=HEAT_MAP, vmax=0.0)   \n",
    "ax.set_title('LQR Lyapunov function')\n",
    "cbar = fig.colorbar(im, ax=ax, label=r'$v(f(x)) - v(x)$')\n",
    "\n",
    "z = dv.reshape(grid.num_points)\n",
    "ax = axes[1, 1]\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=maxes[0] / maxes[1], cmap=HEAT_MAP, vmax=0.0)   \n",
    "ax.set_title('NN Lyapunov function')\n",
    "cbar = fig.colorbar(im, ax=ax, label=r'$v(f(x)) - v(x)$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyapunov.update_values()\n",
    "lyapunov.update_safe_set()\n",
    "\n",
    "print('c_max: {}'.format(lyapunov.feed_dict[lyapunov.c_max]))\n",
    "print('safe set size: {}'.format(lyapunov.safe_set.sum()))\n",
    "debug(lyapunov, dynamics, state_norm, plot='pendulum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5), dpi=OPTIONS.dpi)\n",
    "ax.set_aspect(maxes[0] / maxes[1])\n",
    "ax.set_xlim(plot_limits[0])\n",
    "ax.set_ylim(plot_limits[1])\n",
    "ax.set_xlabel(r'$\\phi$ [deg]', fontproperties=OPTIONS.fontproperties)\n",
    "ax.set_ylabel(r'$\\dot{\\phi}$ [deg/s]', fontproperties=OPTIONS.fontproperties)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontproperties(OPTIONS.fontproperties)\n",
    "    \n",
    "# ROA\n",
    "z = roa.reshape(grid.num_points)\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=maxes[0] / maxes[1], cmap=binary_cmap('green'), vmin=0)\n",
    "\n",
    "# Estimated safe level set\n",
    "z = (values_lqr <= lyapunov_lqr.feed_dict[lyapunov_lqr.c_max]).reshape(grid.num_points)\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=maxes[0] / maxes[1], cmap=binary_cmap('red'), vmin=0)   \n",
    "\n",
    "# Sub-sample discretization for faster and clearer plotting\n",
    "N_traj = 14\n",
    "skip = int(lyapunov.discretization.num_points[0] / N_traj)\n",
    "sub_idx = np.arange(grid.nindex).reshape(grid.num_points)\n",
    "sub_idx = sub_idx[::skip, ::skip].ravel()\n",
    "sub_trajectories = trajectories[sub_idx, :, :]\n",
    "\n",
    "# Trajectories\n",
    "for n in range(sub_trajectories.shape[0]):\n",
    "    theta = sub_trajectories[n, 0, :] * norms[0]\n",
    "    omega = sub_trajectories[n, 1, :] * norms[1]\n",
    "    ax.plot(theta, omega, 'k--', linewidth=0.6)\n",
    "sub_states = grid.all_points[sub_idx]\n",
    "dx_dt = (tf_future_states.eval({tf_states: sub_states}) - sub_states) / dt\n",
    "dx_dt = dx_dt / np.linalg.norm(dx_dt, ord=2, axis=1, keepdims=True)\n",
    "ax.quiver(sub_states[:, 0] * norms[0], sub_states[:, 1] * norms[1], dx_dt[:, 0], dx_dt[:, 1], \n",
    "          scale=None, pivot='mid', headwidth=4, headlength=8, color='k')\n",
    "\n",
    "proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in [(0., 1., 0., 1), (1., 0., 0., 1)]]    \n",
    "legend = ax.legend(proxy, [r'$\\mathcal{R}$', r'$\\mathcal{V}\\!\\ (c_0)$'], prop=OPTIONS.fontproperties, loc='upper right')\n",
    "legend.get_frame().set_alpha(1.)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "if OPTIONS.save_figs:\n",
    "    if OPTIONS.use_bad_lyapunov_start:\n",
    "        save_path = OPTIONS.fig_path + 'pendulum_initial_roa_badstart.pdf'\n",
    "    else:\n",
    "        save_path = OPTIONS.fig_path + 'pendulum_initial_roa.pdf'\n",
    "    fig.savefig(save_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_confusion_weights(y, y_true, scale_by_total=True):\n",
    "    y = y.astype(np.bool)\n",
    "    y_true = y_true.astype(np.bool)\n",
    "    \n",
    "    # Assuming labels in {0, 1}, count entries from confusion matrix\n",
    "    TP = ( y &  y_true).sum()\n",
    "    TN = (~y & ~y_true).sum()\n",
    "    FP = ( y & ~y_true).sum()\n",
    "    FN = (~y &  y_true).sum()\n",
    "    confusion_counts = np.array([[TN, FN], [FP, TP]])\n",
    "    \n",
    "    # Scale up each sample by inverse of confusion weight\n",
    "    weights = np.ones_like(y, dtype=float)\n",
    "    weights[ y &  y_true] /= TP\n",
    "    weights[~y & ~y_true] /= TN\n",
    "    weights[ y & ~y_true] /= FP\n",
    "    weights[~y &  y_true] /= FN\n",
    "    if scale_by_total:\n",
    "        weights *= y.size\n",
    "    \n",
    "    return weights, confusion_counts\n",
    "\n",
    "\n",
    "def balanced_class_weights(y_true, scale_by_total=True):\n",
    "    y = y_true.astype(np.bool)\n",
    "    nP = y.sum()\n",
    "    nN = y.size - y.sum()\n",
    "    class_counts = np.array([nN, nP])\n",
    "    \n",
    "    weights = np.ones_like(y, dtype=float)\n",
    "    weights[ y] /= nP\n",
    "    weights[~y] /= nN\n",
    "    if scale_by_total:\n",
    "        weights *= y.size\n",
    "    \n",
    "    return weights, class_counts\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint for neural net weights\n",
    "saver = tf.train.Saver(var_list=lyapunov.lyapunov_function.parameters)\n",
    "ckpt_path = saver.save(session, \"/tmp/spencerr_pendulum_lyapunov.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('roa_classification'):\n",
    "    # Current maximum level set we want to push the ROA in to\n",
    "    tf_level_multiplier = tf.placeholder(TF_DTYPE, shape=[], name='level_multiplier')\n",
    "    tf_c_max = tf.placeholder(TF_DTYPE, shape=[], name='c_max')\n",
    "    \n",
    "    # True class labels, converted from Boolean ROA labels {0, 1} to {-1, 1}\n",
    "    tf_weights = tf.placeholder(TF_DTYPE, shape=[None, 1], name='class_weights')\n",
    "    tf_roa = tf.placeholder(TF_DTYPE, shape=[None, 1], name='labels')\n",
    "    tf_labels = 2 * tf_roa - 1\n",
    "\n",
    "    # Construct classifier with output (-1, 1)\n",
    "#     tf_classifier_output = tf.tanh(100 * (tf_c_max - tf_values))\n",
    "#     tf_classifier_output = (tf_c_max - tf_values) / (tf.abs(tf_c_max - tf_values) + OPTIONS.eps)\n",
    "    tf_classifier_output = tf_c_max - tf_values\n",
    "    \n",
    "    # Use hinge or perceptron loss for the classification performance\n",
    "    tf_classifier_loss = tf_weights * tf.maximum(- tf_labels * tf_classifier_output, 0, name='hinge_loss')\n",
    "#     tf_classifier_loss = tf_weights * tf.log(1 + tf.exp(- tf_labels * tf_classifier_output))\n",
    "    \n",
    "        \n",
    "    # Enforce decrease constraint with Lagrangian relaxation\n",
    "    tf_lagrange_multiplier = tf.placeholder(TF_DTYPE, shape=[], name='lagrange_multiplier')\n",
    "    tf_decrease_loss = tf_roa * tf.maximum((tf_dv - tf_threshold) / tf.stop_gradient(tf_values + OPTIONS.eps), 0)\n",
    "#     tf_decrease_loss = tf_roa * tf.log(1 + tf.exp(- tf_dv / tf.stop_gradient(tf_values + OPTIONS.eps)))\n",
    "    \n",
    "    # Construct objective and optimizer\n",
    "    tf_objective = tf.reduce_mean(tf_classifier_loss + tf_lagrange_multiplier * tf_decrease_loss, name='objective')\n",
    "    tf_learning_rate = tf.placeholder(TF_DTYPE, shape=[], name='learning_rate')\n",
    "    tf_epsilon = tf.placeholder(TF_DTYPE, shape=[], name='adam_epsilon')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(tf_learning_rate)\n",
    "#     optimizer = tf.train.AdamOptimizer(tf_learning_rate, epsilon=tf_epsilon)\n",
    "    training_update = optimizer.minimize(tf_objective, var_list=lyapunov.lyapunov_function.parameters)\n",
    "    \n",
    "\n",
    "with tf.name_scope('sampling'):\n",
    "    tf_batch_size = tf.placeholder(tf.int32, [], 'batch_size')\n",
    "    tf_idx_range = tf.placeholder(tf.int32, shape=[], name='indices_to_sample')\n",
    "    tf_idx_batch = tf.random_uniform([tf_batch_size, ], 0, tf_idx_range, dtype=tf.int32, name='batch_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore checkpoint\n",
    "saver.restore(session, ckpt_path)\n",
    "lyapunov.update_values()\n",
    "lyapunov.update_safe_set()\n",
    "session.run(tf.variables_initializer(optimizer.variables()))  # TODO\n",
    "\n",
    "obj          = []\n",
    "loss_class   = []\n",
    "loss_dec     = []\n",
    "roa_estimate = np.copy(lyapunov.safe_set)\n",
    "\n",
    "c_max = [lyapunov.feed_dict[lyapunov.c_max], ]\n",
    "safe_size = [lyapunov.safe_set.sum() / lyapunov.discretization.nindex, ]\n",
    "iters_to_converge = []\n",
    "grid = lyapunov.discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_iters = 3\n",
    "inner_iters = 15\n",
    "tol         = 1e-8\n",
    "horizon     = 50\n",
    "batch_size  = int(1e2)\n",
    "test_size   = int(1e4)\n",
    "\n",
    "feed_dict = {\n",
    "    tf_states:               np.zeros((1, lyapunov.discretization.ndim)), # placeholder\n",
    "    tf_batch_size:           batch_size,\n",
    "#     tf_c_max:                lyapunov.feed_dict[lyapunov.c_max],\n",
    "    tf_c_max:                1.,\n",
    "    tf_lagrange_multiplier:  1000,\n",
    "    tf_idx_range:            1,\n",
    "    #\n",
    "    tf_learning_rate:        1e-2,\n",
    "    tf_epsilon:              1e-1,\n",
    "    tf_level_multiplier:     3.,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Current metrics ...')\n",
    "c = lyapunov.feed_dict[lyapunov.c_max]\n",
    "num_safe = lyapunov.safe_set.sum()\n",
    "print('c_max: {}'.format(c))\n",
    "print('grid size: {}'.format(grid.nindex))\n",
    "print('safe set size: {} ({:.2f}% of grid, {:.2f}% of ROA)\\n'.format(int(num_safe), 100 * num_safe / grid.nindex, 100 * num_safe / roa.sum()))\n",
    "print('')\n",
    "time.sleep(0.5)\n",
    "\n",
    "for i in range(outer_iters):\n",
    "    \n",
    "    c = lyapunov.feed_dict[lyapunov.c_max]\n",
    "#     feed_dict[tf_c_max] = c\n",
    "#     time.sleep(0.5)\n",
    "    \n",
    "    # Get states inside V(a * c_max), a > 1    \n",
    "    idx_small = lyapunov.values.ravel() <= c\n",
    "    idx_big = lyapunov.values.ravel() <= feed_dict[tf_level_multiplier] * c\n",
    "    idx_gap = np.logical_and(idx_big, ~idx_small)\n",
    "    \n",
    "    #\n",
    "    V_gap = grid.all_points[idx_gap]\n",
    "    V_future = np.copy(V_gap)\n",
    "    for _ in range(horizon):\n",
    "        V_future = tf_future_states.eval({tf_states: V_future})\n",
    "    V_future = tf_values.eval({tf_states: V_future})\n",
    "    safe_in_future = (V_future <= c).ravel()\n",
    "    \n",
    "    roa_estimate[idx_gap] |= safe_in_future\n",
    "    \n",
    "    target_idx = np.logical_or(idx_big, roa_estimate)\n",
    "    target_set = grid.all_points[target_idx]\n",
    "    target_labels = roa_estimate[target_idx].astype(OPTIONS.np_dtype).reshape([-1, 1])\n",
    "#     target_set = grid.all_points\n",
    "#     target_labels = roa_estimate.astype(OPTIONS.np_dtype).reshape([-1, 1])\n",
    "    feed_dict[tf_idx_range] = target_set.shape[0]\n",
    "    \n",
    "    # Test set\n",
    "#     feed_dict[tf_batch_size] = test_size\n",
    "#     idx_test = tf_idx_batch.eval(feed_dict)\n",
    "#     test_set = target_set[idx_test]\n",
    "#     test_labels = target_labels[idx_test]\n",
    "\n",
    "    test_set = grid.all_points\n",
    "    test_labels = roa.reshape([-1, 1])\n",
    "    \n",
    "    test_set = target_set\n",
    "    test_labels = target_labels\n",
    "    \n",
    "    # SGD for classification\n",
    "    converged = False\n",
    "    feed_dict[tf_batch_size] = batch_size\n",
    "\n",
    "    for j in tqdm(range(inner_iters)):\n",
    "        # Training step\n",
    "        idx_batch = tf_idx_batch.eval(feed_dict)\n",
    "        feed_dict[tf_states] = target_set[idx_batch]\n",
    "        feed_dict[tf_roa] = target_labels[idx_batch]\n",
    "#         feed_dict[tf_weights], counts = balanced_confusion_weights(tf_values.eval(feed_dict) <= feed_dict[tf_c_max], feed_dict[tf_roa].astype(bool))\n",
    "        feed_dict[tf_weights], counts = balanced_class_weights(feed_dict[tf_roa].astype(bool))\n",
    "        session.run(training_update, feed_dict=feed_dict)\n",
    "\n",
    "        # Record objectives\n",
    "        feed_dict[tf_states] = test_set\n",
    "        feed_dict[tf_roa] = test_labels\n",
    "#         feed_dict[tf_weights], counts = balanced_confusion_weights(tf_values.eval(feed_dict) <= feed_dict[tf_c_max], feed_dict[tf_roa].astype(bool))\n",
    "        feed_dict[tf_weights], counts = balanced_class_weights(feed_dict[tf_roa].astype(bool))\n",
    "    \n",
    "        results = session.run([tf_classifier_loss, tf_decrease_loss], feed_dict)\n",
    "        loss_class.append(results[0].mean())\n",
    "        loss_dec.append(results[1].mean())\n",
    "        obj.append(loss_class[-1] + feed_dict[tf_lagrange_multiplier] * loss_dec[-1])\n",
    "\n",
    "        if obj[-1] < tol:\n",
    "            converged = True\n",
    "            break\n",
    "\n",
    "    iters_to_converge.append(j + 1)\n",
    "    if converged:\n",
    "        print('Converged in {} iterations!'.format(j + 1))\n",
    "    else:\n",
    "        print('Did not converge!')\n",
    "\n",
    "    print('Updating values ...')\n",
    "    lyapunov.update_values()\n",
    "\n",
    "    print('Updating c_max ...')\n",
    "    lyapunov.update_safe_set()\n",
    "    roa_estimate |= lyapunov.safe_set\n",
    "\n",
    "    c_max.append(lyapunov.feed_dict[lyapunov.c_max])\n",
    "    safe_size.append(lyapunov.safe_set.sum() / grid.nindex)\n",
    "    print('Done!')\n",
    "#     print(class_ratio)\n",
    "    print(counts)\n",
    "    print('c_max: {}'.format(c_max[-1]))\n",
    "    print('grid size: {}'.format(grid.nindex))\n",
    "    print('safe set size: {} ({:.2f}% of grid, {:.2f}% of ROA)\\n'.format(int(safe_size[-1] * grid.nindex), \n",
    "                                                                         100 * safe_size[-1], \n",
    "                                                                         100 * safe_size[-1] * roa.size / roa.sum()))\n",
    "    time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=6)\n",
    "roa_fraction = roa.sum() / roa.size\n",
    "\n",
    "#\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3), dpi=300)\n",
    "ax.plot(loss_class, '.-r')\n",
    "# ax.tick_params('y', colors='r')\n",
    "# ax.set_ylim([None, 0.9])\n",
    "\n",
    "ax.set_xlabel(r'SGD iteration (accumulated)')\n",
    "ax.set_xticks(list(range(0, len(loss_class) + 1, inner_iters)))\n",
    "\n",
    "# ax = ax.twinx()\n",
    "ax.plot(feed_dict[tf_lagrange_multiplier] * np.asarray(loss_dec), '.-b')\n",
    "# ax.tick_params('y', colors='b')\n",
    "# ax.set_ylim([None, 0.0016])\n",
    "\n",
    "ax.set_ylabel(r'Training loss')\n",
    "\n",
    "\n",
    "proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in ['red', 'blue']]    \n",
    "legend = ax.legend(proxy, ['Classification loss', 'Lyapunov decrease loss'], loc='upper right')\n",
    "legend.get_frame().set_alpha(1.)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if OPTIONS.save_figs:\n",
    "    if OPTIONS.use_bad_lyapunov_start:\n",
    "        save_path = OPTIONS.fig_path + 'pendulum_training_loss_badstart.pdf'\n",
    "    else:\n",
    "        save_path = OPTIONS.fig_path + 'pendulum_training_loss.pdf'\n",
    "    fig.savefig(save_path, bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "plt.rc('font', size=6)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3), dpi=300)\n",
    "\n",
    "ax.plot(c_max, '.-r')\n",
    "ax.set_ylabel(r'$c_k$')\n",
    "ax.tick_params('y', colors='r')\n",
    "# ax.set_ylim([0, 1])\n",
    "\n",
    "ax.set_xlabel(r'Safe set update iteration $k$')\n",
    "ax.set_xticks(list(range(0, len(c_max) + 1, 1)))\n",
    "\n",
    "ax = ax.twinx()\n",
    "ax.plot(np.array(safe_size) / roa_fraction, '.-b')\n",
    "ax.set_ylabel(r'$|\\mathcal{V}(c_k) \\cap \\mathcal{X}_\\tau|\\ /\\ |\\mathcal{R} \\cap \\mathcal{X}_\\tau|$')\n",
    "ax.tick_params('y', colors='b')\n",
    "# ax.set_ylim([0, 1])\n",
    "\n",
    "plt.show()\n",
    "    \n",
    "if OPTIONS.save_figs:\n",
    "    if OPTIONS.use_bad_lyapunov_start:\n",
    "        save_path = OPTIONS.fig_path + 'pendulum_training_roafrac_badstart.pdf'\n",
    "    else:\n",
    "        save_path = OPTIONS.fig_path + 'pendulum_training_roafrac.pdf'\n",
    "    fig.savefig(save_path, bbox_inches='tight')\n",
    "\n",
    "\n",
    "print(np.array(safe_size) / roa_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network: Visualization with Phase Portrait and ROA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = lyapunov.discretization\n",
    "values, dv = session.run([tf_values, tf_dv], {tf_states: grid.all_points})\n",
    "value_max = values.max()\n",
    "value_min = 0.0\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6), dpi=OPTIONS.dpi)\n",
    "fig.subplots_adjust(wspace=0.6, hspace=0.2)\n",
    "for ax in axes.ravel():\n",
    "    ax.set_xlabel(r'$\\theta$ [deg]')\n",
    "    ax.set_ylabel(r'$\\omega$ [deg/s]')\n",
    "\n",
    "# Change in v(x)\n",
    "ax   = axes[0]\n",
    "z    = dv.reshape(grid.num_points)\n",
    "im   = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=maxes[0] / maxes[1], cmap=HEAT_MAP, vmax=0.0)        \n",
    "cbar = fig.colorbar(im, ax=ax, label=r'$v(f(x)) - v(x)$')\n",
    "\n",
    "#\n",
    "# temp = (dv < 0).astype(int)\n",
    "# temp[dv >= 0] = -1\n",
    "\n",
    "temp = (dv < 0) & (values <= 1)\n",
    "temp = temp.astype(int)\n",
    "temp[values > 1] = -1\n",
    "temp[dv >= 0] = -1\n",
    "\n",
    "ax = axes[1]\n",
    "\n",
    "z    = values.reshape(grid.num_points) * temp.reshape(grid.num_points)\n",
    "im   = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=maxes[0] / maxes[1], cmap=LEVEL_MAP, vmin=value_min, vmax=value_max) \n",
    "cbar = fig.colorbar(im, ax=ax, label=r'$v(x)$')\n",
    "\n",
    "z   = roa.reshape(lyapunov.discretization.num_points)\n",
    "# im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=maxes[0] / maxes[1], cmap=BINARY_MAP, alpha=0.2)\n",
    "\n",
    "\n",
    "# ROAs\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6), dpi=OPTIONS.dpi)\n",
    "\n",
    "# True ROA\n",
    "z = roa.reshape(lyapunov.discretization.num_points)\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=maxes[0] / maxes[1], cmap=binary_cmap('green'), alpha=1.)\n",
    "\n",
    "# Estimated ROA\n",
    "z = roa_estimate.reshape(lyapunov.discretization.num_points)\n",
    "# im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=maxes[0] / maxes[1], cmap=binary_cmap('yellow'), alpha=1.)\n",
    "\n",
    "# Safe level set\n",
    "z = lyapunov.safe_set.reshape(lyapunov.discretization.num_points)\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=maxes[0] / maxes[1], cmap=binary_cmap('blue'), alpha=1.)\n",
    "\n",
    "# Initial ROA\n",
    "# z = lyapunov.initial_safe_set.reshape(lyapunov.discretization.num_points)\n",
    "z = lyapunov_lqr.safe_set.reshape(lyapunov.discretization.num_points)\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=maxes[0] / maxes[1], cmap=binary_cmap('red'), alpha=1.)\n",
    "\n",
    "# Trajectories\n",
    "for n in range(sub_trajectories.shape[0]):\n",
    "    theta = sub_trajectories[n, 0, :] * norms[0]\n",
    "    omega = sub_trajectories[n, 1, :] * norms[1]\n",
    "    ax.plot(theta, omega, 'k--', linewidth=0.6)\n",
    "sub_states = grid.all_points[sub_idx]\n",
    "dx_dt = (tf_future_states.eval({tf_states: sub_states}) - sub_states) / dt\n",
    "dx_dt = dx_dt / np.linalg.norm(dx_dt, ord=2, axis=1, keepdims=True)\n",
    "ax.quiver(sub_states[:, 0] * norms[0], sub_states[:, 1] * norms[1], \n",
    "          dx_dt[:, 0], dx_dt[:, 1],\n",
    "          scale=None, pivot='mid', headwidth=4, headlength=8, color='k')\n",
    "ax.set_xlim(plot_limits[0])\n",
    "ax.set_ylim(plot_limits[1])\n",
    "\n",
    "proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in [(0,1,0,1), (1,0,0,1), (0,0,1,1)]]    \n",
    "legend = ax.legend(proxy, [r'$\\mathcal{R}$', r'$\\mathcal{V}\\!\\ (c_0)$', r'$\\mathcal{V}\\!\\ (c_k)$'], loc='upper right')\n",
    "legend.get_frame().set_alpha(1.)\n",
    "\n",
    "ax.set_xlabel(r'$\\phi$ [deg]')\n",
    "ax.set_ylabel(r'$\\dot{\\phi}$ [deg/s]')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if OPTIONS.save_figs:\n",
    "    if OPTIONS.use_bad_lyapunov_start:\n",
    "        save_path = OPTIONS.fig_path + 'pendulum_final_roa_badstart.pdf'\n",
    "    else:\n",
    "        save_path = OPTIONS.fig_path + 'pendulum_final_roa.pdf'\n",
    "    fig.savefig(save_path, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = lyapunov.feed_dict[lyapunov.c_max]\n",
    "num_grid = lyapunov.discretization.nindex\n",
    "num_safe = lyapunov.safe_set.sum()\n",
    "num_safe_lqr = lyapunov_lqr.safe_set.sum()\n",
    "\n",
    "print('c_max: {}'.format(c))\n",
    "print('grid size: {}'.format(num_grid))\n",
    "print('ROA size: {:.2f}%'.format(100 * roa.sum() / roa.size))\n",
    "print('lqr safe set size: {} ({:.2f}%)'.format(num_safe_lqr, 100 * num_safe_lqr / num_grid))\n",
    "print('nn safe set size: {} ({:.2f}%)'.format(num_safe, 100 * num_safe / num_grid))\n",
    "\n",
    "# debug(lyapunov, dynamics, state_norm, plot='pendulum')\n",
    "# debug(lyapunov_lqr, dynamics, state_norm, plot='pendulum')\n",
    "\n",
    "print(num_safe_lqr / roa.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lyapunov_lqr.update_safe_set()\n",
    "\n",
    "# safe = lyapunov_lqr.safe_set\n",
    "# # safe = np.logical_xor(lyapunov.safe_set, lyapunov.initial_safe_set)\n",
    "# X = lyapunov_lqr.discretization.all_points[safe]\n",
    "# Y = tf_future_states.eval({tf_states: X})\n",
    "\n",
    "# w = cvx.Variable(state_dim, 1)\n",
    "# c = cvx.Variable(1)\n",
    "\n",
    "# obj = cvx.Maximize(c)\n",
    "# constraints = [(Y - X) * w <= -EPS, X * w <= c]\n",
    "\n",
    "# mosek_params = {mosek.dparam.ana_sol_infeas_tol:     1e-32,  # print if a constraint violates more than this, 1e-6\n",
    "#                 mosek.dparam.intpnt_qo_tol_mu_red:   1e-12,  # relative complementarity gap feasibility tolerance, 1e-8\n",
    "#                 mosek.iparam.ana_sol_print_violated: True,\n",
    "#                }\n",
    "\n",
    "# prob = cvx.Problem(obj, constraints)\n",
    "# result = prob.solve(solver=cvx.MOSEK,\n",
    "#                     verbose=True,\n",
    "#                     warm_start=False,\n",
    "#                     mosek_params=mosek_params)\n",
    "\n",
    "# print(\"\\nStatus:\", prob.status)\n",
    "# print(\"Optimal objective value:\", prob.value)\n",
    "# print(\"Optimal variable value:\\n\", w.value, c.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify shape matrix of suitable Lyapunov function\n",
    "# print(P / lyapunov.feed_dict[lyapunov.c_max])\n",
    "\n",
    "# def phi(X):\n",
    "#     num_samples = X.shape[0]\n",
    "#     Phi = np.hstack((np.ones([num_samples, 1]), X))\n",
    "#     return Phi\n",
    "\n",
    "# def phi(X):\n",
    "#     Phi = X\n",
    "#     return Phi\n",
    "\n",
    "# # CVXPY 1.0\n",
    "# # def quadratic(X, P, convention='cvx'):\n",
    "# #     if convention=='cvx':\n",
    "# #         linear_form = X * P\n",
    "# #         quadratic = cvx.multiply(linear_form, X)\n",
    "# #         result = cvx.sum(quadratic, axis=1, keepdims=True)\n",
    "# #     else:\n",
    "# #         linear_form = np.matmul(X, P)\n",
    "# #         quadratic = linear_form * X\n",
    "# #         result = np.sum(quadratic, axis=1, keepdims=True)\n",
    "# #     return result\n",
    "\n",
    "# # CVXPY 0.4\n",
    "# def quadratic(X, P, convention='cvx'):\n",
    "#     if convention=='cvx':\n",
    "#         linear_form = X * P\n",
    "#         quadratic = cvx.mul_elemwise(X, linear_form)\n",
    "#         result = cvx.sum_entries(quadratic, axis=1)\n",
    "#     else:\n",
    "#         linear_form = np.matmul(X, P)\n",
    "#         quadratic = np.multiply(linear_form, X)\n",
    "#         result = np.sum(np.array(quadratic), axis=1, keepdims=True)\n",
    "#     return result\n",
    "\n",
    "# # Enforce decrease condition, excluding the initial safe set\n",
    "# # safe = lyapunov.safe_set\n",
    "# safe = np.logical_xor(lyapunov.safe_set, lyapunov.initial_safe_set)\n",
    "# X = lyapunov.discretization.all_points[safe, :]\n",
    "\n",
    "# # remove zero-state\n",
    "# idx = ~np.all(X == 0, axis=1)\n",
    "# X_full = X[idx]\n",
    "# F_full = session.run(lyapunov.dynamics(tf_states, tf_actions), {tf_states: X_full})\n",
    "\n",
    "# # Try only considering states near the boundary\n",
    "# V = tf_values.eval({tf_states: X_full}).ravel()\n",
    "# cutoff = 0.*V.max()\n",
    "# X = X_full[V >= cutoff, :]\n",
    "# F = F_full[V >= cutoff, :]\n",
    "\n",
    "# print(X_full.shape)\n",
    "# print(X.shape)\n",
    "\n",
    "# X = phi(X)\n",
    "# F = phi(F)\n",
    "# n = X.shape[1]\n",
    "\n",
    "# # M = cvx.Variable(n, n, PSD=True)  # CVXPY 1.0\n",
    "# M = cvx.Semidef(n)                # CVXPY 0.4\n",
    "\n",
    "# eps_mat = 1e-6\n",
    "# eps_con = 1e-8\n",
    "\n",
    "# obj = cvx.Minimize(cvx.trace(M))\n",
    "# # obj = cvx.Maximize(- cvx.log(cvx.trace(M)))\n",
    "# # obj = cvx.Maximize(- cvx.log_det(M))\n",
    "# # obj = cvx.Minimize(- cvx.log_det(M))\n",
    "# # print(obj.is_dcp())\n",
    "\n",
    "# # constraints = [quadratic(F, M) - quadratic(X, M) <= -eps_con,\n",
    "# #                M >> eps_mat]\n",
    "\n",
    "# constraints = [quadratic(F, M) - quadratic(X, M) <= -eps_con,\n",
    "#                cvx.lambda_min(M) >= eps_mat]\n",
    "\n",
    "# mosek_params = {mosek.dparam.ana_sol_infeas_tol:     1e-32,  # print if a constraint violates more than this, 1e-6\n",
    "#                 mosek.dparam.intpnt_qo_tol_mu_red:   1e-12,  # relative complementarity gap feasibility tolerance, 1e-8\n",
    "#                 mosek.iparam.ana_sol_print_violated: True,\n",
    "#                }\n",
    "\n",
    "# prob = cvx.Problem(obj, constraints)\n",
    "# result = prob.solve(solver=cvx.MOSEK,\n",
    "#                     verbose=True,\n",
    "#                     warm_start=False,\n",
    "#                     mosek_params=mosek_params)\n",
    "\n",
    "# print(\"\\nStatus:\", prob.status)\n",
    "# print(\"Optimal objective value:\", prob.value)\n",
    "# print(\"Optimal variable value:\\n\", M.value)\n",
    "# print('\\nEigenvalues:', np.linalg.eigvals(M.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob.get_problem_data('MOSEK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_full = phi(X_full)\n",
    "# F_full = phi(F_full)\n",
    "\n",
    "# z = quadratic(F_full, M.value, 'np') - quadratic(X_full, M.value, 'np') < 0\n",
    "# idx = ~z.ravel()\n",
    "\n",
    "# dV = quadratic(F_full[idx, :], M.value, 'np') - quadratic(X_full[idx, :], M.value, 'np')\n",
    "\n",
    "# print('Previously safe states, now unsafe:')\n",
    "# print(X_full[idx, :], '\\n')\n",
    "\n",
    "# print('Value change:')\n",
    "# print(dV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scale new cost function to compare with the old one\n",
    "# P_new = P.max() * M.value / M.value.max()\n",
    "# print('Previous cost matrix:\\n{}\\n'.format(P))\n",
    "# print('Computed cost matrix:\\n{}\\n'.format(P_new))\n",
    "# print('Ratio:\\n{}\\n'.format(P_new / P))\n",
    "\n",
    "# lyapunov_function = safe_learning.QuadraticFunction(P_new)\n",
    "# grad_lyapunov_function = safe_learning.LinearSystem((2*P_new,))\n",
    "\n",
    "# if USE_LIPSCHITZ_SCALING:\n",
    "#     L_v = lambda s: tf.abs(grad_lyapunov_function(s))\n",
    "# else:\n",
    "#     L_v = lambda s: tf.norm(grad_lyapunov_function(s), ord=1, axis=1, keep_dims=True)\n",
    "\n",
    "# initial_safe_set = np.all(state_discretization.all_points == 0.0, axis=1)\n",
    "# new_lyapunov = safe_learning.Lyapunov(state_discretization, lyapunov_function, true_dynamics, \n",
    "#                                       L_dyn, L_v, tau, policy, initial_safe_set)\n",
    "# new_lyapunov.update_safe_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug(lyapunov, true_dynamics, state_norm, plot='pendulum')\n",
    "# debug(new_lyapunov, true_dynamics, state_norm, plot='pendulum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
