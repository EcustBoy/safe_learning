{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning a Lyapunov Function for an Inverted Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import safe_learning\n",
    "from utilities import InvertedPendulum, LyapunovNetwork\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TODO testing **********************************************************#\n",
    "\n",
    "class Options(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Options, self).__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "OPTIONS = Options(np_dtype              = safe_learning.config.np_dtype,\n",
    "                  tf_dtype              = safe_learning.config.dtype,\n",
    "                  eps                   = 1e-8,\n",
    "                  saturate              = True,\n",
    "                  use_zero_threshold    = True,\n",
    "                  use_lipschitz_scaling = True,\n",
    "                  pre_train             = True,\n",
    "                  dpi                   = 150,\n",
    "                  fontproperties        = FontProperties(size=10),\n",
    "                  save_figs             = False,\n",
    "                  fig_path              = 'figures/pendulum_lyapunov/')\n",
    "\n",
    "#************************************************************************#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cmap(color='red', alpha=1.):\n",
    "    if color=='red':\n",
    "        color_code = (1., 0., 0., alpha)\n",
    "    elif color=='green':\n",
    "        color_code = (0., 1., 0., alpha)\n",
    "    elif color=='blue':\n",
    "        color_code = (0., 0., 1., alpha)\n",
    "    else:\n",
    "        color_code = color\n",
    "    transparent_code = (1., 1., 1., 0.)\n",
    "    return ListedColormap([transparent_code, color_code])\n",
    "\n",
    "HEAT_MAP = plt.get_cmap('inferno', lut=None)\n",
    "HEAT_MAP.set_over('white')\n",
    "HEAT_MAP.set_under('black')\n",
    "\n",
    "LEVEL_MAP = plt.get_cmap('viridis', lut=21)\n",
    "LEVEL_MAP.set_over('gold')\n",
    "LEVEL_MAP.set_under('white')\n",
    "\n",
    "\n",
    "def confusion_weights(y, y_true, scale_by_total=True):\n",
    "    y = y.astype(np.bool)\n",
    "    y_true = y_true.astype(np.bool)\n",
    "    \n",
    "    # Assuming labels in {0, 1}, count entries from confusion matrix\n",
    "    TP = ( y &  y_true).sum()\n",
    "    TN = (~y & ~y_true).sum()\n",
    "    FP = ( y & ~y_true).sum()\n",
    "    FN = (~y &  y_true).sum()\n",
    "    confusion_counts = np.array([[TN, FN], [FP, TP]])\n",
    "    \n",
    "    # Scale up each sample by inverse of confusion weight\n",
    "    weights = np.ones_like(y, dtype=float)\n",
    "    weights[ y &  y_true] /= TP\n",
    "    weights[~y & ~y_true] /= TN\n",
    "    weights[ y & ~y_true] /= FP\n",
    "    weights[~y &  y_true] /= FN\n",
    "    if scale_by_total:\n",
    "        weights *= y.size\n",
    "    \n",
    "    return weights, confusion_counts\n",
    "\n",
    "\n",
    "def class_weights(y_true, scale_by_total=True):\n",
    "    y = y_true.astype(np.bool)\n",
    "    nP = y.sum()\n",
    "    nN = y.size - y.sum()\n",
    "    class_counts = np.array([nN, nP])\n",
    "    \n",
    "    weights = np.ones_like(y, dtype=float)\n",
    "    weights[ y] /= nP\n",
    "    weights[~y] /= nN\n",
    "    if scale_by_total:\n",
    "        weights *= y.size\n",
    "    \n",
    "    return weights, class_counts\n",
    "\n",
    "\n",
    "def tf_function_compose(tf_input, tf_function, num_compositions, output_name='function_composition', **kwargs):\n",
    "    '''Apply a function multiple times to the input.'''\n",
    "    \n",
    "    def body(intermediate, idx):\n",
    "        intermediate = tf_function(intermediate, **kwargs)\n",
    "        idx = idx + 1\n",
    "        return intermediate, idx\n",
    "\n",
    "    def condition(rollout, states, idx):\n",
    "        return idx < num_compositions\n",
    "\n",
    "    initial_idx = tf.constant(0, dtype=TF_DTYPE)\n",
    "    initial_intermediate = tf_input\n",
    "    shape_invariants = [initial_intermediate.get_shape(), initial_idx.get_shape()]\n",
    "    tf_output, _ = tf.while_loop(condition, body, [initial_intermediate, initial_idx], shape_invariants, name=output_name)\n",
    "    \n",
    "    return tf_output\n",
    "\n",
    "\n",
    "from safe_learning import config, DeterministicFunction\n",
    "from safe_learning.utilities import concatenate_inputs\n",
    "from scipy import signal\n",
    "\n",
    "class ApproxPendulum(DeterministicFunction):\n",
    "    \"\"\".\"\"\"\n",
    "    def __init__(self, mass, length, friction=0, dt=1 / 80, normalization=None):\n",
    "        super(ApproxPendulum, self).__init__()\n",
    "        self.mass = mass\n",
    "        self.length = length\n",
    "        self.gravity = 9.81\n",
    "        self.friction = friction\n",
    "        self.dt = dt\n",
    "\n",
    "        self.normalization = normalization\n",
    "        if normalization is not None:\n",
    "            self.normalization = [np.array(norm, dtype=OPTIONS.np_dtype)\n",
    "                                  for norm in normalization]\n",
    "            self.inv_norm = [norm ** -1 for norm in self.normalization]\n",
    "\n",
    "    @property\n",
    "    def inertia(self):\n",
    "        \"\"\"Return inertia of the pendulum.\"\"\"\n",
    "        return self.mass * self.length ** 2\n",
    "\n",
    "    def normalize(self, state, action):\n",
    "        \"\"\"Normalize states and actions.\"\"\"\n",
    "        if self.normalization is None:\n",
    "            return state, action\n",
    "\n",
    "        Tx_inv, Tu_inv = map(np.diag, self.inv_norm)\n",
    "        state = tf.matmul(state, Tx_inv)\n",
    "\n",
    "        if action is not None:\n",
    "            action = tf.matmul(action, Tu_inv)\n",
    "\n",
    "        return state, action\n",
    "\n",
    "    def denormalize(self, state, action):\n",
    "        \"\"\"De-normalize states and actions.\"\"\"\n",
    "        if self.normalization is None:\n",
    "            return state, action\n",
    "\n",
    "        Tx, Tu = map(np.diag, self.normalization)\n",
    "\n",
    "        state = tf.matmul(state, Tx)\n",
    "        if action is not None:\n",
    "            action = tf.matmul(action, Tu)\n",
    "\n",
    "        return state, action\n",
    "\n",
    "    def linearize(self):\n",
    "        gravity = self.gravity\n",
    "        length = self.length\n",
    "        friction = self.friction\n",
    "        inertia = self.inertia\n",
    "\n",
    "        A = np.array([[0, 1],\n",
    "                      [gravity / length, -friction / inertia]],\n",
    "                     dtype=OPTIONS.np_dtype)\n",
    "\n",
    "        B = np.array([[0],\n",
    "                      [1 / inertia]],\n",
    "                     dtype=OPTIONS.np_dtype)\n",
    "\n",
    "        if self.normalization is not None:\n",
    "            Tx, Tu = map(np.diag, self.normalization)\n",
    "            Tx_inv, Tu_inv = map(np.diag, self.inv_norm)\n",
    "\n",
    "            A = np.linalg.multi_dot((Tx_inv, A, Tx))\n",
    "            B = np.linalg.multi_dot((Tx_inv, B, Tu))\n",
    "\n",
    "        sys = signal.StateSpace(A, B, np.eye(2), np.zeros((2, 1)))\n",
    "        sysd = sys.to_discrete(self.dt)\n",
    "        return sysd.A, sysd.B\n",
    "\n",
    "    @concatenate_inputs(start=1)\n",
    "    def build_evaluation(self, state_action):\n",
    "        \"\"\"Evaluate the dynamics.\"\"\"\n",
    "        # Denormalize\n",
    "        state, action = tf.split(state_action, [2, 1], axis=1)\n",
    "        state, action = self.denormalize(state, action)\n",
    "\n",
    "        n_inner = 10\n",
    "        dt = self.dt / n_inner\n",
    "        for i in range(n_inner):\n",
    "            state_derivative = self.ode(state, action)\n",
    "            state = state + dt * state_derivative\n",
    "\n",
    "        return self.normalize(state, None)[0]\n",
    "\n",
    "    def ode(self, state, action):\n",
    "        # Physical dynamics\n",
    "        gravity = self.gravity\n",
    "        length = self.length\n",
    "        friction = self.friction\n",
    "        inertia = self.inertia\n",
    "\n",
    "        angle, angular_velocity = tf.split(state, 2, axis=1)\n",
    "\n",
    "#         x_ddot = gravity / length * tf.sin(angle) + action / inertia\n",
    "        sinx = angle - tf.pow(angle,3)/6 + tf.pow(angle,5)/120 - tf.pow(angle,7)/5040\n",
    "        x_ddot = gravity / length * sinx + action / inertia\n",
    "\n",
    "        if friction > 0:\n",
    "            x_ddot -= friction / inertia * angular_velocity\n",
    "\n",
    "        state_derivative = tf.concat((angular_velocity, x_ddot), axis=1)\n",
    "\n",
    "        # Normalize\n",
    "        return state_derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CPU_COUNT = os.cpu_count()\n",
    "NUM_CORES = 8\n",
    "NUM_SOCKETS = 2\n",
    "\n",
    "os.environ[\"KMP_BLOCKTIME\"]    = str(0)\n",
    "os.environ[\"KMP_SETTINGS\"]     = str(1)\n",
    "os.environ[\"KMP_AFFINITY\"]     = 'granularity=fine,noverbose,compact,1,0'\n",
    "os.environ[\"OMP_NUM_THREADS\"]  = str(NUM_CORES)\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads  = NUM_CORES,\n",
    "                        inter_op_parallelism_threads  = NUM_SOCKETS,\n",
    "                        allow_soft_placement          = False,\n",
    "#                         log_device_placement          = True,\n",
    "                        device_count                  = {'CPU': MAX_CPU_COUNT},\n",
    "                       )\n",
    "\n",
    "# TODO manually for CPU-only?\n",
    "config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n",
    "\n",
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "session = tf.InteractiveSession(config=config)\n",
    "\n",
    "_STORAGE = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dt = 0.01   # sampling time\n",
    "g  = 9.81    # gravity\n",
    "\n",
    "# True system parameters\n",
    "m = 0.15    # pendulum mass\n",
    "L = 0.5     # pole length\n",
    "b = 0.1     # rotational friction\n",
    "\n",
    "# State and action normalizers\n",
    "theta_max = np.deg2rad(180)\n",
    "omega_max = np.deg2rad(360)\n",
    "u_max     = g * m * L * np.sin(np.deg2rad(60))\n",
    "\n",
    "# Dimensions and domains\n",
    "state_dim     = 2\n",
    "action_dim    = 1\n",
    "state_limits  = np.array([[-1., 1.]] * state_dim)\n",
    "action_limits = np.array([[-1., 1.]] * action_dim)\n",
    "\n",
    "# Dynamics\n",
    "pendulum = InvertedPendulum(m, L, b, dt, [(theta_max, omega_max), (u_max,)])\n",
    "dynamics = pendulum.__call__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Discretization and Initial Safe Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridify(norms, maxes=None, num_points=25):    \n",
    "    norms = np.asarray(norms).ravel()\n",
    "    if maxes is None:\n",
    "        maxes = norms\n",
    "    else:\n",
    "        maxes = np.asarray(maxes).ravel()\n",
    "    limits = np.column_stack((- maxes / norms, maxes / norms))\n",
    "    if isinstance(num_points, int):\n",
    "        num_points = [num_points, ] * len(norms)\n",
    "    grid = safe_learning.GridWorld(limits, num_points)\n",
    "    return grid\n",
    "\n",
    "\n",
    "norms       = [theta_max, omega_max]\n",
    "maxes       = np.copy(norms)\n",
    "grid        = gridify(norms, maxes, 501)\n",
    "plot_limits = np.column_stack((- np.rad2deg(maxes), np.rad2deg(maxes)))\n",
    "\n",
    "if OPTIONS.use_zero_threshold:\n",
    "    tau = 0\n",
    "else:\n",
    "    tau = np.sum(grid.unit_maxes) / 2\n",
    "\n",
    "# Initial safe set (ball)\n",
    "cutoff_radius    = 0.1\n",
    "initial_safe_set = np.linalg.norm(grid.all_points, ord=2, axis=1)  <= cutoff_radius\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State cost matrix\n",
    "Q = np.diag([0.1, 0.1])\n",
    "\n",
    "# Action cost matrix\n",
    "R = 0.1 * np.identity(action_dim)\n",
    "\n",
    "# Normalize cost matrices\n",
    "cost_norm = np.amax([Q.max(), R.max()])\n",
    "Q = Q / cost_norm\n",
    "R = R / cost_norm\n",
    "\n",
    "# Fix policy to the LQR solution for linearized system and some cost matrices\n",
    "A, B = pendulum.linearize()\n",
    "K, P_lqr = safe_learning.utilities.dlqr(A, B, Q, R)\n",
    "\n",
    "# Normalize cost\n",
    "P_lqr /= P_lqr.max()\n",
    "\n",
    "# K[0, 0] = 2.05\n",
    "# K[0, 1] = 0\n",
    "# print(K)\n",
    "policy = safe_learning.LinearSystem(-K, name='policy')\n",
    "\n",
    "if OPTIONS.saturate:\n",
    "    policy = safe_learning.Saturation(policy, -1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Lipschitz Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy (linear)\n",
    "L_pol = lambda x: tf.constant(np.linalg.norm(-K, 1), dtype=OPTIONS.tf_dtype)\n",
    "\n",
    "# Dynamics (linear approximation)\n",
    "L_dyn = lambda x: np.linalg.norm(A, 1) + np.linalg.norm(B, 1) * L_pol(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LQR Lyapunov Candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyapunov_function      = safe_learning.QuadraticFunction(P_lqr)\n",
    "grad_lyapunov_function = safe_learning.LinearSystem((2 * P_lqr,))\n",
    "\n",
    "if OPTIONS.use_lipschitz_scaling:\n",
    "    L_v = lambda x: tf.abs(grad_lyapunov_function(x))\n",
    "else:\n",
    "    L_v = lambda x: tf.norm(grad_lyapunov_function(x), ord=1, axis=1, keepdims=True)\n",
    "\n",
    "# Initialize class\n",
    "lyapunov_lqr = safe_learning.Lyapunov(grid, lyapunov_function, dynamics, L_dyn, L_v, tau, policy, initial_safe_set)\n",
    "lyapunov_lqr.update_values()\n",
    "lyapunov_lqr.update_safe_set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOS Lyapunov Candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_sos = np.array([[  0.04883,  7.794e-5],\n",
    "                  [ 7.794e-5,  0.0002801]])\n",
    "lyapunov_function      = safe_learning.QuadraticFunction(P_sos)\n",
    "grad_lyapunov_function = safe_learning.LinearSystem((2 * P_sos,))\n",
    "\n",
    "if OPTIONS.use_lipschitz_scaling:\n",
    "    L_v = lambda x: tf.abs(grad_lyapunov_function(x))\n",
    "else:\n",
    "    L_v = lambda x: tf.norm(grad_lyapunov_function(x), ord=1, axis=1, keepdims=True)\n",
    "\n",
    "# Initialize class\n",
    "lyapunov_sos = safe_learning.Lyapunov(grid, lyapunov_function, dynamics, L_dyn, L_v, tau, policy, initial_safe_set)\n",
    "lyapunov_sos.update_values()\n",
    "lyapunov_sos.update_safe_set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Training Lyapunov Candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIONS.pre_train:\n",
    "    # Quadratic candidate\n",
    "    P = np.eye(state_dim)\n",
    "    lyapunov_function      = safe_learning.QuadraticFunction(P)\n",
    "    grad_lyapunov_function = safe_learning.LinearSystem((2 * P,))\n",
    "\n",
    "    if OPTIONS.use_lipschitz_scaling:\n",
    "        L_v = lambda x: tf.abs(grad_lyapunov_function(x))\n",
    "    else:\n",
    "        L_v = lambda x: tf.norm(grad_lyapunov_function(x), ord=1, axis=1, keep_dims=True)\n",
    "\n",
    "    # Initialize class\n",
    "    lyapunov_pre = safe_learning.Lyapunov(grid, lyapunov_function, dynamics, L_dyn, L_v, tau, policy, initial_safe_set)\n",
    "    lyapunov_pre.update_values()\n",
    "    lyapunov_pre.update_safe_set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Lyapunov Candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha      = 0.2\n",
    "leaky_relu = lambda x, name: tf.nn.leaky_relu(x, alpha, name)\n",
    "\n",
    "layer_dims  = [64, 64]\n",
    "activations = [tf.tanh, tf.tanh]\n",
    "# activations = [leaky_relu, leaky_relu, leaky_relu]\n",
    "\n",
    "lyapunov_function      = LyapunovNetwork(state_dim, layer_dims, activations, OPTIONS.eps)\n",
    "grad_lyapunov_function = lambda x: tf.gradients(lyapunov_function(x), x)[0]\n",
    "\n",
    "if OPTIONS.use_lipschitz_scaling:\n",
    "    L_v = lambda x: tf.abs(grad_lyapunov_function(x))\n",
    "else:\n",
    "    L_v = lambda x: tf.norm(grad_lyapunov_function(x), ord=1, axis=1, keepdims=True)\n",
    "\n",
    "# TODO need to use template before variables exist in the graph\n",
    "temp = tf.placeholder(OPTIONS.tf_dtype, shape=[None, state_dim], name='states')\n",
    "temp = lyapunov_function(temp)\n",
    "session.run(tf.variables_initializer(lyapunov_function.parameters))\n",
    "\n",
    "lyapunov_nn = safe_learning.Lyapunov(grid, lyapunov_function, dynamics, L_dyn, L_v, tau, policy, initial_safe_set)\n",
    "lyapunov_nn.update_values()\n",
    "lyapunov_nn.update_safe_set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamics\n",
    "tf_states           = tf.placeholder(OPTIONS.tf_dtype, shape=[None, state_dim], name='states')\n",
    "tf_actions          = policy(tf_states)\n",
    "tf_future_states    = dynamics(tf_states, tf_actions)\n",
    "\n",
    "# Neural network\n",
    "tf_values_nn        = lyapunov_nn.lyapunov_function(tf_states)\n",
    "tf_future_values_nn = lyapunov_nn.lyapunov_function(tf_future_states)\n",
    "tf_dv_nn            = tf_future_values_nn - tf_values_nn\n",
    "tf_threshold        = lyapunov_nn.threshold(tf_states, lyapunov_nn.tau)\n",
    "tf_negative         = tf.squeeze(tf.less(tf_dv_nn, tf_threshold), axis=1)\n",
    "\n",
    "# LQR\n",
    "tf_values_lqr        = lyapunov_lqr.lyapunov_function(tf_states)\n",
    "tf_future_values_lqr = lyapunov_lqr.lyapunov_function(tf_future_states)\n",
    "tf_dv_lqr            = tf_future_values_lqr - tf_values_lqr\n",
    "\n",
    "# SOS\n",
    "# tf_values_sos        = lyapunov_sos.lyapunov_function(tf_states)\n",
    "# tf_future_values_sos = lyapunov_sos.lyapunov_function(tf_future_states)\n",
    "# tf_dv_sos            = tf_future_values_sos - tf_values_sos\n",
    "\n",
    "# Pre-training\n",
    "if OPTIONS.pre_train:\n",
    "    tf_values_pre        = lyapunov_pre.lyapunov_function(tf_states)\n",
    "    tf_future_values_pre = lyapunov_pre.lyapunov_function(tf_future_states)\n",
    "    tf_dv_pre            = tf_future_values_pre - tf_values_pre\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Region of Attraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roa(grid, closed_loop_dynamics, horizon=250, tol=1e-3, equilibrium=None, no_traj=True):\n",
    "    if isinstance(grid, np.ndarray):\n",
    "        all_points = grid\n",
    "        nindex = grid.shape[0]\n",
    "        ndim = grid.shape[1]\n",
    "    else:\n",
    "        all_points = grid.all_points\n",
    "        nindex = grid.nindex\n",
    "        ndim = grid.ndim\n",
    "    \n",
    "    # Forward-simulate all trajectories from initial points in the discretization\n",
    "    if no_traj:\n",
    "        end_states = all_points\n",
    "        for t in range(1, horizon):\n",
    "            end_states = closed_loop_dynamics(end_states)\n",
    "    else:\n",
    "        trajectories = np.empty((nindex, ndim, horizon))\n",
    "        trajectories[:, :, 0] = all_points\n",
    "        for t in range(1, horizon):\n",
    "            trajectories[:, :, t] = closed_loop_dynamics(trajectories[:, :, t - 1])\n",
    "        end_states = trajectories[:, :, -1]\n",
    "            \n",
    "    if equilibrium is None:\n",
    "        equilibrium = np.zeros((1, ndim))\n",
    "    \n",
    "    # Compute an approximate ROA as all states that end up \"close\" to 0\n",
    "    dists = np.linalg.norm(end_states - equilibrium, ord=2, axis=1, keepdims=True).ravel()\n",
    "    roa = (dists <= tol)\n",
    "    if no_traj:\n",
    "        return roa\n",
    "    else:\n",
    "        return roa, trajectories\n",
    "\n",
    "\n",
    "closed_loop_dynamics = lambda x: tf_future_states.eval({tf_states: x})\n",
    "horizon = 500\n",
    "tol = 0.1\n",
    "roa, trajectories = compute_roa(grid, closed_loop_dynamics, horizon, tol, no_traj=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network: Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = []\n",
    "level_states = grid.all_points[initial_safe_set]\n",
    "\n",
    "if OPTIONS.pre_train:\n",
    "    with tf.name_scope('lyapunov_pre_training'):\n",
    "        tf_losses        = tf.abs(tf_values_nn - tf_values_pre) / tf.stop_gradient(tf_values_pre + OPTIONS.eps)\n",
    "        tf_objective     = tf.reduce_mean(tf_losses, name='objective')\n",
    "        tf_learning_rate = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='learning_rate')\n",
    "        optimizer        = tf.train.GradientDescentOptimizer(tf_learning_rate)\n",
    "        lyapunov_update  = optimizer.minimize(tf_objective, var_list=lyapunov_nn.lyapunov_function.parameters)\n",
    "        \n",
    "        tf_batch_size = tf.placeholder(tf.int32, [], 'batch_size')\n",
    "        tf_batch      = tf.random_uniform([tf_batch_size, ], 0, level_states.shape[0], dtype=tf.int32, name='batch_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIONS.pre_train:\n",
    "    # Test set\n",
    "    test_size = int(1e3)\n",
    "    idx       = tf_batch.eval({tf_batch_size: int(1e3)})\n",
    "    test_set  = level_states[idx, :]\n",
    "\n",
    "    feed_dict = {\n",
    "        tf_states:         level_states,\n",
    "        tf_learning_rate:  1e-3,\n",
    "        tf_batch_size:     int(1e3),\n",
    "    }\n",
    "    max_iters = 200\n",
    "\n",
    "    for i in tqdm(range(max_iters)):\n",
    "        idx = tf_batch.eval(feed_dict)\n",
    "        feed_dict[tf_states] = level_states[idx, :]\n",
    "        session.run(lyapunov_update, feed_dict)\n",
    "\n",
    "        feed_dict[tf_states] = test_set\n",
    "        obj.append(tf_objective.eval(feed_dict))\n",
    "        \n",
    "    lyapunov_nn.update_values()\n",
    "    lyapunov_nn.update_safe_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 2), dpi=OPTIONS.dpi)\n",
    "ax.set_xlabel(r'iteration')\n",
    "ax.set_ylabel(r'objective')\n",
    "ax.plot(obj, '.-r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5), dpi=OPTIONS.dpi)\n",
    "ax.set_aspect(maxes[0] / maxes[1])\n",
    "ax.set_xlim(plot_limits[0])\n",
    "ax.set_ylim(plot_limits[1])\n",
    "ax.set_xlabel(r'$\\phi$ [deg]', fontproperties=OPTIONS.fontproperties)\n",
    "ax.set_ylabel(r'$\\dot{\\phi}$ [deg/s]', fontproperties=OPTIONS.fontproperties)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontproperties(OPTIONS.fontproperties)\n",
    "    \n",
    "# ROA\n",
    "z = roa.reshape(grid.num_points)\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=maxes[0] / maxes[1], cmap=binary_cmap('green'), vmin=0)\n",
    "\n",
    "# Estimated safe level set\n",
    "c = lyapunov_nn.feed_dict[lyapunov_nn.c_max]\n",
    "z = (lyapunov_nn.values <= c).reshape(grid.num_points)\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=maxes[0] / maxes[1], cmap=binary_cmap('red'), vmin=0)   \n",
    "\n",
    "# Sub-sample discretization for faster and clearer plotting\n",
    "N_traj = 14\n",
    "skip = int(grid.num_points[0] / N_traj)\n",
    "sub_idx = np.arange(grid.nindex).reshape(grid.num_points)\n",
    "sub_idx = sub_idx[::skip, ::skip].ravel()\n",
    "sub_trajectories = trajectories[sub_idx, :, :]\n",
    "sub_states = grid.all_points[sub_idx]\n",
    "\n",
    "# Trajectories\n",
    "for n in range(sub_trajectories.shape[0]):\n",
    "    theta = sub_trajectories[n, 0, :] * np.rad2deg(norms[0])\n",
    "    omega = sub_trajectories[n, 1, :] * np.rad2deg(norms[1])\n",
    "    ax.plot(theta, omega, 'k--', linewidth=0.6)\n",
    "dx_dt = (tf_future_states.eval({tf_states: sub_states}) - sub_states) / dt\n",
    "dx_dt = dx_dt / np.linalg.norm(dx_dt, ord=2, axis=1, keepdims=True)\n",
    "ax.quiver(sub_states[:, 0] * norms[0], sub_states[:, 1] * norms[1], dx_dt[:, 0], dx_dt[:, 1], \n",
    "          scale=None, pivot='mid', headwidth=4, headlength=8, color='k')\n",
    "\n",
    "proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in [(0., 1., 0., 1), (1., 0., 0., 1)]]    \n",
    "legend = ax.legend(proxy, [r'$\\mathcal{R}$', r'$\\mathcal{V}\\!\\ (c_0)$'], prop=OPTIONS.fontproperties, loc='upper right')\n",
    "legend.get_frame().set_alpha(1.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint for neural net weights\n",
    "saver = tf.train.Saver(var_list=lyapunov_nn.lyapunov_function.parameters)\n",
    "ckpt_prefix = \"/tmp/spencerr/pendulum_lyapunov\"\n",
    "saver.save(session, ckpt_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('roa_classification'):\n",
    "    # Current maximum level set we want to push the ROA in to\n",
    "    tf_c_max            = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='c_max')\n",
    "    tf_level_multiplier = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='level_multiplier')\n",
    "    \n",
    "    # True class labels, converted from ROA booleans {0, 1} to data labels {-1, 1}\n",
    "    tf_roa     = tf.placeholder(OPTIONS.tf_dtype, shape=[None, 1], name='roa')\n",
    "    tf_labels  = 2 * tf_roa - 1\n",
    "\n",
    "    # Classifier output (signed distance to decision boundary c_max = c)\n",
    "    tf_decision_dist = tf_c_max - tf_values_nn\n",
    "    tf_y_est         = 0.5 * (tf.sign(tf_decision_dist) + 1)\n",
    "    \n",
    "    # Use perceptron / hinge / logistic loss with class weights\n",
    "    tf_weights         = tf.placeholder(OPTIONS.tf_dtype, shape=[None, 1], name='class_weights')\n",
    "    tf_classifier_loss = tf_weights * tf.maximum(- tf_labels * tf_decision_dist, 0, name='perceptron_loss')\n",
    "#     tf_classifier_loss = tf_weights * tf.maximum(1 - tf_labels * tf_decision_dist, 0, name='hinge_loss')\n",
    "#     tf_classifier_loss = tf_weights * tf.log(1 + tf.exp(- tf_labels * tf_decision_dist), name='logistic_loss')\n",
    "    \n",
    "    # Enforce decrease constraint with Lagrangian relaxation\n",
    "    tf_lagrange_multiplier = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='lagrange_multiplier')\n",
    "    tf_decrease_loss       = tf_roa * tf.maximum(tf_dv_nn - tf_threshold, 0) / (tf_values_nn + OPTIONS.eps)\n",
    "#     tf_decrease_loss       = tf_y_est * tf.maximum(tf_dv_nn - tf_threshold, 0) / tf.stop_gradient(tf_values_nn + OPTIONS.eps)\n",
    "    \n",
    "    # Define update step\n",
    "    tf_objective     = tf.reduce_mean(tf_classifier_loss + tf_lagrange_multiplier * tf_decrease_loss, name='objective')\n",
    "    tf_learning_rate = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='learning_rate')\n",
    "    optimizer        = tf.train.GradientDescentOptimizer(tf_learning_rate)\n",
    "    training_update  = optimizer.minimize(tf_objective, var_list=lyapunov_nn.lyapunov_function.parameters)\n",
    "\n",
    "with tf.name_scope('sampling'):\n",
    "    tf_batch_size = tf.placeholder(tf.int32, [], 'batch_size')\n",
    "    tf_idx_range  = tf.placeholder(tf.int32, shape=[], name='indices_to_sample')\n",
    "    tf_idx_batch  = tf.random_uniform([tf_batch_size, ], 0, tf_idx_range, dtype=tf.int32, name='batch_sample')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore checkpoint\n",
    "saver.restore(session, ckpt_prefix)\n",
    "lyapunov_nn.update_values()\n",
    "lyapunov_nn.update_safe_set()\n",
    "# session.run(tf.variables_initializer(optimizer.variables()))\n",
    "\n",
    "obj          = []\n",
    "loss_class   = []\n",
    "loss_dec     = []\n",
    "roa_estimate = np.copy(lyapunov_nn.safe_set)\n",
    "idx_visited  = np.zeros_like(lyapunov_nn.safe_set)\n",
    "\n",
    "c_max     = [lyapunov_nn.feed_dict[lyapunov_nn.c_max], ]\n",
    "safe_size = [lyapunov_nn.safe_set.sum() / grid.nindex, ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_iters         = 3\n",
    "inner_iters         = 10\n",
    "horizon             = 50\n",
    "lagrange_multiplier = 1000\n",
    "\n",
    "feed_dict = {\n",
    "    tf_states:               np.zeros((1, grid.ndim)), # placeholder\n",
    "    tf_batch_size:           int(1e2),\n",
    "    tf_c_max:                1,\n",
    "    tf_lagrange_multiplier:  lagrange_multiplier,\n",
    "    tf_idx_range:            grid.nindex,\n",
    "    #\n",
    "    tf_learning_rate:        1e-2,\n",
    "    tf_level_multiplier:     2.,\n",
    "}\n",
    "\n",
    "test_set = grid.all_points\n",
    "test_labels = roa.reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Current metrics ...')\n",
    "c = lyapunov_nn.feed_dict[lyapunov_nn.c_max]\n",
    "num_safe = lyapunov_nn.safe_set.sum()\n",
    "print('c_max: {}'.format(c))\n",
    "print('grid size: {}'.format(grid.nindex))\n",
    "print('safe set size: {} ({:.2f}% of grid, {:.2f}% of ROA)\\n'.format(int(num_safe), 100 * num_safe / grid.nindex, 100 * num_safe / roa.sum()))\n",
    "print('')\n",
    "time.sleep(0.5)\n",
    "\n",
    "for _ in range(outer_iters):\n",
    "    # Identify current safe set and gap states around it\n",
    "    c         = lyapunov_nn.feed_dict[lyapunov_nn.c_max]\n",
    "    idx_small = lyapunov_nn.values.ravel() <= c\n",
    "    idx_big   = lyapunov_nn.values.ravel() <= feed_dict[tf_level_multiplier] * c\n",
    "    idx_gap   = np.logical_and(idx_big, ~idx_small)\n",
    "    \n",
    "    # Update ROA estimate by propagating gap states forward\n",
    "    propagated_states = grid.all_points[idx_gap]\n",
    "    for _ in range(horizon):\n",
    "        propagated_states = tf_future_states.eval({tf_states: propagated_states})\n",
    "#         np.clip(propagated_states, -1, 1, out=propagated_states)\n",
    "    safe_in_future = (tf_values_nn.eval({tf_states: propagated_states}) <= c).ravel()\n",
    "    roa_estimate[idx_gap] |= safe_in_future\n",
    "    \n",
    "    # Train classifier on current ROA estimate and any states from scaled level set\n",
    "    target_idx              = np.logical_or(idx_big, roa_estimate)\n",
    "    target_set              = grid.all_points[target_idx]\n",
    "    target_labels           = roa_estimate[target_idx].astype(OPTIONS.np_dtype).reshape([-1, 1])\n",
    "    feed_dict[tf_idx_range] = target_set.shape[0]\n",
    "    idx_visited |= target_idx\n",
    "    \n",
    "    # Test set\n",
    "    test_set = target_set\n",
    "    test_labels = target_labels\n",
    "    \n",
    "    # SGD for classification\n",
    "    for _ in tqdm(range(inner_iters)):\n",
    "        # Training step\n",
    "        idx_batch                     = tf_idx_batch.eval(feed_dict)\n",
    "        feed_dict[tf_states]          = target_set[idx_batch]\n",
    "        feed_dict[tf_roa]             = target_labels[idx_batch]\n",
    "#         feed_dict[tf_weights], counts = confusion_weights(tf_values.eval(feed_dict) <= feed_dict[tf_c_max], feed_dict[tf_roa].astype(bool))\n",
    "        feed_dict[tf_weights], counts = class_weights(feed_dict[tf_roa].astype(bool))\n",
    "        session.run(training_update, feed_dict=feed_dict)\n",
    "\n",
    "        # Record objectives\n",
    "        feed_dict[tf_states]          = test_set\n",
    "        feed_dict[tf_roa]             = test_labels\n",
    "#         feed_dict[tf_weights], counts = confusion_weights(tf_values.eval(feed_dict) <= feed_dict[tf_c_max], feed_dict[tf_roa].astype(bool))\n",
    "        feed_dict[tf_weights], counts = class_weights(feed_dict[tf_roa].astype(bool))\n",
    "\n",
    "        results = session.run([tf_classifier_loss, tf_decrease_loss], feed_dict)\n",
    "        loss_class.append(results[0].mean())\n",
    "        loss_dec.append(results[1].mean())\n",
    "        obj.append(loss_class[-1] + feed_dict[tf_lagrange_multiplier] * loss_dec[-1])\n",
    "\n",
    "    lyapunov_nn.update_values()\n",
    "    lyapunov_nn.update_safe_set()\n",
    "    roa_estimate |= lyapunov_nn.safe_set\n",
    "    safe_size.append(lyapunov_nn.safe_set.sum() / grid.nindex)\n",
    "    c_max.append(lyapunov_nn.feed_dict[lyapunov_nn.c_max])\n",
    "    \n",
    "    print(counts)\n",
    "    print('c_max: {}'.format(c_max[-1]))\n",
    "    print('safe set size: {} ({:.2f}% of grid, {:.2f}% of ROA)\\n'.format(int(safe_size[-1] * grid.nindex), \n",
    "                                                                         100 * safe_size[-1], \n",
    "                                                                         100 * safe_size[-1] * roa.size / roa.sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3), dpi=OPTIONS.dpi)\n",
    "\n",
    "ax.plot(loss_class, '.-r')\n",
    "ax.plot(feed_dict[tf_lagrange_multiplier] * np.asarray(loss_dec), '.-b')\n",
    "\n",
    "ax.set_xlabel(r'SGD iteration (accumulated)')\n",
    "ax.set_ylabel(r'Training loss')\n",
    "# ax.set_xticks(list(range(0, len(loss_class) + 1, inner_iters)))\n",
    "\n",
    "proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in ['red', 'blue']]    \n",
    "legend = ax.legend(proxy, ['Classification loss', 'Lyapunov decrease loss'], loc='upper right', fontsize=8)\n",
    "legend.get_frame().set_alpha(0.5)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3), dpi=OPTIONS.dpi)\n",
    "roa_fraction = roa.sum() / roa.size\n",
    "\n",
    "ax.plot(c_max, '.-r')\n",
    "ax.set_ylabel(r'$c_k$')\n",
    "ax.tick_params('y', colors='r')\n",
    "# ax.set_ylim([0, 1])\n",
    "\n",
    "ax.set_xlabel(r'Safe set update iteration $k$')\n",
    "# ax.set_xticks(list(range(0, len(c_max) + 1, 1)))\n",
    "\n",
    "ax = ax.twinx()\n",
    "ax.plot(np.array(safe_size) / roa_fraction, '.-b')\n",
    "ax.set_ylabel(r'$|\\mathcal{V}(c_k) \\cap \\mathcal{X}_\\tau|\\ /\\ |\\mathcal{R} \\cap \\mathcal{X}_\\tau|$')\n",
    "ax.tick_params('y', colors='b')\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(np.array(safe_size) / roa_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network: Visualization with Phase Portrait and ROA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4), dpi=OPTIONS.dpi)\n",
    "ax.set_aspect(theta_max / omega_max)\n",
    "ax.set_xlim(plot_limits[0])\n",
    "ax.set_ylim(plot_limits[1])\n",
    "    \n",
    "# True ROA\n",
    "z = roa.reshape(grid.num_points)\n",
    "# im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=theta_max / omega_max, cmap=binary_cmap('blue', 0.5), vmin=0)\n",
    "im = ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), colors='darkgreen', linestyles='dashed')\n",
    "\n",
    "# Safe set\n",
    "z = lyapunov_nn.safe_set.reshape(grid.num_points)\n",
    "# im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=theta_max / omega_max, cmap=binary_cmap('blue', 1), vmin=0)\n",
    "im = ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), colors='blue')\n",
    "\n",
    "# Decrease region\n",
    "z = tf_dv_nn.eval({tf_states: grid.all_points}).reshape(grid.num_points) < 0\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=theta_max / omega_max, cmap=binary_cmap('blue', 0.3), vmin=0)\n",
    "# im = ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), cmap=ListedColormap(['blue']), levels=0, linestyles='dashed')\n",
    "\n",
    "# Estimated ROA\n",
    "alpha = 0.5\n",
    "cmap = ListedColormap([(1., 1., 1., 0.), (1., 0., 0., alpha), (0., 1., 0., alpha)])\n",
    "# z = tf_negative.eval({tf_states: grid.all_points}).reshape(grid.num_points)\n",
    "z = roa_estimate.astype(int)\n",
    "z[idx_visited] += 1\n",
    "z = z.reshape(grid.num_points)\n",
    "# im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=theta_max / omega_max, cmap=cmap, vmin=0)\n",
    "# im = ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), cmap=ListedColormap(['blue', 'red', 'green']))\n",
    "\n",
    "# Neural-network level sets\n",
    "z = tf_values_nn.eval({tf_states: grid.all_points}).reshape(grid.num_points)\n",
    "im = ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), colors='darkviolet', levels=1, linestyles='dashed')\n",
    "im = ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), colors='darkviolet', \n",
    "                levels=2 * lyapunov_nn.feed_dict[lyapunov_nn.c_max], linestyles='dashed')\n",
    "\n",
    "# SOS Lyapunov function\n",
    "z = lyapunov_sos.safe_set.reshape(grid.num_points)\n",
    "im = ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), colors='tomato', linestyles='dashed')\n",
    "\n",
    "# LQR Lyapunov function\n",
    "z = lyapunov_lqr.safe_set.reshape(grid.num_points)\n",
    "im = ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), colors='hotpink', linestyles='dashed')\n",
    "\n",
    "# # Trajectories\n",
    "for n in range(sub_trajectories.shape[0]):\n",
    "    x = sub_trajectories[n, 0, :] * np.rad2deg(theta_max)\n",
    "    y = sub_trajectories[n, 1, :] * np.rad2deg(omega_max)\n",
    "    ax.plot(x, y, 'k--', linewidth=0.25)\n",
    "# sub_states = grid.all_points[sub_idx]\n",
    "# dx_dt = (dynamics(sub_states) - sub_states) / dt\n",
    "# dx_dt = dx_dt / np.linalg.norm(dx_dt, ord=2, axis=1, keepdims=True)\n",
    "# ax.quiver(sub_states[:, 0] * theta_max, sub_states[:, 1] * omega_max, dx_dt[:, 0], dx_dt[:, 1], \n",
    "#           scale=None, pivot='mid', headwidth=3, headlength=6, color='k')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4), dpi=OPTIONS.dpi)\n",
    "ax.set_aspect(theta_max / omega_max)\n",
    "ax.set_xlim(plot_limits[0])\n",
    "ax.set_ylim(plot_limits[1])\n",
    "    \n",
    "a = tf_dv_nn.eval({tf_states: grid.all_points}).reshape(grid.num_points)\n",
    "b = lyapunov_nn.threshold(tf_states, state_dim / 2000).eval({tf_states: grid.all_points}).reshape(grid.num_points)\n",
    "\n",
    "z = a - b < 0\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=theta_max / omega_max, cmap=binary_cmap('blue', 0.3), vmin=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model (FOR PAPER ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saver       = tf.train.Saver(var_list=lyapunov_nn.lyapunov_function.parameters)\n",
    "# ckpt_prefix = \"./tf_checkpoints/pendulum\"\n",
    "# saver.save(session, ckpt_prefix)\n",
    "\n",
    "# var_list = [grid, tau, initial_safe_set, roa, lagrange_multiplier, obj, loss_class, loss_dec, \n",
    "#             roa_estimate, idx_visited, c_max, safe_size, sub_trajectories, sub_states, dx_dt]\n",
    "\n",
    "# # Saving the objects:\n",
    "# with open(ckpt_prefix + '.pkl', 'wb') as file:\n",
    "#     pickle.dump(var_list, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
