{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning a Lyapunov Function for the Inverted Pendulum\n",
    "\n",
    "Construct and train a parameterized Lyapunov function for the inverted pendulum system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import safe_learning\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "from scipy.linalg import block_diag\n",
    "from utilities import InvertedPendulum, LyapunovNetwork, balanced_class_weights, find_nearest, binary_cmap, compute_roa\n",
    "\n",
    "# Nice progress bars\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x\n",
    "\n",
    "_STORAGE = {}\n",
    "\n",
    "HEAT_MAP = plt.get_cmap('inferno', lut=None)\n",
    "HEAT_MAP.set_over('white')\n",
    "HEAT_MAP.set_under('black')\n",
    "\n",
    "LEVEL_MAP = plt.get_cmap('viridis', lut=21)\n",
    "LEVEL_MAP.set_over('gold')\n",
    "LEVEL_MAP.set_under('white')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Options, self).__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "OPTIONS = Options(np_dtype              = safe_learning.config.np_dtype,\n",
    "                  tf_dtype              = safe_learning.config.dtype,\n",
    "                  saturate              = True,                            # apply saturation constraints to the control input\n",
    "                  eps                   = 1e-8,                            # numerical tolerance\n",
    "                  use_linear_dynamics   = False,                           # use the linearized form of the dynamics as the true dynamics (for testing)\n",
    "                  use_lipschitz_scaling = True,                            # use different Lipschitz constants in each state for the Lyapunov function\n",
    "                  use_zero_threshold    = True,                            # assume the discretization is infinitely fine (i.e., tau = 0; for testing)\n",
    "                  dpi                   = 200,\n",
    "                  num_cores             = 4,\n",
    "                  num_sockets           = 1,\n",
    "                  tf_checkpoint_path    = \"./tmp/pendulum_lyapunov.ckpt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Session\n",
    "\n",
    "Customize the TensorFlow session for the current device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KMP_BLOCKTIME\"]    = str(0)\n",
    "os.environ[\"KMP_SETTINGS\"]     = str(1)\n",
    "os.environ[\"KMP_AFFINITY\"]     = 'granularity=fine,noverbose,compact,1,0'\n",
    "os.environ[\"OMP_NUM_THREADS\"]  = str(OPTIONS.num_cores)\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads  = OPTIONS.num_cores,\n",
    "                        inter_op_parallelism_threads  = OPTIONS.num_sockets,\n",
    "                        allow_soft_placement          = False,\n",
    "                        device_count                  = {'CPU': OPTIONS.num_cores})\n",
    "\n",
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "session = tf.InteractiveSession(config=config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics\n",
    "\n",
    "Define the nonlinear and linearized forms of the inverted pendulum dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dt = 0.01   # sampling time\n",
    "g = 9.81    # gravity\n",
    "\n",
    "# True system parameters\n",
    "m = 0.15    # pendulum mass\n",
    "L = 0.5     # pole length\n",
    "b = 0.1     # rotational friction\n",
    "\n",
    "# State and action normalizers\n",
    "theta_max = np.deg2rad(180)                     # angular position [rad]\n",
    "omega_max = np.deg2rad(360)                     # angular velocity [rad/s]\n",
    "u_max     = g * m * L * np.sin(np.deg2rad(60))  # torque [N], control action\n",
    "\n",
    "state_norm = (theta_max, omega_max)\n",
    "action_norm = (u_max,)\n",
    "\n",
    "# Dimensions and domains\n",
    "state_dim     = 2\n",
    "action_dim    = 1\n",
    "state_limits  = np.array([[-1., 1.]] * state_dim)\n",
    "action_limits = np.array([[-1., 1.]] * action_dim)\n",
    "\n",
    "# Initialize system class and its linearization\n",
    "pendulum = InvertedPendulum(m, L, b, dt, [state_norm, action_norm])\n",
    "A, B = pendulum.linearize()\n",
    "\n",
    "if OPTIONS.use_linear_dynamics:\n",
    "    dynamics = safe_learning.functions.LinearSystem((A, B), name='dynamics')\n",
    "else:\n",
    "    dynamics = pendulum.__call__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Discretization and Initial Safe Set\n",
    "\n",
    "Define a uniform discretization, and an initial known safe set as a subset of this discretization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of states along each dimension\n",
    "num_states = [251, ] * state_dim\n",
    "\n",
    "# State grid\n",
    "grid_limits = np.array([[-1., 1.], ] * state_dim)\n",
    "state_discretization = safe_learning.GridWorld(grid_limits, num_states)\n",
    "\n",
    "# Discretization constant\n",
    "if OPTIONS.use_zero_threshold:\n",
    "    tau = 0.0\n",
    "else:\n",
    "    tau = np.sum(state_discretization.unit_maxes) / 2\n",
    "\n",
    "print('Grid size: {}'.format(state_discretization.nindex))\n",
    "print('Discretization constant (tau): {}'.format(tau))\n",
    "\n",
    "# Set initial safe set as a ball around the origin (in normalized coordinates)\n",
    "cutoff_radius    = 0.1\n",
    "initial_safe_set = np.linalg.norm(state_discretization.all_points, ord=2, axis=1) <= cutoff_radius\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed Policy\n",
    "\n",
    "Fix the policy to the LQR solution for the linearized, discretized system, possibly with saturation constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 0.1 * np.identity(state_dim).astype(OPTIONS.np_dtype)     # state cost matrix\n",
    "R = 0.1 * np.identity(action_dim).astype(OPTIONS.np_dtype)    # action cost matrix\n",
    "K, P = safe_learning.utilities.dlqr(A, B, Q, R)\n",
    "P /= np.abs(P).max()                                          # normalize cost\n",
    "\n",
    "policy = safe_learning.LinearSystem(- K, name='policy')\n",
    "if OPTIONS.saturate:\n",
    "    policy = safe_learning.Saturation(policy, -1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed-Loop Dynamics Lipschitz Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy (linear)\n",
    "L_pol = np.linalg.norm(-K, 1)\n",
    "\n",
    "# Dynamics (linear approximation)\n",
    "L_dyn = np.linalg.norm(A, 1) + np.linalg.norm(B, 1) * L_pol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LQR Lyapunov Candidate\n",
    "\n",
    "Define a Lyapunov candidate function corresponding to the LQR solution for the linearized system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Lyapunov function corresponding to the LQR policy\n",
    "lyapunov_function = safe_learning.QuadraticFunction(P)\n",
    "\n",
    "# Approximate local Lipschitz constants with gradients\n",
    "grad_lyapunov_function = safe_learning.LinearSystem((2 * P,))\n",
    "if OPTIONS.use_lipschitz_scaling:\n",
    "    L_v = lambda s: tf.abs(grad_lyapunov_function(s))\n",
    "else:\n",
    "    L_v = lambda s: tf.norm(grad_lyapunov_function(s), ord=1, axis=1, keep_dims=True)\n",
    "\n",
    "# Initialize Lyapunov class\n",
    "lyapunov_lqr = safe_learning.Lyapunov(state_discretization, lyapunov_function, dynamics, L_dyn, L_v, tau, policy, initial_safe_set)\n",
    "lyapunov_lqr.update_values()\n",
    "lyapunov_lqr.update_safe_set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Lyapunov Candidate\n",
    "\n",
    "Define a parameterized Lyapunov candidate function with a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [64, 64]\n",
    "activations = [tf.tanh, tf.tanh]\n",
    "lyapunov_function = LyapunovNetwork(state_dim, layer_dims, activations, OPTIONS.eps)\n",
    "\n",
    "# Approximate local Lipschitz constants with gradients\n",
    "grad_lyapunov_function = lambda x: tf.gradients(lyapunov_function(x), x)[0]\n",
    "if OPTIONS.use_lipschitz_scaling:\n",
    "    L_v = lambda x: tf.abs(grad_lyapunov_function(x))\n",
    "else:\n",
    "    L_v = lambda x: tf.norm(grad_lyapunov_function(x), ord=1, axis=1, keepdims=True)\n",
    "\n",
    "# Initialize parameters; TODO need to use the template before parameter variables exist in the TensorFlow graph\n",
    "temp = tf.placeholder(OPTIONS.tf_dtype, shape=[None, state_dim], name='states')\n",
    "temp = lyapunov_function(temp)\n",
    "session.run(tf.variables_initializer(lyapunov_function.parameters))\n",
    "\n",
    "# Initialize Lyapunov class\n",
    "lyapunov_nn = safe_learning.Lyapunov(state_discretization, lyapunov_function, dynamics, L_dyn, L_v, tau, policy, initial_safe_set)\n",
    "lyapunov_nn.update_values()\n",
    "lyapunov_nn.update_safe_set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamics\n",
    "tf_states           = tf.placeholder(OPTIONS.tf_dtype, shape=[None, state_dim], name='states')\n",
    "tf_actions          = policy(tf_states)\n",
    "tf_future_states    = dynamics(tf_states, tf_actions)\n",
    "\n",
    "# Neural network\n",
    "tf_values_nn        = lyapunov_nn.lyapunov_function(tf_states)\n",
    "tf_future_values_nn = lyapunov_nn.lyapunov_function(tf_future_states)\n",
    "tf_dv_nn            = tf_future_values_nn - tf_values_nn\n",
    "tf_threshold        = lyapunov_nn.threshold(tf_states, lyapunov_nn.tau)\n",
    "tf_negative         = tf.squeeze(tf.less(tf_dv_nn, tf_threshold), axis=1)\n",
    "\n",
    "# LQR\n",
    "tf_values_lqr        = lyapunov_lqr.lyapunov_function(tf_states)\n",
    "tf_future_values_lqr = lyapunov_lqr.lyapunov_function(tf_future_states)\n",
    "tf_dv_lqr            = tf_future_values_lqr - tf_values_lqr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Region of Attraction\n",
    "\n",
    "Compute the true largest region of attraction (ROA), denoted as $\\mathcal{S}_\\pi$, by forward-simulating the closed-loop dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_loop_dynamics = lambda x: tf_future_states.eval({tf_states: x})\n",
    "horizon = 1000\n",
    "tol = 0.001\n",
    "roa, trajectories = compute_roa(lyapunov_nn.discretization, closed_loop_dynamics, horizon, tol, no_traj=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network: Training\n",
    "\n",
    "Train the parameteric Lyapunov candidate in order to expand the verifiable safe set towards $\\mathcal{S}_\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TensorFlow checkpoint for the neural network parameters\n",
    "saver = tf.train.Saver(var_list=lyapunov_nn.lyapunov_function.parameters)\n",
    "ckpt_path = saver.save(session, OPTIONS.tf_checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('roa_classification'):\n",
    "    # Target the safe level set to extend out towards\n",
    "    safe_level = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='c_max')\n",
    "    level_multiplier = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='level_multiplier')\n",
    "    \n",
    "    # True class labels, converted from Boolean ROA labels {0, 1} to {-1, 1}\n",
    "    roa_labels = tf.placeholder(OPTIONS.tf_dtype, shape=[None, 1], name='labels')\n",
    "    class_labels = 2 * roa_labels - 1\n",
    "\n",
    "    # Signed, possibly normalized distance from the decision boundary\n",
    "    decision_distance = safe_level - tf_values_nn\n",
    "    \n",
    "    # Perceptron loss with class weights\n",
    "    class_weights = tf.placeholder(OPTIONS.tf_dtype, shape=[None, 1], name='class_weights')\n",
    "    classifier_loss = class_weights * tf.maximum(- class_labels * decision_distance, 0, name='classifier_loss')\n",
    "    \n",
    "    # Enforce decrease constraint with Lagrangian relaxation\n",
    "    lagrange_multiplier = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='lagrange_multiplier')\n",
    "    decrease_loss = roa_labels * tf.maximum(tf_dv_nn, 0) / tf.stop_gradient(tf_values_nn + OPTIONS.eps)\n",
    "        \n",
    "    # Construct objective and optimizer\n",
    "    objective = tf.reduce_mean(classifier_loss + lagrange_multiplier * decrease_loss, name='objective')\n",
    "    learning_rate = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='learning_rate')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_update = optimizer.minimize(objective, var_list=lyapunov_nn.lyapunov_function.parameters)\n",
    "\n",
    "with tf.name_scope('sampling'):\n",
    "    batch_size = tf.placeholder(tf.int32, [], 'batch_size')\n",
    "    idx_range = tf.placeholder(tf.int32, shape=[], name='indices_to_sample')\n",
    "    idx_batch = tf.random_uniform([batch_size, ], 0, idx_range, dtype=tf.int32, name='batch_sample')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "Restore parameter checkpoint, and try training again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.restore(session, ckpt_path)\n",
    "lyapunov_nn.update_values()\n",
    "lyapunov_nn.update_safe_set()\n",
    "\n",
    "test_classifier_loss = []\n",
    "test_decrease_loss   = []\n",
    "roa_estimate         = np.copy(lyapunov_nn.safe_set)\n",
    "\n",
    "grid              = lyapunov_nn.discretization\n",
    "c_max             = [lyapunov_nn.feed_dict[lyapunov_nn.c_max], ]\n",
    "safe_set_fraction = [lyapunov_nn.safe_set.sum() / grid.nindex, ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "outer_iters = 5\n",
    "inner_iters = 10\n",
    "horizon     = 100\n",
    "test_size   = int(1e4)\n",
    "\n",
    "feed_dict = {\n",
    "    tf_states:           np.zeros((1, grid.ndim)), # placeholder\n",
    "    safe_level:          1.,\n",
    "    level_multiplier:    3.,\n",
    "    lagrange_multiplier: 1000,\n",
    "    learning_rate:       3e-3,\n",
    "    batch_size:          int(1e3),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Current metrics ...')\n",
    "c = lyapunov_nn.feed_dict[lyapunov_nn.c_max]\n",
    "num_safe = lyapunov_nn.safe_set.sum()\n",
    "print('Safe level (c_k): {}'.format(c))\n",
    "print('Safe set size: {} ({:.2f}% of grid, {:.2f}% of ROA)\\n'.format(int(num_safe), 100 * num_safe / grid.nindex, 100 * num_safe / roa.sum()))\n",
    "print('')\n",
    "time.sleep(0.5)\n",
    "\n",
    "for i in range(outer_iters):\n",
    "    # Identify the \"gap\" states, i.e., those between V(c_k) and V(a * c_k) for a > 1\n",
    "    c = lyapunov_nn.feed_dict[lyapunov_nn.c_max]\n",
    "    idx_small = lyapunov_nn.values.ravel() <= c\n",
    "    idx_big = lyapunov_nn.values.ravel() <= feed_dict[level_multiplier] * c\n",
    "    idx_gap = np.logical_and(idx_big, ~idx_small)\n",
    "    \n",
    "    # Forward-simulate \"gap\" states to determine which ones we can add to our ROA estimate\n",
    "    gap_states = grid.all_points[idx_gap]\n",
    "    for _ in range(horizon):\n",
    "        gap_states = tf_future_states.eval({tf_states: gap_states})\n",
    "    gap_future_values = tf_values_nn.eval({tf_states: gap_states})    \n",
    "    roa_estimate[idx_gap] |= (gap_future_values <= c).ravel()\n",
    "    \n",
    "    # Identify the class labels for our current ROA estimate and the expanded level set\n",
    "    target_idx = np.logical_or(idx_big, roa_estimate)\n",
    "    target_set = grid.all_points[target_idx]\n",
    "    target_labels = roa_estimate[target_idx].astype(OPTIONS.np_dtype).reshape([-1, 1])\n",
    "    feed_dict[idx_range] = target_set.shape[0]\n",
    "    \n",
    "    # Test set\n",
    "    idx_test = idx_batch.eval({batch_size: test_size, idx_range: target_set.shape[0]})\n",
    "    test_set = target_set[idx_test]\n",
    "    test_labels = target_labels[idx_test]\n",
    "    \n",
    "    # SGD for classification\n",
    "    for j in tqdm(range(inner_iters)):\n",
    "        # Training step\n",
    "        idx_batch_eval = idx_batch.eval(feed_dict)\n",
    "        feed_dict[tf_states] = target_set[idx_batch_eval]\n",
    "        feed_dict[roa_labels] = target_labels[idx_batch_eval]\n",
    "        feed_dict[class_weights], class_counts = balanced_class_weights(feed_dict[roa_labels].astype(bool))\n",
    "        session.run(training_update, feed_dict=feed_dict)\n",
    "\n",
    "        # Record losses on test set\n",
    "        feed_dict[tf_states] = test_set\n",
    "        feed_dict[roa_labels] = test_labels\n",
    "        feed_dict[class_weights], class_counts = balanced_class_weights(feed_dict[roa_labels].astype(bool))\n",
    "        results = session.run([classifier_loss, decrease_loss], feed_dict)\n",
    "        test_classifier_loss.append(results[0].mean())\n",
    "        test_decrease_loss.append(results[1].mean())\n",
    "\n",
    "    # Update Lyapunov values and ROA estimate, based on new parameter values\n",
    "    lyapunov_nn.update_values()\n",
    "    lyapunov_nn.update_safe_set()\n",
    "    roa_estimate |= lyapunov_nn.safe_set\n",
    "\n",
    "    c_max.append(lyapunov_nn.feed_dict[lyapunov_nn.c_max])\n",
    "    safe_set_fraction.append(lyapunov_nn.safe_set.sum() / grid.nindex)\n",
    "    print('Done!')\n",
    "    print('Current safe level (c_k): {}'.format(c_max[-1]))\n",
    "    print('Safe set size: {} ({:.2f}% of grid, {:.2f}% of ROA)\\n'.format(int(lyapunov_nn.safe_set.sum()), \n",
    "                                                                         100 * safe_set_fraction[-1], \n",
    "                                                                         100 * safe_set_fraction[-1] * roa.size / roa.sum()))\n",
    "    time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3), dpi=OPTIONS.dpi)\n",
    "\n",
    "ax.plot(test_classifier_loss, '.-r')\n",
    "ax.plot(feed_dict[lagrange_multiplier] * np.asarray(test_decrease_loss), '.-b')\n",
    "\n",
    "ax.set_xlabel(r'SGD iteration (accumulated)')\n",
    "ax.set_ylabel(r'test loss')\n",
    "\n",
    "proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in ['red', 'blue']]    \n",
    "legend = ax.legend(proxy, ['classification loss', 'Lyapunov decrease loss'], loc='upper right')\n",
    "legend.get_frame().set_alpha(1.)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roa_fraction = roa.sum() / grid.nindex\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3), dpi=OPTIONS.dpi)\n",
    "\n",
    "ax.plot(c_max, '.-r')\n",
    "ax.set_xlabel(r'safe set update iteration $k$')\n",
    "ax.set_ylabel(r'safe level $c_k$')\n",
    "ax.set_ylim([0, None])\n",
    "ax.tick_params('y', colors='r')\n",
    "\n",
    "ax = ax.twinx()\n",
    "ax.plot(np.array(safe_set_fraction) / roa_fraction, '.-b')\n",
    "ax.set_ylabel(r'$|\\mathcal{V}(c_k) \\cap \\mathcal{X}_\\tau|\\ /\\ |\\mathcal{R} \\cap \\mathcal{X}_\\tau|$')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.tick_params('y', colors='b')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Safe Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6), dpi=OPTIONS.dpi)\n",
    "ax.set_aspect(theta_max / omega_max)\n",
    "plot_limits = np.column_stack((- np.rad2deg([theta_max, omega_max]), np.rad2deg([theta_max, omega_max])))\n",
    "\n",
    "ax.set_xlim(plot_limits[0])\n",
    "ax.set_ylim(plot_limits[1])\n",
    "ax.set_xlabel(r'$\\theta$ [deg]')\n",
    "ax.set_ylabel(r'$\\omega$ [deg]')\n",
    "\n",
    "colors = ['green', 'blue', 'red']\n",
    "    \n",
    "# True ROA\n",
    "z = roa.reshape(grid.num_points)\n",
    "im = ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), colors=colors[0])\n",
    "\n",
    "# Neural network safe set\n",
    "z = lyapunov_nn.safe_set.reshape(grid.num_points)\n",
    "im = ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), colors=colors[1])\n",
    "\n",
    "# Decrease region\n",
    "z = tf_dv_nn.eval({tf_states: grid.all_points}).reshape(grid.num_points) < 0\n",
    "im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=theta_max / omega_max, cmap=binary_cmap(colors[1], 0.2), vmin=0)\n",
    "\n",
    "# LQR safe set\n",
    "z = lyapunov_lqr.safe_set.reshape(grid.num_points)\n",
    "im = ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), colors=colors[2])\n",
    "\n",
    "\n",
    "# Sub-sample discretization for faster and clearer plotting of trajectories\n",
    "N_traj = 14\n",
    "skip = int(grid.num_points[0] / N_traj)\n",
    "sub_idx = np.arange(grid.nindex).reshape(grid.num_points)\n",
    "sub_idx = sub_idx[::skip, ::skip].ravel()\n",
    "sub_trajectories = trajectories[sub_idx, :, :]\n",
    "sub_states = grid.all_points[sub_idx]\n",
    "\n",
    "# Trajectories\n",
    "for n in range(sub_trajectories.shape[0]):\n",
    "    x = sub_trajectories[n, 0, :] * np.rad2deg(theta_max)\n",
    "    y = sub_trajectories[n, 1, :] * np.rad2deg(omega_max)\n",
    "    ax.plot(x, y, 'k--', linewidth=0.25)\n",
    "sub_states = grid.all_points[sub_idx]\n",
    "dx_dt = (tf_future_states.eval({tf_states: sub_states}) - sub_states) / dt\n",
    "dx_dt = dx_dt / np.linalg.norm(dx_dt, ord=2, axis=1, keepdims=True)\n",
    "ax.quiver(sub_states[:, 0] * np.rad2deg(theta_max), sub_states[:, 1] * np.rad2deg(omega_max), dx_dt[:, 0], dx_dt[:, 1], \n",
    "          scale=None, pivot='mid', headwidth=3, headlength=6, color='k')\n",
    "\n",
    "proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in colors]    \n",
    "legend = ax.legend(proxy, [r'$\\mathcal{S}_\\pi$', 'NN', 'LQR'], loc='upper right')\n",
    "legend.get_frame().set_alpha(1.)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
