{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning a Lyapunov Function for a Cart-Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "import safe_learning\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas\n",
    "import mosek\n",
    "import cvxpy as cvx\n",
    "import os\n",
    "\n",
    "from scipy.linalg import block_diag\n",
    "from utilities import CartPole, debug, LyapunovNetwork\n",
    "from safe_learning.utilities import get_storage, set_storage\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.client import timeline\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# TODO testing ****************************************#\n",
    "class Options(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Options, self).__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "OPTIONS = Options(np_dtype              = safe_learning.config.np_dtype,\n",
    "                  tf_dtype              = safe_learning.config.dtype,\n",
    "                  saturate              = True,\n",
    "                  eps                   = 1e-8,\n",
    "                  use_linear_dynamics   = False,\n",
    "                  use_lipschitz_scaling = True,\n",
    "                  use_zero_threshold    = True,\n",
    "                  dpi                   = 200,\n",
    "                  fontproperties        = FontProperties(size=5),\n",
    "                  log_path              ='./tensorflow_logs/cartpole/')\n",
    "#******************************************************#\n",
    "\n",
    "_STORAGE = {}\n",
    "\n",
    "HEAT_MAP = plt.get_cmap('inferno', lut=None)\n",
    "HEAT_MAP.set_over('white')\n",
    "HEAT_MAP.set_under('black')\n",
    "\n",
    "LEVEL_MAP = plt.get_cmap('viridis', lut=21)\n",
    "LEVEL_MAP.set_over('gold')\n",
    "LEVEL_MAP.set_under('white')\n",
    "\n",
    "BINARY_MAP = ListedColormap([(1., 1., 1., 0.), (0., 1., 0., 0.65)])\n",
    "\n",
    "pandas.options.display.float_format = '{:,.4f}'.format\n",
    "pandas.set_option('expand_frame_repr', False)\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "plt.rc('font', size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CPU_COUNT = os.cpu_count()\n",
    "NUM_CORES = 8\n",
    "NUM_SOCKETS = 2\n",
    "\n",
    "os.environ[\"KMP_BLOCKTIME\"]    = str(0)\n",
    "os.environ[\"KMP_SETTINGS\"]     = str(1)\n",
    "os.environ[\"KMP_AFFINITY\"]     = 'granularity=fine,noverbose,compact,1,0'\n",
    "os.environ[\"OMP_NUM_THREADS\"]  = str(NUM_CORES)\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads  = NUM_CORES,\n",
    "                        inter_op_parallelism_threads  = NUM_SOCKETS,\n",
    "                        allow_soft_placement          = False,\n",
    "#                         log_device_placement          = True,\n",
    "                        device_count                  = {'CPU': MAX_CPU_COUNT},\n",
    "                       )\n",
    "\n",
    "# TODO manually for CPU-only?\n",
    "config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n",
    "\n",
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "session = tf.InteractiveSession(config=config)\n",
    "\n",
    "# print('Found MAX_CPU_COUNT =', MAX_CPU_COUNT)\n",
    "# for dev in session.list_devices():\n",
    "#     print(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dt = 0.01   # sampling time\n",
    "g = 9.81    # gravity\n",
    "\n",
    "# True system parameters\n",
    "m = 0.175    # pendulum mass\n",
    "M = 1.732    # cart mass\n",
    "L = 0.28     # pole length\n",
    "b = 0.1      # rotational friction\n",
    "\n",
    "# State and action normalizers\n",
    "x_max     = 0.5\n",
    "theta_max = np.deg2rad(30)\n",
    "v_max     = 2. # 5.\n",
    "omega_max = np.sqrt(g / L)\n",
    "u_max     = (M + m) * g * np.tan(theta_max)\n",
    "\n",
    "state_norm = (x_max, theta_max, v_max, omega_max)\n",
    "action_norm = (u_max, )\n",
    "\n",
    "# Dimensions and domains\n",
    "state_dim = 4\n",
    "action_dim = 1\n",
    "state_limits = np.array([[-1., 1.]] * state_dim)\n",
    "action_limits = np.array([[-1., 1.]] * action_dim)\n",
    "\n",
    "# True system\n",
    "cartpole = CartPole(m, M, L, b, dt, [state_norm, action_norm])\n",
    "A, B = cartpole.linearize()\n",
    "\n",
    "if OPTIONS.use_linear_dynamics:\n",
    "    dynamics = safe_learning.functions.LinearSystem((A, B), name='dynamics')\n",
    "else:\n",
    "    dynamics = cartpole.__call__\n",
    "    \n",
    "print(state_norm)\n",
    "print(action_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of states along each dimension\n",
    "num_states = [51, ] * state_dim\n",
    "\n",
    "# State grid\n",
    "grid_limits = np.array([[-1., 1.], [-1., 1.], [-1., 1.], [-1., 1.]])\n",
    "state_discretization = safe_learning.GridWorld(grid_limits, num_states)\n",
    "\n",
    "# Discretization constant\n",
    "if OPTIONS.use_zero_threshold:\n",
    "    tau = 0.0\n",
    "else:\n",
    "    tau = np.sum(state_discretization.unit_maxes) / 2\n",
    "\n",
    "print('Grid size: {}'.format(state_discretization.nindex))\n",
    "print('Discretization constant: {}'.format(tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State cost matrix\n",
    "Q = np.diag([0.1, 0.1, 0.1, 0.1]).astype(OPTIONS.np_dtype)\n",
    "\n",
    "# Action cost matrix\n",
    "R = 0.1 * np.identity(action_dim).astype(OPTIONS.np_dtype)\n",
    "\n",
    "# Normalize cost matrices\n",
    "# cost_norm = np.max([Q.max(), R.max()])\n",
    "# Q = Q / cost_norm\n",
    "# R = R / cost_norm\n",
    "\n",
    "# Quadratic cost function\n",
    "cost_function = safe_learning.QuadraticFunction(block_diag(Q, R), name='cost_function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix policy to the LQR solution for the true system\n",
    "K, P = safe_learning.utilities.dlqr(A, B, Q, R)\n",
    "policy = safe_learning.LinearSystem(- K, name='policy')\n",
    "if OPTIONS.saturate:\n",
    "    policy = safe_learning.Saturation(policy, -1, 1)\n",
    "\n",
    "\n",
    "def find_nearest(array, value, sorted_1d=True):\n",
    "    if not sorted_1d:\n",
    "        array = np.sort(array)\n",
    "    idx = np.searchsorted(array, value, side='left')\n",
    "    if idx > 0 and (idx == len(array) or np.abs(value - array[idx - 1]) < np.abs(value - array[idx])):\n",
    "        idx -= 1\n",
    "    return idx, array[idx]\n",
    "    \n",
    "    \n",
    "def plot_policy(policy, discretization, state_norm=None, fixed_state=[0., 0., 0., 0.]):\n",
    "    # Snap fixed_state to the closest grid point\n",
    "    fixed_state = np.asarray(fixed_state, dtype=OPTIONS.np_dtype)\n",
    "    fixed_index = np.zeros_like(fixed_state, dtype=int)\n",
    "    for d in range(discretization.ndim):\n",
    "        fixed_index[d], fixed_state[d] = find_nearest(discretization.discrete_points[d], fixed_state[d])\n",
    "        \n",
    "    # Get 2d-planes of the discretization (x vs. v, theta vs. omega) according to fixed_state\n",
    "    planes = [[0, 2], [1, 3]]\n",
    "    disc_slices = [0, ] * len(planes)\n",
    "    for i, p in enumerate(planes):\n",
    "        disc_slices[i] = np.logical_and(discretization.all_points[:, p[0]] == fixed_state[p[0]], \n",
    "                                        discretization.all_points[:, p[1]] == fixed_state[p[1]])\n",
    "            \n",
    "    # Plot x vs. v, and theta vs. omega\n",
    "    if state_norm is not None:\n",
    "        x_max, theta_max, v_max, omega_max = state_norm\n",
    "        scale = np.array([x_max, np.rad2deg(theta_max), v_max, np.rad2deg(omega_max)]).reshape((-1, 1))\n",
    "        limits = scale * discretization.limits\n",
    "    else:\n",
    "        limits = discretization.limits\n",
    "    \n",
    "    plt.rc('font', size=5)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 3), dpi=OPTIONS.dpi)\n",
    "    fig.subplots_adjust(wspace=0.4, hspace=0.2)\n",
    "    ticks = np.linspace(-1., 1., 9)\n",
    "    cutoff = 1. - 1e-10\n",
    "    \n",
    "    for i, p in enumerate(planes):\n",
    "        z = policy(discretization.all_points[disc_slices[i]]).eval()\n",
    "        z = z.reshape(discretization.num_points[p])\n",
    "        im = axes[i].imshow(z.T, origin='lower', extent=limits[p, :].ravel(), aspect=limits[p[0], 0] / limits[p[1], 0], \n",
    "                            cmap=HEAT_MAP, vmin=-cutoff, vmax=cutoff)\n",
    "        cbar = fig.colorbar(im, ax=axes[i], label=r'$u = \\pi(x)$', ticks=ticks)\n",
    "        if i == 0:\n",
    "            axes[i].set_xlabel(r'$x$ [m]')\n",
    "            axes[i].set_ylabel(r'$v$ [m/s]')  \n",
    "        else:\n",
    "            axes[i].set_xlabel(r'$\\theta$ [deg]')\n",
    "            axes[i].set_ylabel(r'$\\omega$ [deg/s]')\n",
    " \n",
    "    plt.show()\n",
    "\n",
    "# Visualize policy\n",
    "plot_policy(policy, state_discretization, state_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LQR Lyapunov Candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Lyapunov function corresponding to the known policy\n",
    "lyapunov_function = safe_learning.QuadraticFunction(P)\n",
    "grad_lyapunov_function = safe_learning.LinearSystem((2 * P,))\n",
    "\n",
    "# initial_safe_set = np.all(state_discretization.all_points == 0.0, axis=1)\n",
    "values = session.run(lyapunov_function(state_discretization.all_points))\n",
    "cutoff = 0.01 * np.max(values)\n",
    "initial_safe_set = np.squeeze(values, axis=1) <= cutoff\n",
    "\n",
    "# Scaling\n",
    "lyapunov_function = safe_learning.QuadraticFunction(P / np.max(values))\n",
    "grad_lyapunov_function = safe_learning.LinearSystem((2 * P / np.max(values),))\n",
    "\n",
    "# Lipschitz constants\n",
    "L_pol = lambda s: tf.constant(np.linalg.norm(-K, 1), dtype=OPTIONS.tf_dtype)\n",
    "L_dyn = lambda s: np.linalg.norm(A, 1) + np.linalg.norm(B, 1)*L_pol(s)\n",
    "\n",
    "if OPTIONS.use_lipschitz_scaling:\n",
    "    L_v = lambda s: tf.abs(grad_lyapunov_function(s))\n",
    "else:\n",
    "    L_v = lambda s: tf.norm(grad_lyapunov_function(s), ord=1, axis=1, keep_dims=True)\n",
    "\n",
    "# Initialize class\n",
    "lyapunov_lqr = safe_learning.Lyapunov(state_discretization, lyapunov_function, dynamics, L_dyn, L_v, tau, policy, initial_safe_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = lyapunov_lqr.feed_dict[lyapunov_lqr.c_max]\n",
    "num_grid = lyapunov_lqr.discretization.nindex\n",
    "num_safe = lyapunov_lqr.safe_set.sum()\n",
    "\n",
    "print('Before update ...')\n",
    "print('c_max: {}'.format(c))\n",
    "print('grid size: {}'.format(num_grid))\n",
    "print('safe set size: {} ({:.2f}%)'.format(num_safe, 100 * num_safe / num_grid))\n",
    "debug(lyapunov_lqr, dynamics, state_norm, plot='cartpole')\n",
    "\n",
    "lyapunov_lqr.update_values()\n",
    "lyapunov_lqr.update_safe_set()\n",
    "c = lyapunov_lqr.feed_dict[lyapunov_lqr.c_max]\n",
    "num_safe = lyapunov_lqr.safe_set.sum()\n",
    "\n",
    "print('After update ...')\n",
    "print('c_max: {}'.format(c))\n",
    "print('grid size: {}'.format(num_grid))\n",
    "print('safe set size: {} ({:.2f}%)'.format(num_safe, 100 * num_safe / num_grid))\n",
    "debug(lyapunov_lqr, dynamics, state_norm, plot='cartpole')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Lyapunov Candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [64, 64]\n",
    "activations = [tf.tanh, tf.tanh]\n",
    "lyapunov_function = LyapunovNetwork(state_dim, layer_dims, activations, OPTIONS.eps)\n",
    "\n",
    "# TODO outputs tensor of shape (1, ?, 2)\n",
    "# grad_lyapunov_function = lambda s: tf.gradients(lyapunov_function(s), s)\n",
    "# if USE_LIPSCHITZ_SCALING:\n",
    "#     L_v = lambda s: tf.abs(grad_lyapunov_function(s))\n",
    "# else:\n",
    "#     L_v = lambda s: tf.norm(grad_lyapunov_function(s), ord=1, axis=1, keepdims=True)\n",
    "\n",
    "L_v = 1.\n",
    "\n",
    "# TODO need to use template before variables exist in the graph\n",
    "tf_states = tf.placeholder(OPTIONS.tf_dtype, shape=[None, state_dim], name='states')\n",
    "temp = lyapunov_function(tf_states)\n",
    "session.run(tf.variables_initializer(lyapunov_function.parameters))\n",
    "\n",
    "lyapunov = safe_learning.Lyapunov(state_discretization, lyapunov_function, dynamics, L_dyn, L_v, tau, policy, initial_safe_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = get_storage(_STORAGE)\n",
    "if storage is None:\n",
    "    tf_states = tf.placeholder(OPTIONS.tf_dtype, shape=[None, state_dim], name='states')\n",
    "    tf_actions = policy(tf_states)\n",
    "    tf_future_states = dynamics(tf_states, tf_actions)\n",
    "    \n",
    "    tf_values_lqr = lyapunov_lqr.lyapunov_function(tf_states)\n",
    "    tf_future_values_lqr = lyapunov_lqr.lyapunov_function(tf_future_states)\n",
    "    tf_dv_lqr = tf_future_values_lqr - tf_values_lqr\n",
    "\n",
    "    tf_values = lyapunov.lyapunov_function(tf_states)\n",
    "    tf_future_values = lyapunov.lyapunov_function(tf_future_states)\n",
    "    tf_dv = tf_future_values - tf_values\n",
    "    \n",
    "    tf_threshold = lyapunov.threshold(tf_states, lyapunov.tau)\n",
    "    tf_negative = tf.squeeze(tf.less(tf_dv, tf_threshold), axis=1)\n",
    "    \n",
    "    storage = [('states', tf_states), \n",
    "               ('future_states', tf_future_states), \n",
    "               ('values_lqr', tf_values_lqr), \n",
    "               ('values', tf_values), \n",
    "               ('future_values_lqr', tf_future_values_lqr), \n",
    "               ('future_values', tf_future_values),\n",
    "               ('dv_lqr', tf_dv_lqr), \n",
    "               ('dv', tf_dv),\n",
    "               ('threshold', tf_threshold), \n",
    "               ('negative', tf_negative)]\n",
    "    set_storage(_STORAGE, storage)\n",
    "else:\n",
    "    (tf_states, tf_future_states, tf_values_lqr, tf_values, tf_future_values_lqr, tf_future_values, \n",
    "     tf_dv_lqr, tf_dv, tf_threshold, tf_negative)  = storage.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Region of Attraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridify(norms, maxes=None, num_points=25):    \n",
    "    norms = np.asarray(norms).ravel()\n",
    "    if maxes is None:\n",
    "        maxes = norms\n",
    "    else:\n",
    "        maxes = np.asarray(maxes).ravel()\n",
    "    limits = np.column_stack((- maxes / norms, maxes / norms))\n",
    "    \n",
    "    if isinstance(num_points, int):\n",
    "        num_points = [num_points, ] * len(norms)\n",
    "    grid = safe_learning.GridWorld(limits, num_points)\n",
    "    return grid\n",
    "\n",
    "\n",
    "def compute_roa(grid, closed_loop_dynamics, horizon=250, tol=1e-3, equilibrium=None, no_traj=True):\n",
    "    if isinstance(grid, np.ndarray):\n",
    "        all_points = grid\n",
    "        nindex = grid.shape[0]\n",
    "        ndim = grid.shape[1]\n",
    "    else:\n",
    "        all_points = grid.all_points\n",
    "        nindex = grid.nindex\n",
    "        ndim = grid.ndim\n",
    "    \n",
    "    # Forward-simulate all trajectories from initial points in the discretization\n",
    "    if no_traj:\n",
    "        end_states = all_points\n",
    "        for t in range(1, horizon):\n",
    "            end_states = closed_loop_dynamics(end_states)\n",
    "    else:\n",
    "        trajectories = np.empty((nindex, ndim, horizon))\n",
    "        trajectories[:, :, 0] = all_points\n",
    "        for t in range(1, horizon):\n",
    "            trajectories[:, :, t] = closed_loop_dynamics(trajectories[:, :, t - 1])\n",
    "        end_states = trajectories[:, :, -1]\n",
    "            \n",
    "    if equilibrium is None:\n",
    "        equilibrium = np.zeros((1, ndim))\n",
    "    \n",
    "    # Compute an approximate ROA as all states that end up \"close\" to 0\n",
    "    dists = np.linalg.norm(end_states - equilibrium, ord=2, axis=1, keepdims=True).ravel()\n",
    "    roa = (dists <= tol)\n",
    "    if no_traj:\n",
    "        return roa, dists\n",
    "    else:\n",
    "        return roa, dists, trajectories\n",
    "\n",
    "\n",
    "norms = np.array([x_max, np.rad2deg(theta_max), v_max, np.rad2deg(omega_max)])\n",
    "maxes = np.copy(norms)\n",
    "plot_limits = np.column_stack((- maxes, maxes))\n",
    "# N = 21\n",
    "# grid = gridify(norms, maxes, N)\n",
    "grid = lyapunov.discretization\n",
    "\n",
    "closed_loop_dynamics = lambda x: tf_future_states.eval({tf_states: x})\n",
    "horizon = 1000\n",
    "tol = 0.001\n",
    "roa, dists, trajectories = compute_roa(grid, closed_loop_dynamics, horizon, tol, no_traj=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roa(roa, grid, plot_limits, fixed_state=[0., 0., 0., 0.]):\n",
    "    # Snap fixed_state to the closest grid point\n",
    "    fixed_state = np.asarray(fixed_state, dtype=OPTIONS.np_dtype)\n",
    "    fixed_index = np.zeros_like(fixed_state, dtype=int)\n",
    "    for d in range(grid.ndim):\n",
    "        fixed_index[d], fixed_state[d] = find_nearest(grid.discrete_points[d], fixed_state[d])\n",
    "\n",
    "    plt.rc('font', size=5)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 3), dpi=OPTIONS.dpi)\n",
    "    fig.subplots_adjust(wspace=0.4, hspace=0.2)\n",
    "\n",
    "    ax = axes[0]\n",
    "    z = roa[:, fixed_index[1], :, fixed_index[3]]\n",
    "    im = ax.imshow(z.T, origin='lower', extent=plot_limits[[0, 2], :].ravel(), aspect=plot_limits[0, 1] / plot_limits[2, 1], cmap=BINARY_MAP, vmin=0)\n",
    "    ax.set_xlabel(r'$x$ [m]')\n",
    "    ax.set_ylabel(r'$v$ [m/s]')\n",
    "\n",
    "    ax = axes[1]\n",
    "    z = roa[fixed_index[0], :, fixed_index[2], :]\n",
    "    im = ax.imshow(z.T, origin='lower', extent=plot_limits[[1, 3], :].ravel(), aspect=plot_limits[1, 1] / plot_limits[3, 1], cmap=BINARY_MAP, vmin=0)\n",
    "    ax.set_xlabel(r'$\\theta$ [deg]')\n",
    "    ax.set_ylabel(r'$\\omega$ [deg/s]')\n",
    "    \n",
    "    return fig, axes\n",
    "\n",
    "print('ROA fraction: {}'.format(roa.sum() / grid.nindex))\n",
    "fig, axes = plot_roa(roa, grid, plot_limits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network: Supervised Training with LQR Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('supervised_lyapunov_learning'):\n",
    "    tf_costs = tf.abs(tf_values_lqr - tf_values)\n",
    "    tf_inverse_weights = tf_values_lqr + OPTIONS.eps\n",
    "#     tf_inverse_weights = 1\n",
    "    tf_objective = tf.reduce_mean(tf_costs / tf_inverse_weights, name='objective')\n",
    "    \n",
    "    tf_learning_rate = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='learning_rate')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(tf_learning_rate)\n",
    "    lyapunov_update = optimizer.minimize(tf_objective, var_list=lyapunov.lyapunov_function.parameters)\n",
    "\n",
    "session.run(tf.variables_initializer(lyapunov_function.parameters))\n",
    "# lyapunov.update_values()\n",
    "# lyapunov.update_safe_set()\n",
    "# debug(lyapunov, dynamics, state_norm, plot='cartpole')\n",
    "\n",
    "obj = []\n",
    "level_states = lyapunov_lqr.discretization.all_points[lyapunov.initial_safe_set]\n",
    "# level_states = lyapunov_lqr.discretization.all_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training batch from level set\n",
    "tf_batch_size = tf.placeholder(tf.int32, [], 'batch_size')\n",
    "tf_batch = tf.random_uniform([tf_batch_size, ], 0, level_states.shape[0], dtype=tf.int32, name='batch_sample')\n",
    "\n",
    "# Test set\n",
    "test_size = int(1e4)\n",
    "idx = tf_batch.eval({tf_batch_size: test_size})\n",
    "test_set = level_states[idx, :]\n",
    "\n",
    "# Uniformly-distributed test set\n",
    "# test_size = int(1e5)\n",
    "# grid_length = np.power(test_size, 1 / state_dim)        # test_size = N^d, solve for N\n",
    "# grid_length = int(2 * np.floor(grid_length / 2) + 1)    # round N to the nearest odd integer to include 0 in grid\n",
    "# state_limits = np.array([[-1., 1.]] * state_dim)        # states are normalized to [-1, 1]^d\n",
    "# num_points = [grid_length, ] * state_dim\n",
    "# test_set = safe_learning.GridWorld(state_limits, num_points).all_points\n",
    "    \n",
    "feed_dict = {\n",
    "    tf_states:         level_states,\n",
    "    tf_learning_rate:  1e-3,\n",
    "    tf_batch_size:     int(1e2),\n",
    "}\n",
    "max_iters = 250\n",
    "\n",
    "for i in tqdm(range(max_iters)):\n",
    "    idx = tf_batch.eval(feed_dict)\n",
    "    feed_dict[tf_states] = level_states[idx, :]\n",
    "    session.run(lyapunov_update, feed_dict)\n",
    "\n",
    "    feed_dict[tf_states] = test_set\n",
    "    obj.append(tf_objective.eval(feed_dict).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 2), dpi=300)\n",
    "ax.set_xlabel(r'iteration')\n",
    "ax.set_ylabel(r'objective')\n",
    "ax.plot(obj, '.-r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_functions(grid, maxes, fixed_state=[0., 0., 0., 0.]):\n",
    "    outputs = session.run([tf_values, tf_values_lqr, tf_dv, tf_dv_lqr], {tf_states: grid.all_points})\n",
    "    values, values_lqr, dv, dv_lqr = [out.reshape(grid.num_points) for out in outputs]\n",
    "\n",
    "    # Snap fixed_state to the closest grid point\n",
    "    fixed_state = np.asarray(fixed_state, dtype=OPTIONS.np_dtype)\n",
    "    fixed_index = np.zeros_like(fixed_state, dtype=int)\n",
    "    for d in range(grid.ndim):\n",
    "        fixed_index[d], fixed_state[d] = find_nearest(grid.discrete_points[d], fixed_state[d])\n",
    "\n",
    "    plt.rc('font', size=6)    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6), dpi=OPTIONS.dpi)\n",
    "    fig.subplots_adjust(wspace=0.6, hspace=0.1)\n",
    "    for ax in axes.ravel():\n",
    "        ax.set_xlabel(r'$\\theta$ [deg]')\n",
    "        ax.set_ylabel(r'$\\omega$ [deg/s]')\n",
    "\n",
    "    limits = np.column_stack((- maxes, maxes))\n",
    "        \n",
    "    for i, (v, dv) in enumerate(zip((values, values_lqr), (dv, dv_lqr))):\n",
    "        ax = axes[i, 0]\n",
    "        z = v[:, fixed_index[1], :, fixed_index[3]]\n",
    "        im = ax.imshow(z.T, origin='lower', extent=limits[[0, 2], :].ravel(), aspect=limits[0, 0] / limits[2, 0], cmap=LEVEL_MAP, vmin=0)\n",
    "        ax.set_xlabel(r'$x$ [m]')\n",
    "        ax.set_ylabel(r'$v$ [m/s]')\n",
    "        cbar = fig.colorbar(im, ax=ax, label=r'$v(x)$')\n",
    "        \n",
    "        ax = axes[i, 1]\n",
    "        z = v[fixed_index[0], :, fixed_index[2], :]\n",
    "        im = ax.imshow(z.T, origin='lower', extent=limits[[1, 3], :].ravel(), aspect=limits[1, 0] / limits[3, 0], cmap=LEVEL_MAP, vmin=0)\n",
    "        ax.set_xlabel(r'$\\theta$ [deg]')\n",
    "        ax.set_ylabel(r'$\\omega$ [deg/s]')\n",
    "        cbar = fig.colorbar(im, ax=ax, label=r'$v(x)$')\n",
    "        \n",
    "        ax = axes[i, 2]\n",
    "        z = dv[:, fixed_index[1], :, fixed_index[3]]\n",
    "        im = ax.imshow(z.T, origin='lower', extent=limits[[0, 2], :].ravel(), aspect=limits[0, 0] / limits[2, 0], cmap=HEAT_MAP, vmax=0)\n",
    "        ax.set_xlabel(r'$x$ [m]')\n",
    "        ax.set_ylabel(r'$v$ [m/s]')\n",
    "        cbar = fig.colorbar(im, ax=ax, label=r'$v(f(x)) - v(x)$')\n",
    "        \n",
    "        ax = axes[i, 3]\n",
    "        z = dv[fixed_index[0], :, fixed_index[2], :]\n",
    "        im = ax.imshow(z.T, origin='lower', extent=limits[[1, 3], :].ravel(), aspect=limits[1, 0] / limits[3, 0], cmap=HEAT_MAP, vmax=0)\n",
    "        ax.set_xlabel(r'$\\theta$ [deg]')\n",
    "        ax.set_ylabel(r'$\\omega$ [deg/s]')\n",
    "        cbar = fig.colorbar(im, ax=ax, label=r'$v(f(x)) - v(x)$')\n",
    "        \n",
    "        for j in range(4):\n",
    "            if i == 0:\n",
    "                axes[i, j].set_title('NN')\n",
    "            else:\n",
    "                axes[i, j].set_title('LQR')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_value_functions(grid, maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyapunov.update_values()\n",
    "lyapunov.update_safe_set()\n",
    "\n",
    "print('c_max: {}'.format(lyapunov.feed_dict[lyapunov.c_max]))\n",
    "print('safe set size: {}'.format(lyapunov.safe_set.sum()))\n",
    "debug(lyapunov, dynamics, state_norm, plot='cartpole')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint for neural net weights\n",
    "saver = tf.train.Saver(var_list=lyapunov.lyapunov_function.parameters)\n",
    "ckpt_path = saver.save(session, \"/tmp/spencerr_cartpole_lyapunov.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('roa_classification'):\n",
    "    # Current maximum level set we want to push the ROA in to\n",
    "    tf_level_multiplier = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='level_multiplier')\n",
    "    tf_c_max = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='c_max')\n",
    "    \n",
    "    # True class labels, converted from Boolean ROA labels {0, 1} to {-1, 1}\n",
    "    tf_weights = tf.placeholder(OPTIONS.tf_dtype, shape=[None, 1], name='class_weights')\n",
    "    tf_roa = tf.placeholder(OPTIONS.tf_dtype, shape=[None, 1], name='labels')\n",
    "    tf_labels = 2 * tf_roa - 1\n",
    "\n",
    "    # Construct classifier with output (-1, 1)\n",
    "#     tf_classifier_output = tf.tanh(100 * (tf_c_max - tf_values))\n",
    "#     tf_classifier_output = (tf_c_max - tf_values) / (tf.abs(tf_c_max - tf_values) + OPTIONS.eps)\n",
    "    tf_classifier_output = tf_c_max - tf_values\n",
    "    \n",
    "    # Use hinge or perceptron loss for the classification performance\n",
    "    tf_classifier_loss = tf_weights * tf.maximum(- tf_labels * tf_classifier_output, 0, name='hinge_loss')\n",
    "        \n",
    "    # Enforce decrease constraint with Lagrangian relaxation\n",
    "    tf_lagrange_multiplier = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='lagrange_multiplier')\n",
    "    tf_decrease_loss = tf_roa * tf.maximum(tf_dv / (tf_values + OPTIONS.eps), 0)\n",
    "    \n",
    "    # Construct objective and optimizer\n",
    "    tf_objective = tf.reduce_mean(tf_classifier_loss + tf_lagrange_multiplier * tf_decrease_loss, name='objective')\n",
    "    tf_learning_rate = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='learning_rate')\n",
    "    tf_epsilon = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='adam_epsilon')\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(tf_learning_rate)\n",
    "#     optimizer = tf.train.AdamOptimizer(tf_learning_rate, epsilon=tf_epsilon)\n",
    "#     training_update = optimizer.minimize(tf_objective, var_list=lyapunov.lyapunov_function.parameters)\n",
    "    grads_and_vars = optimizer.compute_gradients(tf_objective, lyapunov.lyapunov_function.parameters)\n",
    "    training_update = optimizer.apply_gradients(grads_and_vars)\n",
    "      \n",
    "\n",
    "with tf.name_scope('sampling'):\n",
    "    tf_batch_size = tf.placeholder(tf.int32, [], 'batch_size')\n",
    "    tf_idx_range = tf.placeholder(tf.int32, shape=[], name='indices_to_sample')\n",
    "    tf_idx_batch = tf.random_uniform([tf_batch_size, ], 0, tf_idx_range, dtype=tf.int32, name='batch_sample')\n",
    "    \n",
    "#     with tf.name_scope('accuracy'):\n",
    "#         tf_accuracy = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "#         acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "#         acc = tf.reduce_mean(tf.cast(acc, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_confusion_weights(y, y_true, scale_by_total=True):\n",
    "    y = y.astype(np.bool)\n",
    "    y_true = y_true.astype(np.bool)\n",
    "    \n",
    "    # Assuming labels in {0, 1}, count entries from confusion matrix\n",
    "    TP = ( y &  y_true).sum()\n",
    "    TN = (~y & ~y_true).sum()\n",
    "    FP = ( y & ~y_true).sum()\n",
    "    FN = (~y &  y_true).sum()\n",
    "    confusion_counts = np.array([[TN, FN], [FP, TP]])\n",
    "    \n",
    "    # Scale up each sample by inverse of confusion weight\n",
    "    weights = np.ones_like(y, dtype=float)\n",
    "    weights[ y &  y_true] /= TP\n",
    "    weights[~y & ~y_true] /= TN\n",
    "    weights[ y & ~y_true] /= FP\n",
    "    weights[~y &  y_true] /= FN\n",
    "    if scale_by_total:\n",
    "        weights *= y.size\n",
    "    \n",
    "    return weights, confusion_counts\n",
    "\n",
    "\n",
    "def balanced_class_weights(y_true, scale_by_total=True):\n",
    "    y = y_true.astype(np.bool)\n",
    "    nP = y.sum()\n",
    "    nN = y.size - y.sum()\n",
    "    class_counts = np.array([nN, nP])\n",
    "    \n",
    "    weights = np.ones_like(y, dtype=float)\n",
    "    weights[ y] /= nP\n",
    "    weights[~y] /= nN\n",
    "    if scale_by_total:\n",
    "        weights *= y.size\n",
    "    \n",
    "    return weights, class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore checkpoint\n",
    "saver.restore(session, ckpt_path)\n",
    "lyapunov.update_values()\n",
    "lyapunov.update_safe_set()\n",
    "session.run(tf.variables_initializer(optimizer.variables()))  # TODO\n",
    "\n",
    "outer_offset = 0\n",
    "inner_offset = 0\n",
    "\n",
    "obj          = []\n",
    "loss_class   = []\n",
    "loss_dec     = []\n",
    "roa_estimate = np.copy(lyapunov.safe_set)\n",
    "\n",
    "c_max        = [lyapunov.feed_dict[lyapunov.c_max], ]\n",
    "safe_size    = [lyapunov.safe_set.sum() / lyapunov.discretization.nindex, ]\n",
    "grid         = lyapunov.discretization\n",
    "iters_to_converge = []\n",
    "\n",
    "\n",
    "summaries = []\n",
    "summaries.append(tf.summary.scalar('objective', tf_objective))\n",
    "\n",
    "for param in lyapunov.lyapunov_function.parameters:\n",
    "    summaries.append(tf.summary.histogram(param.name[:-2], param))\n",
    "\n",
    "for grad, var in grads_and_vars:\n",
    "    summaries.append(tf.summary.histogram(var.name[:-2] + '/gradient', grad))\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge(summaries)\n",
    "\n",
    "# TODO\n",
    "summary_writer = tf.summary.FileWriter(OPTIONS.log_path, graph=tf.get_default_graph())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_iters      = 3\n",
    "inner_iters      = 15\n",
    "tol              = 1e-8\n",
    "horizon          = 50\n",
    "batch_size       = int(1e2)\n",
    "test_size        = int(1e4)\n",
    "\n",
    "feed_dict = {\n",
    "    tf_states:               np.zeros((1, lyapunov.discretization.ndim)), # placeholder\n",
    "    tf_idx_range:            1,                                           # placeholder\n",
    "    tf_batch_size:           batch_size,\n",
    "    tf_c_max:                1.,\n",
    "    tf_lagrange_multiplier:  100,\n",
    "    #\n",
    "    tf_learning_rate:        6e-3,\n",
    "    tf_epsilon:              1e-1,\n",
    "    tf_level_multiplier:     3.,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Current metrics ...')\n",
    "c = lyapunov.feed_dict[lyapunov.c_max]\n",
    "num_safe = lyapunov.safe_set.sum()\n",
    "print('c_max: {}'.format(c))\n",
    "print('grid size: {}'.format(grid.nindex))\n",
    "print('safe set size: {} ({:.2f}% of grid, {:.2f}% of ROA)\\n'.format(int(num_safe), 100 * num_safe / grid.nindex, 100 * num_safe / roa.sum()))\n",
    "print('')\n",
    "time.sleep(0.5)\n",
    "\n",
    "for i in range(outer_iters):\n",
    "    outer_offset += 1\n",
    "    c = lyapunov.feed_dict[lyapunov.c_max]\n",
    "#     feed_dict[tf_c_max] = c\n",
    "#     time.sleep(0.5)\n",
    "    \n",
    "    # Get states inside V(a * c_max), a > 1    \n",
    "    idx_small = lyapunov.values.ravel() <= c\n",
    "    idx_big = lyapunov.values.ravel() <= feed_dict[tf_level_multiplier] * c\n",
    "    idx_gap = np.logical_and(idx_big, ~idx_small)\n",
    "    \n",
    "    #\n",
    "    V_gap = grid.all_points[idx_gap]\n",
    "    V_future = np.copy(V_gap)\n",
    "    for _ in range(horizon):\n",
    "        V_future = tf_future_states.eval({tf_states: V_future})\n",
    "    V_future = tf_values.eval({tf_states: V_future})\n",
    "    safe_in_future = (V_future <= c).ravel()\n",
    "    \n",
    "    roa_estimate[idx_gap] |= safe_in_future\n",
    "    \n",
    "    target_idx = np.logical_or(idx_big, roa_estimate)\n",
    "    target_set = grid.all_points[target_idx]\n",
    "    target_labels = roa_estimate[target_idx].astype(OPTIONS.np_dtype).reshape([-1, 1])\n",
    "#     target_set = grid.all_points\n",
    "#     target_labels = roa_estimate.astype(OPTIONS.np_dtype).reshape([-1, 1])\n",
    "    feed_dict[tf_idx_range] = target_set.shape[0]\n",
    "    \n",
    "    # Test set\n",
    "#     feed_dict[tf_batch_size] = test_size\n",
    "#     idx_test = tf_idx_batch.eval(feed_dict)\n",
    "#     test_set = target_set[idx_test]\n",
    "#     test_labels = target_labels[idx_test]\n",
    "\n",
    "    test_set = grid.all_points\n",
    "    test_labels = roa.reshape([-1, 1])\n",
    "    \n",
    "    test_set = target_set\n",
    "    test_labels = target_labels\n",
    "    \n",
    "    # SGD for classification\n",
    "    converged = False\n",
    "    feed_dict[tf_batch_size] = batch_size\n",
    "\n",
    "    for j in tqdm(range(inner_iters)):\n",
    "        inner_offset += 1\n",
    "        # Training step\n",
    "        idx_batch = tf_idx_batch.eval(feed_dict)\n",
    "        feed_dict[tf_states] = target_set[idx_batch]\n",
    "        feed_dict[tf_roa] = target_labels[idx_batch]\n",
    "#         feed_dict[tf_weights], counts = balanced_confusion_weights(tf_values.eval(feed_dict) <= feed_dict[tf_c_max], feed_dict[tf_roa].astype(bool))\n",
    "        feed_dict[tf_weights], counts = balanced_class_weights(feed_dict[tf_roa].astype(bool))\n",
    "        _, summary = session.run([training_update, merged_summary_op], feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(summary, inner_offset)\n",
    "\n",
    "        # Record objectives\n",
    "        feed_dict[tf_states] = test_set\n",
    "        feed_dict[tf_roa] = test_labels\n",
    "#         feed_dict[tf_weights], counts = balanced_confusion_weights(tf_values.eval(feed_dict) <= feed_dict[tf_c_max], feed_dict[tf_roa].astype(bool))\n",
    "        feed_dict[tf_weights], counts = balanced_class_weights(feed_dict[tf_roa].astype(bool))\n",
    "    \n",
    "        results = session.run([tf_classifier_loss, tf_decrease_loss], feed_dict)\n",
    "        loss_class.append(results[0].mean())\n",
    "        loss_dec.append(results[1].mean())\n",
    "        obj.append(loss_class[-1] + feed_dict[tf_lagrange_multiplier] * loss_dec[-1])\n",
    "\n",
    "        if obj[-1] < tol:\n",
    "            converged = True\n",
    "            break\n",
    "\n",
    "    iters_to_converge.append(j + 1)\n",
    "    if converged:\n",
    "        print('Converged in {} iterations!'.format(j + 1))\n",
    "    else:\n",
    "        print('Did not converge!')\n",
    "\n",
    "    print('Updating values ...')\n",
    "    lyapunov.update_values()\n",
    "\n",
    "    print('Updating c_max ...')\n",
    "    lyapunov.update_safe_set()\n",
    "    roa_estimate |= lyapunov.safe_set\n",
    "\n",
    "    c_max.append(lyapunov.feed_dict[lyapunov.c_max])\n",
    "    safe_size.append(lyapunov.safe_set.sum() / grid.nindex)\n",
    "    print('Done!')\n",
    "#     print(class_ratio)\n",
    "    print(counts)\n",
    "    print('c_max: {}'.format(c_max[-1]))\n",
    "    print('grid size: {}'.format(grid.nindex))\n",
    "    print('safe set size: {} ({:.2f}% of grid, {:.2f}% of ROA)\\n'.format(int(safe_size[-1] * grid.nindex), \n",
    "                                                                         100 * safe_size[-1], \n",
    "                                                                         100 * safe_size[-1] * roa.size / roa.sum()))\n",
    "    time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=6)\n",
    "roa_fraction = roa.sum() / roa.size\n",
    "\n",
    "#\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3), dpi=300)\n",
    "ax.plot(loss_class, '.-r')\n",
    "# ax.tick_params('y', colors='r')\n",
    "# ax.set_ylim([None, 0.9])\n",
    "\n",
    "ax.set_xlabel(r'SGD iteration (accumulated)')\n",
    "ax.set_xticks(list(range(0, len(loss_class) + 1, inner_iters)))\n",
    "\n",
    "# ax = ax.twinx()\n",
    "ax.plot(feed_dict[tf_lagrange_multiplier] * np.asarray(loss_dec), '.-b')\n",
    "# ax.tick_params('y', colors='b')\n",
    "# ax.set_ylim([None, 0.0016])\n",
    "\n",
    "ax.set_ylabel(r'Training loss')\n",
    "\n",
    "\n",
    "proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in ['red', 'blue']]    \n",
    "legend = ax.legend(proxy, ['Classification loss', 'Lyapunov decrease loss'], loc='upper right')\n",
    "legend.get_frame().set_alpha(1.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "plt.rc('font', size=6)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3), dpi=300)\n",
    "\n",
    "ax.plot(c_max, '.-r')\n",
    "ax.set_ylabel(r'$c_k$')\n",
    "ax.tick_params('y', colors='r')\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "ax.set_xlabel(r'Safe set update iteration $k$')\n",
    "ax.set_xticks(list(range(0, len(c_max) + 1, 1)))\n",
    "\n",
    "ax = ax.twinx()\n",
    "ax.plot(np.array(safe_size) / roa_fraction, '.-b')\n",
    "ax.set_ylabel(r'$|\\mathcal{V}(c_k) \\cap \\mathcal{X}_\\tau|\\ /\\ |\\mathcal{R} \\cap \\mathcal{X}_\\tau|$')\n",
    "ax.tick_params('y', colors='b')\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(np.array(safe_size) / roa_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = lyapunov.feed_dict[lyapunov.c_max]\n",
    "num_grid = lyapunov.discretization.nindex\n",
    "num_safe = lyapunov.safe_set.sum()\n",
    "num_safe_lqr = lyapunov_lqr.safe_set.sum()\n",
    "\n",
    "print('c_max: {}'.format(c))\n",
    "print('grid size: {}'.format(num_grid))\n",
    "print('ROA size: {:.2f}%'.format(100 * roa.sum() / roa.size))\n",
    "print('lqr safe set size: {} ({:.2f}%)'.format(num_safe_lqr, 100 * num_safe_lqr / num_grid))\n",
    "print('nn safe set size: {} ({:.2f}%)'.format(num_safe, 100 * num_safe / num_grid))\n",
    "\n",
    "# debug(lyapunov, dynamics, state_norm, plot='pendulum')\n",
    "# debug(lyapunov_lqr, dynamics, state_norm, plot='pendulum')\n",
    "\n",
    "print(num_safe_lqr / roa.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
